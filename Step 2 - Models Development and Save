Models Development and Valadation
Schedule refresh for models
###pip install schedule
#import sys

#sys.path.append("/past/the/path/you/copied/here")
import time
import threading
import schedule
from IPython.display import display, Javascript
#def run_notebook_code():
    ### Your notebook code to be executed every 24 hours
#    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.ncells())'))

#def run_scheduler():
    ### Schedule the code to run every 24 hours
#    schedule.every(24).hours.do(run_notebook_code)

    ### Run the scheduler
#    while True:
#        schedule.run_pending()
#        time.sleep(1)

    ### Create a thread for the scheduler and start it
#scheduler_thread = threading.Thread(target=run_scheduler)
#scheduler_thread.daemon = True  # Daemonize the thread so it exits when the main thread exits
#scheduler_thread.start()
Import Packages
#pip install factor_analyzer
#pip install --upgrade numpy
#pip install --user numpy
#pip install statsmodels
import pandas as pd
import numpy as np

import scipy.stats as ss
import researchpy as rp
import scipy.stats as stats
import csv
import pandas as pd
import numpy as np

from scipy import stats
from scipy.stats import chisquare
from scipy.stats import chi2_contingency
from scipy.stats import f_oneway
from scipy.stats import kstest
from scipy.stats import ks_2samp
from scipy.stats import norm
from scipy.stats import iqr

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.datasets import load_boston
from sklearn import linear_model

import matplotlib.pyplot as plt
plt.style.use('classic')
import seaborn as sns
plt.style.use('seaborn')
get_ipython().run_line_magic('matplotlib', 'inline')

#from ydata_profiling import ProfileReport
import statsmodels.api as sm
import statsmodels.formula.api as smf

import chart_studio
import re
import cv2

import missingno as msno
import warnings
warnings.filterwarnings("ignore")

import cloudinary
import cloudinary.uploader
import cloudinary.api

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#port csv
#mport pyodbc
#.read_sql_query

import researchpy as rp
import scipy.stats as stats


import pandas as pd
import numpy as np
import scipy.stats as ss
from math import log
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
Import Data
df = pd.read_excel("C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/2Input_processed/Final_Data_Prep.XLSX")
df.head()
               Date  Index  Index_Real     ID_Num First_Name Last_Name  \
0  21/08/2023 14:44      1        5439   37107588          *         *   
1  17/08/2023 09:37      2        5403  211968169          *         *   
2  17/08/2023 07:43      3        5395  340866375          *         *   
3  17/08/2023 07:34      4        5391  206253106          *         *   
4  17/08/2023 07:45      5        5398  310274717          *         *   

                        Mail  Gender_Dico  Married_Dico  Nation_Dico  ...  \
0  Roei.zami@mail.huji.ac.il            1             1            1  ...   
1     Adimarko6309@gmail.com            1             0            1  ...   
2       ntordjman3@gmail.com            1             1            1  ...   
3       Yardenza26@gmail.com            1             0            1  ...   
4      basalov1710@yahoo.com            1             1            1  ...   

   Heb_Educational_achievement_index  Heb_Job_Motivators_Index  \
0                                  4                     0.590   
1                                  1                     0.075   
2                                  1                    -0.088   
3                                  0                     0.005   
4                                  0                     0.151   

   Heb_Interests_and_Activities_Index  Normot_Job_Motivators_Index  \
0                               0.308                       -0.323   
1                              -0.060                       -0.128   
2                               0.444                        0.277   
3                              -0.126                        0.009   
4                              -0.205                       -0.234   

   Normot_Interests_and_Activities_Index  \
0                                  0.000   
1                                  0.000   
2                                 -0.135   
3                                  0.000   
4                                  0.000   

   Personality_Dis_Job_Motivators_Index  Gibushon_Job_Motivators_Index  \
0                              0.278108                       0.505280   
1                             -0.378540                       0.156756   
2                             -1.102258                       0.247961   
3                              0.109498                       0.205794   
4                             -0.707263                       0.331454   

   Gibushon_Job_Interests_and_Activities_Index  \
0                                       -0.189   
1                                        0.065   
2                                        0.182   
3                                        0.108   
4                                        0.185   

   Gius_Interests_and_Activities_Index  Gius_Job_Motivators_Index  
0                             0.756408                  -0.460926  
1                            -0.250902                  -0.313457  
2                            -1.189658                   0.634635  
3                            -0.042658                   0.804962  
4                            -0.360891                  -0.288304  

[5 rows x 206 columns]
Gold Lists
Gibushon_Sofi_Gold_List = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Job_Motivators_Index", "United_Commander_or_Kazin", "Interests_and_Activities_Index", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Evaluation_Center_Target"])
Rama_Sofi_Gold_List = pd.DataFrame(df, columns=["Rama_Educational_achievement_index", "Rama_United_Commander_or_Kazin", "Rama_Index_Criminal_History", "Sacham_officer_Dico", "Married_Dico", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Rama_Temp_Mean_Num", "Number_of_Attempts_Num", "RAMA_Target"])
Dapar_Sofi_Gold_List = pd.DataFrame(df, columns=["Dapar_Educational_achievement_index", "Psych_Tests_Subjective_Num", "Kaba_Grade_51_54_Dico", "Kaba_Grade_55_UP_Dico", "Married_Dico", "Special_Unit_Army_Dico", "Dapar_Job_Motivators_Index", "Dapar_Interests_and_Activities_Index", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Dapar_Target"])
Heb_Sofi_Gold_List = pd.DataFrame(df, columns=["Heb_Educational_achievement_index", "Hebrew_Meam_Num", "Service_Period_Commitment_Dico", "Age_Num", "Sacham_officer_Dico", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Evaluation_Center_Filed_Last_Attempt_Dico", "Max_Procedure_Duration_Num", "Work_Perceived_Maching_Num", "Heb_Job_Motivators_Index", "Heb_Interests_and_Activities_Index", "Hebrew_Target"])
Normot_Sofi_Gold_List = pd.DataFrame(df, columns=["Hebrew_Meam_Num", "Work_Perceived_Maching_Num", "Number_of_Attempts_Num", "Nation_Dico", "Previous_Job_Salary_Num", "Salary_Expectations_Num", "Mental_Difficulties_Dico", "Normot_Job_Motivators_Index", "Normot_Interests_and_Activities_Index","Yeodi_Liba", "Normot_Target"])
#Normot_Sofi_Gold_List = pd.DataFrame(df, columns=["Mental_Difficulties_Dico", "Hebrew_Meam_Num", "Normot_Job_Motivators_Index","Graduation_Average_60_Less_Dico", "Gender_Dico", "Financial_Difficulties_Dico", "Psych_Tests_Subjective_Num", "Math_3_Units_Num_Dico", "Weight_Num", "Temp_Mean_Num","Yeodi_Liba", "Normot_Target"])
Personality_Diss_Sofi_Gold_List = pd.DataFrame(df, columns=["Salary_Expectations_Num", "United_Commander_or_Kazin", "Psyc_Test_450_600_Dico", "Psyc_Test_450_Less_Dico", "Psyc_Test_600_Up_Dico", "Math_4_5_Units_Num_Dico", "Combat_Service_Army_Dico", "Chronic_Disease_Dico", "Relevant_Job_Experience_Dico", "Work_Perceived_Maching_Num", "Gender_Dico", "Actuavlia", "Max_Procedure_Duration_Num", "Saham_Officer_Past_Dico", "Nation_Dico", "Personality_Dis_Safek_Target"])
Gibushon_Grade_Sofi_Gold_List = pd.DataFrame(df, columns=["Psyc_Test_Index", "Gibushon_Job_Motivators_Index", "United_Commander_or_Kazin", "Age_Num", "Hebrew_1_Expressive_Num", "Gibushon_Job_Interests_and_Activities_Index", "Gibushon_Target"])
Gius_Sofi_Gold_List = pd.DataFrame(df, columns=["United_Commander_or_Kazin","Work_Perceived_Maching_Num","Saham_Officer_Past_Dico","Misconduct_Index","Number_of_Attempts_Num","Psychiatric_Drugs_Dico","Achievements_Preception_Num","Physical_Fitness_Frequ_Num","Weight_Num","Gius_Interests_and_Activities_Index","Gius_Job_Motivators_Index","Gius_Target"])
Final_Gibushon_Dico_Gold_List = pd.DataFrame(df, columns=["Hebrew_1_Expressive_Num","United_Commander_or_Kazin","Misconduct_Index","English_3_Units_Num_Dico","Job_Motivators_Index","Interests_and_Activities_Index","Previous_Job_Salary_Num","Psychiatric_Drugs_Dico","Evaluation_Center_Filed_Last_Attempt_Dico","Year_of_Service_Army_Dico", "Number_of_Attempts_Num", "Evaluation_Center_Dico_Target"])
#Final_Gibushon_Dico_Gold_List = pd.DataFrame(df, columns=["Hebrew_1_Expressive_Num","United_Commander_or_Kazin","Misconduct_Index","English_3_Units_Num_Dico","Job_Motivators_Index","Interests_and_Activities_Index","Previous_Job_Salary_Num","Married_Dico","Psychiatric_Drugs_Dico","Evaluation_Center_Filed_Last_Attempt_Dico","Physical_Fitness_Frequ_Num", "Year_of_Service_Army_Dico", "Number_of_Attempts_Num", "Evaluation_Center_Dico_Target"])
Change Lists Variables Type
for col in Gibushon_Sofi_Gold_List:
    if col in Gibushon_Sofi_Gold_List.columns:
        Gibushon_Sofi_Gold_List[col] = Gibushon_Sofi_Gold_List[col].astype(np.float64)
for col in Rama_Sofi_Gold_List:
    if col in Rama_Sofi_Gold_List.columns:
        Rama_Sofi_Gold_List[col] = Rama_Sofi_Gold_List[col].astype(np.float64)
for col in Dapar_Sofi_Gold_List:
    if col in Dapar_Sofi_Gold_List.columns:
        Dapar_Sofi_Gold_List[col] = Dapar_Sofi_Gold_List[col].astype(np.float64)
for col in Heb_Sofi_Gold_List:
    if col in Heb_Sofi_Gold_List.columns:
        Heb_Sofi_Gold_List[col] = Heb_Sofi_Gold_List[col].astype(np.float64)
for col in Normot_Sofi_Gold_List:
    if col in Normot_Sofi_Gold_List.columns:
        Normot_Sofi_Gold_List[col] = Normot_Sofi_Gold_List[col].astype(np.float64)
for col in Personality_Diss_Sofi_Gold_List:
    if col in Personality_Diss_Sofi_Gold_List.columns:
        Personality_Diss_Sofi_Gold_List[col] = Personality_Diss_Sofi_Gold_List[col].astype(np.float64)
for col in Gibushon_Grade_Sofi_Gold_List:
    if col in Gibushon_Grade_Sofi_Gold_List.columns:
        Gibushon_Grade_Sofi_Gold_List[col] = Gibushon_Grade_Sofi_Gold_List[col].astype(np.float64)
for col in Gius_Sofi_Gold_List:
    if col in Gius_Sofi_Gold_List.columns:
        Gius_Sofi_Gold_List[col] = Gius_Sofi_Gold_List[col].astype(np.float64)
for col in Final_Gibushon_Dico_Gold_List :
    if col in Final_Gibushon_Dico_Gold_List .columns:
        Final_Gibushon_Dico_Gold_List [col] = Final_Gibushon_Dico_Gold_List[col].astype(np.float64)
Normot_Sofi_Gold_List
      Hebrew_Meam_Num  Work_Perceived_Maching_Num  Number_of_Attempts_Num  \
0                 5.6                         6.0                     0.0   
1                 6.0                         6.0                     0.0   
2                 6.0                         5.0                     1.0   
3                 4.4                         5.0                     0.0   
4                 5.8                         6.0                     1.0   
...               ...                         ...                     ...   
2244              6.6                         5.0                     0.0   
2245              2.8                         4.0                     0.0   
2246              5.4                         5.0                     1.0   
2247              6.0                         5.0                     0.0   
2248              6.8                         7.0                     0.0   

      Nation_Dico  Previous_Job_Salary_Num  Salary_Expectations_Num  \
0             1.0                      4.0                      7.0   
1             1.0                      2.0                      3.0   
2             1.0                      3.0                      3.0   
3             1.0                      3.0                      4.0   
4             1.0                      4.0                      4.0   
...           ...                      ...                      ...   
2244          0.0                      5.0                      3.0   
2245          0.0                      2.0                      2.0   
2246          1.0                      3.0                      4.0   
2247          1.0                      3.0                      5.0   
2248          1.0                      2.0                      7.0   

      Mental_Difficulties_Dico  Normot_Job_Motivators_Index  \
0                          0.0                       -0.323   
1                          0.0                       -0.128   
2                          0.0                        0.277   
3                          0.0                        0.009   
4                          0.0                       -0.234   
...                        ...                          ...   
2244                       0.0                       -0.105   
2245                       1.0                        0.235   
2246                       0.0                       -0.234   
2247                       0.0                        0.031   
2248                       0.0                       -0.119   

      Normot_Interests_and_Activities_Index  Yeodi_Liba  Normot_Target  
0                                     0.000         0.0            NaN  
1                                     0.000         1.0            0.0  
2                                    -0.135         1.0            1.0  
3                                     0.000         1.0            5.0  
4                                     0.000         1.0            0.0  
...                                     ...         ...            ...  
2244                                 -0.135         0.0            NaN  
2245                                 -0.135         0.0            NaN  
2246                                 -0.135         0.0            NaN  
2247                                  0.000         0.0            NaN  
2248                                  0.000         0.0            NaN  

[2249 rows x 11 columns]
Drop Missing Values
Gibushon_Sofi_Gold_List = Gibushon_Sofi_Gold_List.dropna()
Rama_Sofi_Gold_List = Rama_Sofi_Gold_List.dropna()
Dapar_Sofi_Gold_List = Dapar_Sofi_Gold_List.dropna()
Heb_Sofi_Gold_List = Heb_Sofi_Gold_List.dropna()
Normot_Sofi_Gold_List = Normot_Sofi_Gold_List.dropna()
Personality_Diss_Sofi_Gold_List = Personality_Diss_Sofi_Gold_List.dropna()
Gibushon_Grade_Sofi_Gold_List = Gibushon_Grade_Sofi_Gold_List.dropna()
Gius_Sofi_Gold_List = Gius_Sofi_Gold_List.dropna()
Final_Gibushon_Dico_Gold_List  = Final_Gibushon_Dico_Gold_List .dropna()
Models Development**
Import Packages
#pip install shap
import matplotlib.pyplot as plt
plt.style.use('classic')
%matplotlib inline

import pandas as pd
import numpy as np

from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
from sklearn.tree import  DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR

from sklearn.metrics import mean_squared_error
import sklearn.metrics as metrics 

import seaborn as sns

from prettytable import PrettyTable
import prettytable

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import make_scorer, roc_auc_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.metrics import confusion_matrix

import time
import xgboost as xgb
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score
import pandas as pd
import pickle
import statistics
from statistics import mean
from sklearn import metrics
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import AdaBoostClassifier


from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.datasets import load_boston
from sklearn import linear_model
from sklearn.linear_model import Lasso

import matplotlib.pyplot as plt
plt.style.use('classic')
import seaborn as sns
plt.style.use('seaborn')
get_ipython().run_line_magic('matplotlib', 'inline')

import chart_studio
import re
import cv2

import missingno as msno
import warnings
warnings.filterwarnings("ignore")

import cloudinary
import cloudinary.uploader
import cloudinary.api

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#port csv
#mport pyodbc
#.read_sql_query

import researchpy as rp
import scipy.stats as stats

import pandas as pd
import numpy as np
import scipy.stats as ss
from math import log

import statsmodels.api as sm
import statsmodels.formula.api as smf
#from ydata_profiling import ProfileReport

#import shap
#import eli5
#from eli5.sklearn import PermutationImportance
Final Golden Lists - Target Variables
#Gibushon_Sofi_Gold_List.head()
#Rama_Sofi_Gold_List.head()
#Dapar_Sofi_Gold_List.head()
#Heb_Sofi_Gold_List.head()
#Normot_Sofi_Gold_List.head()
#Personality_Diss_Sofi_Gold_List.head()
Set Relevant Prediction Algoritems & Metrics
Continuous Targets - Ratio  Scale Level
Relevant Prediction Algoritems
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
from sklearn.tree import  DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
Relevant Prediction Metrics
def regressionMetrics(y_true, y_pred):
    res = {'r2_score' : metrics.r2_score(y_true, y_pred),
           'MSE': metrics.mean_squared_error(y_true, y_pred),
           'RMSE': np.sqrt(metrics.mean_squared_error(y_true, y_pred)),
           'MAE': metrics.mean_absolute_error(y_true, y_pred)
          }
    return res


#'MSLE': metrics.mean_squared_log_error(y_true, y_pred)
#'RMSLE': np.sqrt(metrics.mean_squared_log_error(y_true, y_pred))
Dico Targets - Binary Level
Relevant Prediction Algoritems
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
Relevant Prediction Metrics
def classificationMetrics(y, yhat):
    prf1 = metrics.precision_recall_fscore_support(y,yhat)
    res_Dico = {'Accuracy': metrics.accuracy_score(y,yhat),
           'Precision':prf1[0][1],
           'Recall': prf1[1][1],
           'f1-score': prf1[2][1],
           'Log-loss': metrics.log_loss(y,yhat),
           'AUC': metrics.roc_auc_score(y,yhat)
          }
    return res_Dico
Gibushon Final Grade Prediction Models
###Change target variable to categor type

Gibushon_Sofi_Gold_List["Evaluation_Center_Target"] = Gibushon_Sofi_Gold_List["Evaluation_Center_Target"].astype('category')
Gibushon_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 193 entries, 2 to 2225
Data columns (total 8 columns):
 #   Column                           Non-Null Count  Dtype   
---  ------                           --------------  -----   
 0   Education_and_Inteligence_Index  193 non-null    float64 
 1   Job_Motivators_Index             193 non-null    float64 
 2   United_Commander_or_Kazin        193 non-null    float64 
 3   Interests_and_Activities_Index   193 non-null    float64 
 4   Age_Num                          193 non-null    float64 
 5   Hebrew_Meam_Num                  193 non-null    float64 
 6   Temp_Mean_Num                    193 non-null    float64 
 7   Evaluation_Center_Target         193 non-null    category
dtypes: category(1), float64(7)
memory usage: 12.6 KB
re-orginize variables in final file
Gibushon_Sofi_Gold_List = pd.DataFrame(Gibushon_Sofi_Gold_List)
temp_cols=Gibushon_Sofi_Gold_List.columns.tolist()
index=Gibushon_Sofi_Gold_List.columns.get_loc("Evaluation_Center_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Gibushon_Sofi_Gold_List=Gibushon_Sofi_Gold_List[new_cols]
Gibushon_Sofi_Gold_List = pd.DataFrame(Gibushon_Sofi_Gold_List)
temp_cols=Gibushon_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Gibushon_Sofi_Gold_List=Gibushon_Sofi_Gold_List[new_cols]
#Gibushon_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(15)

# Apply random reorder of all rows
Gibushon_Sofi_Gold_List = Gibushon_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Gibushon_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Evaluation_Center_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

#Gibushon_Sofi_Gold_List=Gibushon_Sofi_Gold_List.apply(zscore)
#Gibushon_Sofi_Gold_List.head()
data = pd.DataFrame(Gibushon_Sofi_Gold_List)
#data
#data = pd.DataFrame(Gibushon_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Evaluation_Center_Target"])
X_data = data[data.columns[~data.columns.isin(['Evaluation_Center_Target'])]]
y_data = data['Evaluation_Center_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.45, random_state=5)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (106, 7)
Validation data shape: (86, 7)
Test data shape: (1, 7)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
The models will be tested and compared based on the quality indicators that will be obtained in three testing environments:
Phase A - Basic examination of all models - train-dataset & validation-dataset
models_list_train = pd.DataFrame()
models_list_validation = pd.DataFrame()
Phase B - selected model finetuning - test-dataset
models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Linear Regression
Algorithm Setting
Model_Linear_Regression = LinearRegression()
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Model_Linear_Regression.fit(X_train, y_train)
LinearRegression()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Linear_Regression.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.27, 'MSE': 0.73, 'RMSE': 0.85, 'MAE': 0.69}
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.29, 'MSE': 0.71, 'RMSE': 0.84, 'MAE': 0.69}
model_dict = {'model': "Linear-Regression"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.29  0.71  0.84  0.69
Cross Validation
from sklearn.model_selection import cross_validate
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Model_Linear_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.82046274 0.88515527 0.87362548]
Mean RMSE from cross-validation: 0.8597478313582818
Cross-validated R-squared scores: [0.23720068 0.21144656 0.28524602]
Mean R-squared from cross-validation: 0.24463108791024926
Model_Linear_Regression
LinearRegression()
Linear Reggresion Regulation
ridge
Algorithm Setting
Ridge_Regression = Ridge(alpha=1.0, max_iter=3, solver='auto', random_state=5)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Ridge_Regression.fit(X_train, y_train)
Ridge(max_iter=3, random_state=5)
Model Evaluation on train envairment
y_pred_train_enva = Ridge_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.26, 'MSE': 0.73, 'RMSE': 0.85, 'MAE': 0.7}
Model Evaluation on valid envairment
y_pred_valid_enva = Ridge_Regression.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.29, 'MSE': 0.7, 'RMSE': 0.84, 'MAE': 0.69}
model_dict = {'model': "Ridge-Regression"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.29  0.71  0.84  0.69
1   Ridge-Regression      0.29  0.70  0.84  0.69
Cross Validation
# Cross-validation

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Ridge_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.82193298 0.88474091 0.87453291]
Mean RMSE from cross-validation: 0.8604022664753694
Cross-validated R-squared scores: [0.23446442 0.21218465 0.28376044]
Mean R-squared from cross-validation: 0.24346983821597803
Lasso
Algorithm Setting
Lasso_Gibushon = Lasso(random_state=3)
parameters :
(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Lasso_Gibushon.fit(X_train, y_train)
Lasso(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Lasso_Gibushon.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 0.99, 'RMSE': 1.0, 'MAE': 0.79}
Model Evaluation on valid envairment
y_pred_valid_enva = Lasso_Gibushon.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.0, 'MSE': 1.0, 'RMSE': 1.0, 'MAE': 0.83}
model_dict = {'model': "Lasso-Regression"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.29  0.71  0.84  0.69
1   Ridge-Regression      0.29  0.70  0.84  0.69
2   Lasso-Regression     -0.00  1.00  1.00  0.83
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Lasso_Gibushon, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.92901311 1.01296986 1.0409037 ]
Mean RMSE from cross-validation: 0.9942955589761713
Cross-validated R-squared scores: [ 0.02200585 -0.03272637 -0.01467532]
Mean R-squared from cross-validation: -0.008465277675363314
Elastic_net
Algorithm Setting
Elastic_Net = ElasticNet(random_state=3)
parameters :
(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Elastic_Net.fit(X_train, y_train)
ElasticNet(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Elastic_Net.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.02, 'MSE': 0.97, 'RMSE': 0.99, 'MAE': 0.78}
Model Evaluation on valid envairment
y_pred_valid_enva = Elastic_Net.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.05, 'MSE': 0.94, 'RMSE': 0.97, 'MAE': 0.81}
model_dict = {'model': "Elastic_Net"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.29  0.71  0.84  0.69
1   Ridge-Regression      0.29  0.70  0.84  0.69
2   Lasso-Regression     -0.00  1.00  1.00  0.83
3        Elastic_Net      0.05  0.94  0.97  0.81
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Elastic_Net, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.92074404 1.00986541 1.00424902]
Mean RMSE from cross-validation: 0.9782861576182557
Cross-validated R-squared scores: [ 0.03933848 -0.02640607  0.05552857]
Mean R-squared from cross-validation: 0.022820323243095058
Random Forrest
Algorithm Setting
Random_Forest = RandomForestRegressor(random_state=3,bootstrap=True)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Random_Forest.fit(X_train, y_train)
RandomForestRegressor(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.85, 'MSE': 0.15, 'RMSE': 0.39, 'MAE': 0.3}
Model Evaluation on valid envairment
y_pred_valid_enva = Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.28, 'MSE': 0.72, 'RMSE': 0.85, 'MAE': 0.68}
model_dict = {'model': "Random_Forest"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.29  0.71  0.84  0.69
1   Ridge-Regression      0.29  0.70  0.84  0.69
2   Lasso-Regression     -0.00  1.00  1.00  0.83
3        Elastic_Net      0.05  0.94  0.97  0.81
4      Random_Forest      0.28  0.72  0.85  0.68
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Random_Forest, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.79230919 0.90697751 0.86002475]
Mean RMSE from cross-validation: 0.8531038166795755
Cross-validated R-squared scores: [0.28865227 0.17208595 0.30732758]
Mean R-squared from cross-validation: 0.2560219345219765
AdaBoost-Regressor
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
AdaBoost_Regressor = AdaBoostRegressor()
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
AdaBoost_Regressor.fit(X_train, y_train)
AdaBoostRegressor()
Model Evaluation on train envairment
y_pred_train_enva = AdaBoost_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.58, 'MSE': 0.42, 'RMSE': 0.65, 'MAE': 0.56}
Model Evaluation on valid envairment
y_pred_valid_enva = AdaBoost_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.19, 'MSE': 0.81, 'RMSE': 0.9, 'MAE': 0.71}
model_dict = {'model': "AdaBoost_Regressor"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.29  0.71  0.84  0.69
1    Ridge-Regression      0.29  0.70  0.84  0.69
2    Lasso-Regression     -0.00  1.00  1.00  0.83
3         Elastic_Net      0.05  0.94  0.97  0.81
4       Random_Forest      0.28  0.72  0.85  0.68
5  AdaBoost_Regressor      0.19  0.81  0.90  0.71
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(AdaBoost_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.82632441 0.90448041 0.91054281]
Mean RMSE from cross-validation: 0.8804492102081646
Cross-validated R-squared scores: [0.22626236 0.17663852 0.22356209]
Mean R-squared from cross-validation: 0.20882098622835055
Gradient Boosting Machine
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
Gradient_Boosting = GradientBoostingRegressor()
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Gradient_Boosting.fit(X_train, y_train)
GradientBoostingRegressor()
Model Evaluation on train envairment
y_pred_train_enva = Gradient_Boosting.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.91, 'MSE': 0.09, 'RMSE': 0.3, 'MAE': 0.23}
Model Evaluation on valid envairment
y_pred_valid_enva = Gradient_Boosting.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.14, 'MSE': 0.85, 'RMSE': 0.92, 'MAE': 0.77}
model_dict = {'model': "Gradient_Boosting"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.29  0.71  0.84  0.69
1    Ridge-Regression      0.29  0.70  0.84  0.69
2    Lasso-Regression     -0.00  1.00  1.00  0.83
3         Elastic_Net      0.05  0.94  0.97  0.81
4       Random_Forest      0.28  0.72  0.85  0.68
5  AdaBoost_Regressor      0.19  0.81  0.90  0.71
6   Gradient_Boosting      0.14  0.85  0.92  0.77
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Gradient_Boosting, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.85087431 0.96265315 0.9131777 ]
Mean RMSE from cross-validation: 0.9089017202724446
Cross-validated R-squared scores: [0.17960427 0.06732166 0.21906195]
Mean R-squared from cross-validation: 0.15532929366637158

kNN
Algorithm Setting
Knn_model = KNeighborsRegressor(3)
parameters :
(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Knn_model.fit(X_train, y_train)
KNeighborsRegressor(n_neighbors=3)
Model Evaluation on train envairment
y_pred_train_enva = Knn_model.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.34, 'MSE': 0.66, 'RMSE': 0.81, 'MAE': 0.67}
Model Evaluation on valid envairment
y_pred_valid_enva = Knn_model.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.11, 'MSE': 0.89, 'RMSE': 0.94, 'MAE': 0.73}
model_dict = {'model': "Knn-model"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.29  0.71  0.84  0.69
1    Ridge-Regression      0.29  0.70  0.84  0.69
2    Lasso-Regression     -0.00  1.00  1.00  0.83
3         Elastic_Net      0.05  0.94  0.97  0.81
4       Random_Forest      0.28  0.72  0.85  0.68
5  AdaBoost_Regressor      0.19  0.81  0.90  0.71
6   Gradient_Boosting      0.14  0.85  0.92  0.77
7           Knn-model      0.11  0.89  0.94  0.73
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Knn_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.00277393 1.06331668 1.1918779 ]
Mean RMSE from cross-validation: 1.0859895016876586
Cross-validated R-squared scores: [-0.13945882 -0.13793504 -0.33036106]
Mean R-squared from cross-validation: -0.20258497143769763
SVM Model
Algorithm Setting
from sklearn.svm import SVR
SVM_model = SVR()
parameters :
(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]
Training Scores
Model Fit Base on Train Set Only :
SVM_model.fit(X_train, y_train)
SVR()
Model Evaluation on train envairment
y_pred_train_enva = SVM_model.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.09, 'MSE': 0.91, 'RMSE': 0.95, 'MAE': 0.76}
Model Evaluation on valid envairment
y_pred_valid_enva = SVM_model.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Evaluation_Center_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.12, 'MSE': 0.88, 'RMSE': 0.94, 'MAE': 0.77}
model_dict = {'model': "SVM_model"}
models_list_validation = models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.29  0.71  0.84  0.69
1    Ridge-Regression      0.29  0.70  0.84  0.69
2    Lasso-Regression     -0.00  1.00  1.00  0.83
3         Elastic_Net      0.05  0.94  0.97  0.81
4       Random_Forest      0.28  0.72  0.85  0.68
5  AdaBoost_Regressor      0.19  0.81  0.90  0.71
6   Gradient_Boosting      0.14  0.85  0.92  0.77
7           Knn-model      0.11  0.89  0.94  0.73
8           SVM_model      0.12  0.88  0.94  0.77
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(SVM_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.92377516 0.98423868 0.95642997]
Mean RMSE from cross-validation: 0.9548146023599969
Cross-validated R-squared scores: [0.033003   0.02502592 0.1433324 ]
Mean R-squared from cross-validation: 0.06712043754339925
Selecting the Highest-Quality Model
models_list_validation.sort_values('r2_score',ascending=False).round(2)
                model  r2_score   MSE  RMSE   MAE
1    Ridge-Regression      0.29  0.70  0.84  0.69
0   Linear-Regression      0.29  0.71  0.84  0.69
4       Random_Forest      0.28  0.72  0.85  0.68
5  AdaBoost_Regressor      0.19  0.81  0.90  0.71
6   Gradient_Boosting      0.14  0.85  0.92  0.77
8           SVM_model      0.12  0.88  0.94  0.77
7           Knn-model      0.11  0.89  0.94  0.73
3         Elastic_Net      0.05  0.94  0.97  0.81
2    Lasso-Regression     -0.00  1.00  1.00  0.83
selected_columns = ['model', 'r2_score', 'MSE', 'RMSE', 'MAE']
validation_ranked_list = models_list_validation[selected_columns];

column_mapping = {'r2_score': 'r2_score_valid_set','MSE': 'MSE_valid_set','RMSE': 'RMSE_valid_set','MAE': 'MAE_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.289297       0.708685        0.841834   
1    Ridge-Regression            0.293022       0.704970        0.839625   
2    Lasso-Regression           -0.003611       1.000761        1.000381   
3         Elastic_Net            0.054899       0.942418        0.970782   
4       Random_Forest            0.278099       0.719851        0.848440   
5  AdaBoost_Regressor            0.188914       0.808783        0.899324   
6   Gradient_Boosting            0.143989       0.853581        0.923894   
7           Knn-model            0.105665       0.891796        0.944349   
8           SVM_model            0.122460       0.875048        0.935440   

   MAE_valid_set  
0       0.689571  
1       0.686849  
2       0.830957  
3       0.812290  
4       0.677616  
5       0.711777  
6       0.766477  
7       0.730620  
8       0.767987  
validation_ranked_list[['MSE_valid_set', 'RMSE_valid_set', 'MAE_valid_set']] *= -1
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.289297      -0.708685       -0.841834   
1    Ridge-Regression            0.293022      -0.704970       -0.839625   
2    Lasso-Regression           -0.003611      -1.000761       -1.000381   
3         Elastic_Net            0.054899      -0.942418       -0.970782   
4       Random_Forest            0.278099      -0.719851       -0.848440   
5  AdaBoost_Regressor            0.188914      -0.808783       -0.899324   
6   Gradient_Boosting            0.143989      -0.853581       -0.923894   
7           Knn-model            0.105665      -0.891796       -0.944349   
8           SVM_model            0.122460      -0.875048       -0.935440   

   MAE_valid_set  
0      -0.689571  
1      -0.686849  
2      -0.830957  
3      -0.812290  
4      -0.677616  
5      -0.711777  
6      -0.766477  
7      -0.730620  
8      -0.767987  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 8.0            8.0             8.0   
1    Ridge-Regression                 9.0            9.0             9.0   
2    Lasso-Regression                 1.0            1.0             1.0   
3         Elastic_Net                 2.0            2.0             2.0   
4       Random_Forest                 7.0            7.0             7.0   
5  AdaBoost_Regressor                 6.0            6.0             6.0   
6   Gradient_Boosting                 5.0            5.0             5.0   
7           Knn-model                 3.0            3.0             3.0   
8           SVM_model                 4.0            4.0             4.0   

   MAE_valid_set  
0            7.0  
1            8.0  
2            1.0  
3            2.0  
4            9.0  
5            6.0  
6            4.0  
7            5.0  
8            3.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
1    Ridge-Regression                 9.0            9.0             9.0   
0   Linear-Regression                 8.0            8.0             8.0   
4       Random_Forest                 7.0            7.0             7.0   
5  AdaBoost_Regressor                 6.0            6.0             6.0   
6   Gradient_Boosting                 5.0            5.0             5.0   
8           SVM_model                 4.0            4.0             4.0   
7           Knn-model                 3.0            3.0             3.0   
3         Elastic_Net                 2.0            2.0             2.0   
2    Lasso-Regression                 1.0            1.0             1.0   

   MAE_valid_set  Combined_Column  
1            8.0             35.0  
0            7.0             31.0  
4            9.0             30.0  
5            6.0             24.0  
6            4.0             19.0  
8            3.0             15.0  
7            5.0             14.0  
3            2.0              8.0  
2            1.0              4.0  
Selected Model - Fine_Tuning With Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Linear Regression model
Final_Model_Linear_Regression = Ridge()

param_grid = {
    'alpha': [0.1, 1.0, 10.0],  # Regularization strength
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'copy_X': [True, False],  # Whether to copy X data before fitting
    'max_iter': [1000, 2000],  # Maximum number of iterations
}

# Create GridSearchCV object
Gibushon_Final_Model_Linear_Regression = GridSearchCV(Final_Model_Linear_Regression, param_grid, scoring='neg_mean_squared_error', cv=3)

# Fit the model with grid search
Gibushon_Final_Model_Linear_Regression.fit(X_data, y_data)

# Best parameters from the grid search
best_params = Gibushon_Final_Model_Linear_Regression.best_params_
print(f"Best parameters: {best_params}")

# Best model from the grid search
best_model = Gibushon_Final_Model_Linear_Regression.best_estimator_

# Evaluate the best model on training data
y_pred_train_enva = best_model.predict(X_train)

# Display scatter plot for training data
sns.scatterplot(y_pred_train_enva, y_train)

# Evaluate metrics for training data
metrics_train = regressionMetrics(y_train, y_pred_train_enva)
formatted_metrics_train = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_train.items()}
print("Metrics on training data:")
print(formatted_metrics_train)

# Evaluate the best model on validation data
y_pred_valid_enva = best_model.predict(X_valid)

# Display scatter plot for validation data
sns.scatterplot(y_pred_valid_enva, y_valid)

# Evaluate metrics for validation data
metrics_valid = regressionMetrics(y_valid, y_pred_valid_enva)
formatted_metrics_valid = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_valid.items()}
print("Metrics on validation data:")
print(formatted_metrics_valid)

# Cross-validate the best model
cv_scores = cross_validate(best_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Best parameters: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': False, 'max_iter': 1000, 'normalize': True}
Metrics on training data:
{'r2_score': 0.25, 'MSE': 0.75, 'RMSE': 0.87, 'MAE': 0.69}
Metrics on validation data:
{'r2_score': 0.37, 'MSE': 0.63, 'RMSE': 0.79, 'MAE': 0.64}
Cross-validated RMSE scores: [0.8249009  0.885691   0.82529588]
Mean RMSE from cross-validation: 0.8452959281914687
Cross-validated R-squared scores: [0.2289259  0.21049173 0.36214003]
Mean R-squared from cross-validation: 0.26718588447577746
 
Save Gibushon final grade selected model - Auto Set Hyper Parameters
#import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
#joblib.dump(Gibushon_Final_Model_Linear_Regression, 'Gibushon_Final_Model_Linear_Regression.pkl')
Final Algorithm Setting
Gibushon_Final_Model_Linear_Regression = Ridge(alpha=0.1, copy_X=True, fit_intercept=False, max_iter=1000, normalize=True)
Gibushon_Final_Model_Linear_Regression.fit(X_train, y_train)
Ridge(alpha=0.1, fit_intercept=False, max_iter=1000, normalize=True)
###Model Evaluation on train envairment

y_pred_train_enva = Gibushon_Final_Model_Linear_Regression.predict(X_train)

Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.27, 'MSE': 0.73, 'RMSE': 0.85, 'MAE': 0.69}
Model Evaluation on valid envairment
###Model Evaluation on valid envairment

y_pred_valid_enva = Gibushon_Final_Model_Linear_Regression.predict(X_valid)

Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.29, 'MSE': 0.71, 'RMSE': 0.84, 'MAE': 0.69}
Save Gibushon final grade selected model - Constant Set Hyper Parameters
import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
joblib.dump(Gibushon_Final_Model_Linear_Regression, 'Gibushon_Final_Model_Linear_Regression.pkl')
['Gibushon_Final_Model_Linear_Regression.pkl']
Rama Final Grade Prediction Models
###Change target variable to categor type

Rama_Sofi_Gold_List["RAMA_Target"] = Rama_Sofi_Gold_List["RAMA_Target"].astype('category')
#Rama_Sofi_Gold_List.info()
re-orginize variables in final file
Rama_Sofi_Gold_List = pd.DataFrame(Rama_Sofi_Gold_List)
temp_cols=Rama_Sofi_Gold_List.columns.tolist()
index=Rama_Sofi_Gold_List.columns.get_loc("RAMA_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Rama_Sofi_Gold_List=Rama_Sofi_Gold_List[new_cols]
Rama_Sofi_Gold_List = pd.DataFrame(Rama_Sofi_Gold_List)
temp_cols=Rama_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Rama_Sofi_Gold_List=Rama_Sofi_Gold_List[new_cols]
#Rama_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(15)

# Apply random reorder of all rows
Rama_Sofi_Gold_List = Rama_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Rama_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "RAMA_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

#Rama_Sofi_Gold_List=Rama_Sofi_Gold_List.apply(zscore)
#Rama_Sofi_Gold_List.head()
data = pd.DataFrame(Rama_Sofi_Gold_List)
#data
#data = pd.DataFrame(Rama_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "RAMA_Target"])
X_data = data[data.columns[~data.columns.isin(['RAMA_Target'])]]
y_data = data['RAMA_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.5, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=42)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (336, 9)
Validation data shape: (335, 9)
Test data shape: (1, 9)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Rama_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Linear Regression
Algorithm Setting
from sklearn.linear_model import LinearRegression

# Create a LinearRegression object
Model_Linear_Regression = LinearRegression()
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Model_Linear_Regression.fit(X_train, y_train)
LinearRegression()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Linear_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.16, 'MSE': 0.47, 'RMSE': 0.69, 'MAE': 0.54}
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.22, 'MSE': 0.4, 'RMSE': 0.63, 'MAE': 0.5}
model_dict = {'model': "Linear-Regression"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score  MSE  RMSE  MAE
0  Linear-Regression      0.22  0.4  0.63  0.5
Cross Validation
from sklearn.model_selection import cross_validate
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Model_Linear_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.68604853 0.68532195 0.62498364]
Mean RMSE from cross-validation: 0.6654513732420813
Cross-validated R-squared scores: [0.22105149 0.05946799 0.21915662]
Mean R-squared from cross-validation: 0.16655870037629808
Linear Reggresion Regulation
ridge
Algorithm Setting
Ridge_Regression = Ridge(random_state=5)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Ridge_Regression.fit(X_train, y_train)
Ridge(random_state=5)
Model Evaluation on train envairment
y_pred_train_enva = Ridge_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.16, 'MSE': 0.47, 'RMSE': 0.69, 'MAE': 0.54}
Model Evaluation on valid envairment
y_pred_valid_enva = Ridge_Regression.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.22, 'MSE': 0.4, 'RMSE': 0.63, 'MAE': 0.5}
model_dict = {'model': "Ridge-Regression"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score  MSE  RMSE  MAE
0  Linear-Regression      0.22  0.4  0.63  0.5
1   Ridge-Regression      0.22  0.4  0.63  0.5
Cross Validation
# Cross-validation

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Ridge_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.6863774  0.68497431 0.6249005 ]
Mean RMSE from cross-validation: 0.6654174010581653
Cross-validated R-squared scores: [0.22030452 0.06042195 0.21936434]
Mean R-squared from cross-validation: 0.16669694020371717
Lasso
Algorithm Setting
from sklearn.linear_model import Lasso

# Create an instance of the Lasso model with random_state
lasso_rama = Lasso(random_state=3)
parameters :
(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
lasso_rama.fit(X_train, y_train)
Lasso(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = lasso_rama.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 0.56, 'RMSE': 0.75, 'MAE': 0.58}
Model Evaluation on valid envairment
y_pred_valid_enva = lasso_rama.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.0, 'MSE': 0.51, 'RMSE': 0.72, 'MAE': 0.56}
model_dict = {'model': "Lasso-Regression"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.22  0.40  0.63  0.50
1   Ridge-Regression      0.22  0.40  0.63  0.50
2   Lasso-Regression     -0.00  0.51  0.72  0.56
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(lasso_rama, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.77853882 0.70820866 0.70728027]
Mean RMSE from cross-validation: 0.7313425865923208
Cross-validated R-squared scores: [-3.13553941e-03 -4.40014368e-03 -2.24105818e-05]
Mean R-squared from cross-validation: -0.002519364557223384
Elastic_net
Algorithm Setting
Elastic_Net = ElasticNet(random_state=3)
parameters :
(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Elastic_Net.fit(X_train, y_train)
ElasticNet(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Elastic_Net.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 0.56, 'RMSE': 0.75, 'MAE': 0.58}
Model Evaluation on valid envairment
y_pred_valid_enva = Elastic_Net.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.0, 'MSE': 0.51, 'RMSE': 0.72, 'MAE': 0.56}
model_dict = {'model': "Elastic_Net"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.22  0.40  0.63  0.50
1   Ridge-Regression      0.22  0.40  0.63  0.50
2   Lasso-Regression     -0.00  0.51  0.72  0.56
3        Elastic_Net     -0.00  0.51  0.72  0.56
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Elastic_Net, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.77853882 0.70820866 0.70728027]
Mean RMSE from cross-validation: 0.7313425865923208
Cross-validated R-squared scores: [-3.13553941e-03 -4.40014368e-03 -2.24105818e-05]
Mean R-squared from cross-validation: -0.002519364557223384
Random Forrest
Algorithm Setting
Random_Forest = RandomForestRegressor(n_estimators=15,  # Number of trees in the forest
                                      max_depth=10,      # Maximum depth of the trees
                                      min_samples_split=15,  # Minimum number of samples required to split an internal node
                                      min_samples_leaf=15,   # Minimum number of samples required to be at a leaf node
                                      max_features='sqrt',  # Number of features to consider when looking for the best split
                                      random_state=10)      # Random state for reproducibility
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Random_Forest.fit(X_train, y_train)
RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=15,
                      min_samples_split=15, n_estimators=15, random_state=10)
Model Evaluation on train envairment
y_pred_train_enva = Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.16, 'MSE': 0.47, 'RMSE': 0.69, 'MAE': 0.54}
Model Evaluation on valid envairment
y_pred_valid_enva = Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.14, 'MSE': 0.44, 'RMSE': 0.66, 'MAE': 0.54}
model_dict = {'model': "Random_Forest"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.22  0.40  0.63  0.50
1   Ridge-Regression      0.22  0.40  0.63  0.50
2   Lasso-Regression     -0.00  0.51  0.72  0.56
3        Elastic_Net     -0.00  0.51  0.72  0.56
4      Random_Forest      0.14  0.44  0.66  0.54
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Random_Forest, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.7271746  0.6873768  0.64326981]
Mean RMSE from cross-validation: 0.6859404028012497
Cross-validated R-squared scores: [0.12486214 0.05381942 0.17279532]
Mean R-squared from cross-validation: 0.11715896052675401
AdaBoost-Regressor
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Initialize DecisionTreeRegressor as the base estimator
base_estimator = DecisionTreeRegressor(max_depth=3)  # Adjust max_depth to control the complexity of the base estimator

# Initialize AdaBoostRegressor with hyperparameters to reduce overfitting
AdaBoost_Regressor = AdaBoostRegressor(base_estimator=base_estimator,
                                       n_estimators=100,     # Number of weak learners (base estimators)
                                       learning_rate=0.1,    # Learning rate shrinks the contribution of each base estimator
                                       loss='exponential',        # The loss function to use (linear, square, exponential)
                                       random_state=42)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
AdaBoost_Regressor.fit(X_train, y_train)
AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),
                  learning_rate=0.1, loss='exponential', n_estimators=100,
                  random_state=42)
Model Evaluation on train envairment
y_pred_train_enva = AdaBoost_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.21, 'MSE': 0.44, 'RMSE': 0.67, 'MAE': 0.53}
Model Evaluation on valid envairment
y_pred_valid_enva = AdaBoost_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.14, 'MSE': 0.44, 'RMSE': 0.66, 'MAE': 0.54}
model_dict = {'model': "AdaBoost_Regressor"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.22  0.40  0.63  0.50
1    Ridge-Regression      0.22  0.40  0.63  0.50
2    Lasso-Regression     -0.00  0.51  0.72  0.56
3         Elastic_Net     -0.00  0.51  0.72  0.56
4       Random_Forest      0.14  0.44  0.66  0.54
5  AdaBoost_Regressor      0.14  0.44  0.66  0.54
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(AdaBoost_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.7387732  0.68542195 0.65630114]
Mean RMSE from cross-validation: 0.6934987637823239
Cross-validated R-squared scores: [0.09672221 0.05919348 0.13894091]
Mean R-squared from cross-validation: 0.09828553306207959
Gradient Boosting Machine
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
# Initialize GradientBoostingRegressor with hyperparameters to reduce overfitting
Gradient_Boosting = GradientBoostingRegressor(n_estimators=100,     # Number of boosting stages to be performed
                                              learning_rate=0.1,    # Learning rate shrinks the contribution of each tree
                                              max_depth=3,          # Maximum depth of the individual trees
                                              min_samples_split=25,  # Minimum number of samples required to split an internal node
                                              min_samples_leaf=25,   # Minimum number of samples required to be at a leaf node
                                              max_features='sqrt',  # Number of features to consider when looking for the best split
                                              loss='ls',            # Loss function to be optimized ('ls' refers to least squares regression)
                                              random_state=17)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Gradient_Boosting.fit(X_train, y_train)
GradientBoostingRegressor(loss='ls', max_features='sqrt', min_samples_leaf=25,
                          min_samples_split=25, random_state=17)
Model Evaluation on train envairment
y_pred_train_enva = Gradient_Boosting.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.24, 'MSE': 0.42, 'RMSE': 0.65, 'MAE': 0.5}
Model Evaluation on valid envairment
y_pred_valid_enva = Gradient_Boosting.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.15, 'MSE': 0.43, 'RMSE': 0.66, 'MAE': 0.53}
model_dict = {'model': "Gradient_Boosting"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.22  0.40  0.63  0.50
1    Ridge-Regression      0.22  0.40  0.63  0.50
2    Lasso-Regression     -0.00  0.51  0.72  0.56
3         Elastic_Net     -0.00  0.51  0.72  0.56
4       Random_Forest      0.14  0.44  0.66  0.54
5  AdaBoost_Regressor      0.14  0.44  0.66  0.54
6   Gradient_Boosting      0.15  0.43  0.66  0.53
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Gradient_Boosting, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.73156962 0.67894529 0.63173069]
Mean RMSE from cross-validation: 0.6807485348183567
Cross-validated R-squared scores: [0.11425156 0.07688915 0.20220631]
Mean R-squared from cross-validation: 0.13111567388012013

kNN
Algorithm Setting
Knn_model = KNeighborsRegressor(n_neighbors=5,  # Number of neighbors to consider
                                weights='uniform',  # Weight function used in prediction ('uniform' for equal weights)
                                algorithm='brute',  # Algorithm used to compute the nearest neighbors ('auto' for automatic selection)
                                leaf_size=30,      # Leaf size passed to BallTree or KDTree
                                p=2)               # Power parameter for the Minkowski metric (2 for Euclidean distance)
parameters :
(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Knn_model.fit(X_train, y_train)
KNeighborsRegressor(algorithm='brute')
Model Evaluation on train envairment
y_pred_train_enva = Knn_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.28, 'MSE': 0.4, 'RMSE': 0.63, 'MAE': 0.49}
Model Evaluation on valid envairment
y_pred_valid_enva = Knn_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.06, 'MSE': 0.48, 'RMSE': 0.69, 'MAE': 0.55}
model_dict = {'model': "Knn-model"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.22  0.40  0.63  0.50
1    Ridge-Regression      0.22  0.40  0.63  0.50
2    Lasso-Regression     -0.00  0.51  0.72  0.56
3         Elastic_Net     -0.00  0.51  0.72  0.56
4       Random_Forest      0.14  0.44  0.66  0.54
5  AdaBoost_Regressor      0.14  0.44  0.66  0.54
6   Gradient_Boosting      0.15  0.43  0.66  0.53
7           Knn-model      0.06  0.48  0.69  0.55
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Knn_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.76321688 0.73839232 0.70758013]
Mean RMSE from cross-validation: 0.7363964417354584
Cross-validated R-squared scores: [ 0.03596012 -0.09183908 -0.00087053]
Mean R-squared from cross-validation: -0.018916494896027314
SVM Model
Algorithm Setting
from sklearn.svm import SVR
SVM_model = SVR(kernel='rbf',    # Kernel function ('rbf' for radial basis function)
                C=1.0,          # Regularization parameter
                epsilon=0.1,    # Epsilon in the epsilon-insensitive loss function
                gamma='scale', # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'
                degree=3)       # Degree of the polynomial kernel function (for 'poly' kernel)
parameters :
(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]
Training Scores
Model Fit Base on Train Set Only :
SVM_model.fit(X_train, y_train)
SVR()
Model Evaluation on train envairment
y_pred_train_enva = SVM_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.18, 'MSE': 0.46, 'RMSE': 0.68, 'MAE': 0.51}
Model Evaluation on valid envairment
y_pred_valid_enva = SVM_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.16, 'MSE': 0.43, 'RMSE': 0.65, 'MAE': 0.52}
model_dict = {'model': "SVM_model"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.22  0.40  0.63  0.50
1    Ridge-Regression      0.22  0.40  0.63  0.50
2    Lasso-Regression     -0.00  0.51  0.72  0.56
3         Elastic_Net     -0.00  0.51  0.72  0.56
4       Random_Forest      0.14  0.44  0.66  0.54
5  AdaBoost_Regressor      0.14  0.44  0.66  0.54
6   Gradient_Boosting      0.15  0.43  0.66  0.53
7           Knn-model      0.06  0.48  0.69  0.55
8           SVM_model      0.16  0.43  0.65  0.52
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(SVM_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.69445247 0.682266   0.63414229]
Mean RMSE from cross-validation: 0.6702869225158329
Cross-validated R-squared scores: [0.20185072 0.06783722 0.19610361]
Mean R-squared from cross-validation: 0.1552638507856695
SGD Model
Algorithm Setting
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor(alpha=0.1,      # Regularization strength (L2 penalty)
                              penalty='l2',    # Penalty term ('l2' for L2 regularization)
                              max_iter=5,   # Maximum number of iterations
                              tol=1e-3,        # Tolerance for stopping criteria
                              random_state=10) # Random state for reproducibility
parameters :
(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)[source]¶
Model Fit Base on Train Set Only :
SGD_Regressor.fit(X_train, y_train)
SGDRegressor(alpha=0.1, max_iter=5, random_state=10)
Model Evaluation on train envairment
y_pred_train_enva = SGD_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.09, 'MSE': 0.61, 'RMSE': 0.78, 'MAE': 0.61}
Model Evaluation on valid envairment
y_pred_valid_enva = SGD_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.07, 'MSE': 0.55, 'RMSE': 0.74, 'MAE': 0.61}
model_dict = {'model': "SGD_Regressor"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.22  0.40  0.63  0.50
1    Ridge-Regression      0.22  0.40  0.63  0.50
2    Lasso-Regression     -0.00  0.51  0.72  0.56
3         Elastic_Net     -0.00  0.51  0.72  0.56
4       Random_Forest      0.14  0.44  0.66  0.54
5  AdaBoost_Regressor      0.14  0.44  0.66  0.54
6   Gradient_Boosting      0.15  0.43  0.66  0.53
7           Knn-model      0.06  0.48  0.69  0.55
8           SVM_model      0.16  0.43  0.65  0.52
9       SGD_Regressor     -0.07  0.55  0.74  0.61
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(SGD_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.74574412 0.7357629  0.67818065]
Mean RMSE from cross-validation: 0.7198958892128092
Cross-validated R-squared scores: [ 0.07959547 -0.08407683  0.08057263]
Mean R-squared from cross-validation: 0.02536375648253301
Selecting the Highest-Quality Model
Rama_models_list_validation.sort_values('r2_score',ascending=False).round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.22  0.40  0.63  0.50
1    Ridge-Regression      0.22  0.40  0.63  0.50
8           SVM_model      0.16  0.43  0.65  0.52
6   Gradient_Boosting      0.15  0.43  0.66  0.53
4       Random_Forest      0.14  0.44  0.66  0.54
5  AdaBoost_Regressor      0.14  0.44  0.66  0.54
7           Knn-model      0.06  0.48  0.69  0.55
2    Lasso-Regression     -0.00  0.51  0.72  0.56
3         Elastic_Net     -0.00  0.51  0.72  0.56
9       SGD_Regressor     -0.07  0.55  0.74  0.61
selected_columns = ['model', 'r2_score', 'MSE', 'RMSE', 'MAE']
validation_ranked_list = Rama_models_list_validation[selected_columns];

column_mapping = {'r2_score': 'r2_score_valid_set','MSE': 'MSE_valid_set','RMSE': 'RMSE_valid_set','MAE': 'MAE_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.221303       0.398414        0.631200   
1    Ridge-Regression            0.220880       0.398630        0.631372   
2    Lasso-Regression           -0.001766       0.512545        0.715923   
3         Elastic_Net           -0.001766       0.512545        0.715923   
4       Random_Forest            0.141110       0.439444        0.662906   
5  AdaBoost_Regressor            0.138113       0.440977        0.664061   
6   Gradient_Boosting            0.151619       0.434067        0.658838   
7           Knn-model            0.063244       0.479284        0.692303   
8           SVM_model            0.163651       0.427911        0.654149   
9       SGD_Regressor           -0.072034       0.548498        0.740606   

   MAE_valid_set  
0       0.499416  
1       0.499862  
2       0.561456  
3       0.561456  
4       0.535461  
5       0.537740  
6       0.526221  
7       0.552836  
8       0.521830  
9       0.607451  
validation_ranked_list[['MSE_valid_set', 'RMSE_valid_set', 'MAE_valid_set']] *= -1
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.221303      -0.398414       -0.631200   
1    Ridge-Regression            0.220880      -0.398630       -0.631372   
2    Lasso-Regression           -0.001766      -0.512545       -0.715923   
3         Elastic_Net           -0.001766      -0.512545       -0.715923   
4       Random_Forest            0.141110      -0.439444       -0.662906   
5  AdaBoost_Regressor            0.138113      -0.440977       -0.664061   
6   Gradient_Boosting            0.151619      -0.434067       -0.658838   
7           Knn-model            0.063244      -0.479284       -0.692303   
8           SVM_model            0.163651      -0.427911       -0.654149   
9       SGD_Regressor           -0.072034      -0.548498       -0.740606   

   MAE_valid_set  
0      -0.499416  
1      -0.499862  
2      -0.561456  
3      -0.561456  
4      -0.535461  
5      -0.537740  
6      -0.526221  
7      -0.552836  
8      -0.521830  
9      -0.607451  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                10.0           10.0            10.0   
1    Ridge-Regression                 9.0            9.0             9.0   
2    Lasso-Regression                 2.0            2.0             2.0   
3         Elastic_Net                 2.0            2.0             2.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 5.0            5.0             5.0   
6   Gradient_Boosting                 7.0            7.0             7.0   
7           Knn-model                 4.0            4.0             4.0   
8           SVM_model                 8.0            8.0             8.0   
9       SGD_Regressor                 1.0            1.0             1.0   

   MAE_valid_set  
0           10.0  
1            9.0  
2            2.0  
3            2.0  
4            6.0  
5            5.0  
6            7.0  
7            4.0  
8            8.0  
9            1.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                10.0           10.0            10.0   
1    Ridge-Regression                 9.0            9.0             9.0   
8           SVM_model                 8.0            8.0             8.0   
6   Gradient_Boosting                 7.0            7.0             7.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 5.0            5.0             5.0   
7           Knn-model                 4.0            4.0             4.0   
2    Lasso-Regression                 2.0            2.0             2.0   
3         Elastic_Net                 2.0            2.0             2.0   
9       SGD_Regressor                 1.0            1.0             1.0   

   MAE_valid_set  Combined_Column  
0           10.0             40.0  
1            9.0             36.0  
8            8.0             32.0  
6            7.0             28.0  
4            6.0             24.0  
5            5.0             20.0  
7            4.0             16.0  
2            2.0              8.0  
3            2.0              8.0  
9            1.0              4.0  
Selected Model - Fine_Tuning With Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Linear Regression model
Final_Model_Linear_Regression = Ridge()

# Define the parameter grid for GridSearchCV
param_grid = {
    'alpha': [0.1, 1.0, 10.0],  # Regularization strength
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'copy_X': [True, False],  # Whether to copy X data before fitting
    'max_iter': [1000, 2000],  # Maximum number of iterations
}

# Create GridSearchCV object
Rama_Final_Model_Linear_Regression = GridSearchCV(Final_Model_Linear_Regression, param_grid, scoring='neg_mean_squared_error', cv=3)

# Fit the model with grid search
Rama_Final_Model_Linear_Regression.fit(X_data, y_data)

# Best parameters from the grid search
best_params = Rama_Final_Model_Linear_Regression.best_params_
print(f"Best parameters: {best_params}")

# Best model from the grid search
best_model = Rama_Final_Model_Linear_Regression.best_estimator_

# Evaluate the best model on training data
y_pred_train_enva = best_model.predict(X_train)

# Display scatter plot for training data
sns.scatterplot(y_pred_train_enva, y_train)

# Evaluate metrics for training data
metrics_train = regressionMetrics(y_train, y_pred_train_enva)
formatted_metrics_train = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_train.items()}
print("Metrics on training data:")
print(formatted_metrics_train)

# Evaluate the best model on validation data
y_pred_valid_enva = best_model.predict(X_valid)

# Display scatter plot for validation data
#sns.scatterplot(y_pred_valid_enva, y_valid)

# Evaluate metrics for validation data
metrics_valid = regressionMetrics(y_valid, y_pred_valid_enva)
formatted_metrics_valid = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_valid.items()}
print("Metrics on validation data:")
print(formatted_metrics_valid)

# Cross-validate the best model
cv_scores = cross_validate(best_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Best parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': False}
Metrics on training data:
{'r2_score': 0.16, 'MSE': 0.47, 'RMSE': 0.69, 'MAE': 0.54}
Metrics on validation data:
{'r2_score': 0.23, 'MSE': 0.39, 'RMSE': 0.63, 'MAE': 0.5}
Cross-validated RMSE scores: [0.6863774  0.68497431 0.6249005 ]
Mean RMSE from cross-validation: 0.6654174010581653
Cross-validated R-squared scores: [0.22030452 0.06042195 0.21936434]
Mean R-squared from cross-validation: 0.16669694020371717
 
Save Rama grade selected model - Auto Set Hyper Parameters
#import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
#joblib.dump(Rama_Final_Model_Linear_Regression, 'Rama_Final_Model_Linear_Regression.pkl')
Final Algorithm Setting
Rama_Final_Model_Linear_Regression = Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=True)
Rama_Final_Model_Linear_Regression.fit(X_train, y_train)
Ridge(alpha=0.1, max_iter=1000, normalize=True)
###Model Evaluation on train envairment

y_pred_train_enva = Rama_Final_Model_Linear_Regression.predict(X_train)

Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.16, 'MSE': 0.47, 'RMSE': 0.69, 'MAE': 0.54}
Model Evaluation on valid envairment
###Model Evaluation on valid envairment

y_pred_valid_enva = Rama_Final_Model_Linear_Regression.predict(X_valid)

Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.22, 'MSE': 0.4, 'RMSE': 0.63, 'MAE': 0.5}
Save Rama grade selected model - Constant Set Hyper Parameters
import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
joblib.dump(Rama_Final_Model_Linear_Regression, 'Rama_Final_Model_Linear_Regression.pkl')
['Rama_Final_Model_Linear_Regression.pkl']
Dapar Grade Prediction Models
###Change target variable to categor type

Dapar_Sofi_Gold_List["Dapar_Target"] = Dapar_Sofi_Gold_List["Dapar_Target"].astype('category')
Dapar_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 471 entries, 6 to 2242
Data columns (total 10 columns):
 #   Column                                   Non-Null Count  Dtype   
---  ------                                   --------------  -----   
 0   Dapar_Educational_achievement_index      471 non-null    float64 
 1   Psych_Tests_Subjective_Num               471 non-null    float64 
 2   Kaba_Grade_51_54_Dico                    471 non-null    float64 
 3   Kaba_Grade_55_UP_Dico                    471 non-null    float64 
 4   Married_Dico                             471 non-null    float64 
 5   Special_Unit_Army_Dico                   471 non-null    float64 
 6   Dapar_Job_Motivators_Index               471 non-null    float64 
 7   Dapar_Interests_and_Activities_Index     471 non-null    float64 
 8   Dapar_Hebrew_Failurer_Last_Attempt_Dico  471 non-null    float64 
 9   Dapar_Target                             471 non-null    category
dtypes: category(1), float64(9)
memory usage: 37.6 KB
re-orginize variables in final file
Dapar_Sofi_Gold_List = pd.DataFrame(Dapar_Sofi_Gold_List)
temp_cols=Dapar_Sofi_Gold_List.columns.tolist()
index=Dapar_Sofi_Gold_List.columns.get_loc("Dapar_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Dapar_Sofi_Gold_List=Dapar_Sofi_Gold_List[new_cols]
Dapar_Sofi_Gold_List = pd.DataFrame(Dapar_Sofi_Gold_List)
temp_cols=Dapar_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Dapar_Sofi_Gold_List=Dapar_Sofi_Gold_List[new_cols]
#Dapar_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Dapar_Sofi_Gold_List = Dapar_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Dapar_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Dapar_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

#Dapar_Sofi_Gold_List=Dapar_Sofi_Gold_List.apply(zscore)
#Dapar_Sofi_Gold_List.head()
data = pd.DataFrame(Dapar_Sofi_Gold_List)
#data
#data = pd.DataFrame(Dapar_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Dapar_Target"])
X_data = data[data.columns[~data.columns.isin(['Dapar_Target'])]]
y_data = data['Dapar_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.4, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (282, 9)
Validation data shape: (188, 9)
Test data shape: (1, 9)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Dapar_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Linear Regression
Algorithm Setting
Model_Linear_Regression = LinearRegression()
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Model_Linear_Regression.fit(X_train, y_train)
LinearRegression()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Linear_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.41, 'MSE': 1.85, 'RMSE': 1.36, 'MAE': 1.12}
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.36, 'MSE': 2.15, 'RMSE': 1.47, 'MAE': 1.18}
model_dict = {'model': "Linear-Regression"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.36  2.15  1.47  1.18
Cross Validation
from sklearn.model_selection import cross_validate
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Model_Linear_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.50799281 1.37437203 1.37654577]
Mean RMSE from cross-validation: 1.4196368701155893
Cross-validated R-squared scores: [0.42541701 0.38941618 0.26288028]
Mean R-squared from cross-validation: 0.35923782232536694
Linear Reggresion Regulation
ridge
Algorithm Setting
Ridge_Regression = Ridge(alpha=1.0, max_iter=3, solver='auto', random_state=5)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Ridge_Regression.fit(X_train, y_train)
Ridge(max_iter=3, random_state=5)
Model Evaluation on train envairment
y_pred_train_enva = Ridge_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.41, 'MSE': 1.85, 'RMSE': 1.36, 'MAE': 1.12}
Model Evaluation on valid envairment
y_pred_valid_enva = Ridge_Regression.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.37, 'MSE': 2.13, 'RMSE': 1.46, 'MAE': 1.18}
model_dict = {'model': "Ridge-Regression"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.36  2.15  1.47  1.18
1   Ridge-Regression      0.37  2.13  1.46  1.18
Cross Validation
# Cross-validation

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Ridge_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=5)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.4152061  1.54659434 1.44665878 1.32949302 1.37441956]
Mean RMSE from cross-validation: 1.4224743610248631
Cross-validated R-squared scores: [0.46245926 0.37771446 0.33807596 0.29163249 0.31152197]
Mean R-squared from cross-validation: 0.3562808284123573
Lasso
Algorithm Setting
Lasso_Dapar = Lasso(random_state=3)
parameters :
(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Lasso_Dapar.fit(X_train, y_train)
Lasso(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Lasso_Dapar.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 3.12, 'RMSE': 1.77, 'MAE': 1.43}
Model Evaluation on valid envairment
y_pred_valid_enva = Lasso_Dapar.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.01, 'MSE': 3.4, 'RMSE': 1.84, 'MAE': 1.44}
model_dict = {'model': "Lasso-Regression"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.36  2.15  1.47  1.18
1   Ridge-Regression      0.37  2.13  1.46  1.18
2   Lasso-Regression     -0.01  3.40  1.84  1.44
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Lasso_Dapar, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.99308106 1.76962791 1.57174455]
Mean RMSE from cross-validation: 1.778151173004967
Cross-validated R-squared scores: [-0.00370051 -0.01227985  0.03900607]
Mean R-squared from cross-validation: 0.007675233014674358
Elastic_net
Algorithm Setting
Elastic_Net = ElasticNet(random_state=3)
parameters :
(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Elastic_Net.fit(X_train, y_train)
ElasticNet(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Elastic_Net.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.11, 'MSE': 2.77, 'RMSE': 1.66, 'MAE': 1.37}
Model Evaluation on valid envairment
y_pred_valid_enva = Elastic_Net.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.11, 'MSE': 2.99, 'RMSE': 1.73, 'MAE': 1.37}
model_dict = {'model': "Elastic_Net"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.36  2.15  1.47  1.18
1   Ridge-Regression      0.37  2.13  1.46  1.18
2   Lasso-Regression     -0.01  3.40  1.84  1.44
3        Elastic_Net      0.11  2.99  1.73  1.37
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Elastic_Net, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.89346873 1.65854676 1.51029232]
Mean RMSE from cross-validation: 1.6874359384422697
Cross-validated R-squared scores: [0.09412036 0.11081502 0.11268311]
Mean R-squared from cross-validation: 0.10587283136038317
Random Forrest
Algorithm Setting
Random_Forest = RandomForestRegressor(n_estimators=15,  # Number of trees in the forest
                                      max_depth=10,      # Maximum depth of the trees
                                      min_samples_split=15,  # Minimum number of samples required to split an internal node
                                      min_samples_leaf=15,   # Minimum number of samples required to be at a leaf node
                                      max_features='sqrt',  # Number of features to consider when looking for the best split
                                      random_state=10)      # Random state for reproducibility
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Random_Forest.fit(X_train, y_train)
RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=15,
                      min_samples_split=15, n_estimators=15, random_state=10)
Model Evaluation on train envairment
y_pred_train_enva = Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.36, 'MSE': 2.01, 'RMSE': 1.42, 'MAE': 1.16}
Model Evaluation on valid envairment
y_pred_valid_enva = Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.25, 'MSE': 2.52, 'RMSE': 1.59, 'MAE': 1.25}
model_dict = {'model': "Random_Forest"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.36  2.15  1.47  1.18
1   Ridge-Regression      0.37  2.13  1.46  1.18
2   Lasso-Regression     -0.01  3.40  1.84  1.44
3        Elastic_Net      0.11  2.99  1.73  1.37
4      Random_Forest      0.25  2.52  1.59  1.25
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Random_Forest, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.57828105 1.51425729 1.42137054]
Mean RMSE from cross-validation: 1.5046362928654942
Cross-validated R-squared scores: [0.37060556 0.25879895 0.21409267]
Mean R-squared from cross-validation: 0.28116572721197913
AdaBoost-Regressor
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Initialize DecisionTreeRegressor as the base estimator
base_estimator = DecisionTreeRegressor(max_depth=3)  # Adjust max_depth to control the complexity of the base estimator

# Initialize AdaBoostRegressor with hyperparameters to reduce overfitting
AdaBoost_Regressor = AdaBoostRegressor(base_estimator=base_estimator,
                                       n_estimators=100,     # Number of weak learners (base estimators)
                                       learning_rate=0.1,    # Learning rate shrinks the contribution of each base estimator
                                       loss='exponential',        # The loss function to use (linear, square, exponential)
                                       random_state=42)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
AdaBoost_Regressor.fit(X_train, y_train)
AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),
                  learning_rate=0.1, loss='exponential', n_estimators=100,
                  random_state=42)
Model Evaluation on train envairment
y_pred_train_enva = AdaBoost_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.45, 'MSE': 1.72, 'RMSE': 1.31, 'MAE': 1.08}
Model Evaluation on valid envairment
y_pred_valid_enva = AdaBoost_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.3, 'MSE': 2.36, 'RMSE': 1.54, 'MAE': 1.22}
model_dict = {'model': "AdaBoost_Regressor"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.36  2.15  1.47  1.18
1    Ridge-Regression      0.37  2.13  1.46  1.18
2    Lasso-Regression     -0.01  3.40  1.84  1.44
3         Elastic_Net      0.11  2.99  1.73  1.37
4       Random_Forest      0.25  2.52  1.59  1.25
5  AdaBoost_Regressor      0.30  2.36  1.54  1.22
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(AdaBoost_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.57430993 1.49085278 1.40988371]
Mean RMSE from cross-validation: 1.4916821421242146
Cross-validated R-squared scores: [0.37376881 0.28153405 0.22674398]
Mean R-squared from cross-validation: 0.29401561309779817
Gradient Boosting Machine
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
# Initialize GradientBoostingRegressor with hyperparameters to reduce overfitting
Gradient_Boosting = GradientBoostingRegressor(n_estimators=100,     # Number of boosting stages to be performed
                                              learning_rate=0.1,    # Learning rate shrinks the contribution of each tree
                                              max_depth=3,          # Maximum depth of the individual trees
                                              min_samples_split=25,  # Minimum number of samples required to split an internal node
                                              min_samples_leaf=25,   # Minimum number of samples required to be at a leaf node
                                              max_features='sqrt',  # Number of features to consider when looking for the best split
                                              loss='ls',            # Loss function to be optimized ('ls' refers to least squares regression)
                                              random_state=17)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Gradient_Boosting.fit(X_train, y_train)
GradientBoostingRegressor(loss='ls', max_features='sqrt', min_samples_leaf=25,
                          min_samples_split=25, random_state=17)
Model Evaluation on train envairment
y_pred_train_enva = Gradient_Boosting.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.53, 'MSE': 1.46, 'RMSE': 1.21, 'MAE': 0.98}
Model Evaluation on valid envairment
y_pred_valid_enva = Gradient_Boosting.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.29, 'MSE': 2.38, 'RMSE': 1.54, 'MAE': 1.21}
model_dict = {'model': "Gradient_Boosting"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.36  2.15  1.47  1.18
1    Ridge-Regression      0.37  2.13  1.46  1.18
2    Lasso-Regression     -0.01  3.40  1.84  1.44
3         Elastic_Net      0.11  2.99  1.73  1.37
4       Random_Forest      0.25  2.52  1.59  1.25
5  AdaBoost_Regressor      0.30  2.36  1.54  1.22
6   Gradient_Boosting      0.29  2.38  1.54  1.21
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Gradient_Boosting, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.50311358 1.42723172 1.39268154]
Mean RMSE from cross-validation: 1.441008945371258
Cross-validated R-squared scores: [0.42912922 0.34154568 0.24549806]
Mean R-squared from cross-validation: 0.3387243190982678

kNN
Algorithm Setting
Knn_model = KNeighborsRegressor(n_neighbors=5,  # Number of neighbors to consider
                                weights='uniform',  # Weight function used in prediction ('uniform' for equal weights)
                                algorithm='brute',  # Algorithm used to compute the nearest neighbors ('auto' for automatic selection)
                                leaf_size=30,      # Leaf size passed to BallTree or KDTree
                                p=2)               # Power parameter for the Minkowski metric (2 for Euclidean distance)
parameters :
(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Knn_model.fit(X_train, y_train)
KNeighborsRegressor(algorithm='brute')
Model Evaluation on train envairment
y_pred_train_enva = Knn_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.5, 'MSE': 1.57, 'RMSE': 1.25, 'MAE': 1.01}
Model Evaluation on valid envairment
y_pred_valid_enva = Knn_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.19, 'MSE': 2.73, 'RMSE': 1.65, 'MAE': 1.33}
model_dict = {'model': "Knn-model"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.36  2.15  1.47  1.18
1    Ridge-Regression      0.37  2.13  1.46  1.18
2    Lasso-Regression     -0.01  3.40  1.84  1.44
3         Elastic_Net      0.11  2.99  1.73  1.37
4       Random_Forest      0.25  2.52  1.59  1.25
5  AdaBoost_Regressor      0.30  2.36  1.54  1.22
6   Gradient_Boosting      0.29  2.38  1.54  1.21
7           Knn-model      0.19  2.73  1.65  1.33
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Knn_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.58375511 1.59824745 1.54721859]
Mean RMSE from cross-validation: 1.5764070483310515
Cross-validated R-squared scores: [0.36623204 0.17429538 0.06876334]
Mean R-squared from cross-validation: 0.20309691748814984
SVM Model
Algorithm Setting
from sklearn.svm import SVR
SVM_model = SVR(kernel='rbf',    # Kernel function ('rbf' for radial basis function)
                C=1.0,          # Regularization parameter
                epsilon=0.1,    # Epsilon in the epsilon-insensitive loss function
                gamma='scale', # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'
                degree=3)       # Degree of the polynomial kernel function (for 'poly' kernel)
parameters :
(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]
Training Scores
Model Fit Base on Train Set Only :
SVM_model.fit(X_train, y_train)
SVR()
Model Evaluation on train envairment
y_pred_train_enva = SVM_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.37, 'MSE': 1.98, 'RMSE': 1.41, 'MAE': 1.14}
Model Evaluation on valid envairment
y_pred_valid_enva = SVM_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.36, 'MSE': 2.15, 'RMSE': 1.47, 'MAE': 1.17}
model_dict = {'model': "SVM_model"}
Dapar_models_list_validation = Dapar_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Dapar_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.36  2.15  1.47  1.18
1    Ridge-Regression      0.37  2.13  1.46  1.18
2    Lasso-Regression     -0.01  3.40  1.84  1.44
3         Elastic_Net      0.11  2.99  1.73  1.37
4       Random_Forest      0.25  2.52  1.59  1.25
5  AdaBoost_Regressor      0.30  2.36  1.54  1.22
6   Gradient_Boosting      0.29  2.38  1.54  1.21
7           Knn-model      0.19  2.73  1.65  1.33
8           SVM_model      0.36  2.15  1.47  1.17
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(SVM_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.55212575 1.46030567 1.44207339]
Mean RMSE from cross-validation: 1.4848349343458158
Cross-validated R-squared scores: [0.39129337 0.31067471 0.19103181]
Mean R-squared from cross-validation: 0.29766662879994826
Selecting the Highest-Quality Model
Dapar_models_list_validation.sort_values('r2_score',ascending=False).round(2)
                model  r2_score   MSE  RMSE   MAE
1    Ridge-Regression      0.37  2.13  1.46  1.18
0   Linear-Regression      0.36  2.15  1.47  1.18
8           SVM_model      0.36  2.15  1.47  1.17
5  AdaBoost_Regressor      0.30  2.36  1.54  1.22
6   Gradient_Boosting      0.29  2.38  1.54  1.21
4       Random_Forest      0.25  2.52  1.59  1.25
7           Knn-model      0.19  2.73  1.65  1.33
3         Elastic_Net      0.11  2.99  1.73  1.37
2    Lasso-Regression     -0.01  3.40  1.84  1.44
selected_columns = ['model', 'r2_score', 'MSE', 'RMSE', 'MAE']
validation_ranked_list = Dapar_models_list_validation[selected_columns];

column_mapping = {'r2_score': 'r2_score_valid_set','MSE': 'MSE_valid_set','RMSE': 'RMSE_valid_set','MAE': 'MAE_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.361117       2.146633        1.465139   
1    Ridge-Regression            0.365044       2.133436        1.460629   
2    Lasso-Regression           -0.011116       3.397327        1.843184   
3         Elastic_Net            0.111061       2.986813        1.728240   
4       Random_Forest            0.251436       2.515158        1.585925   
5  AdaBoost_Regressor            0.298332       2.357589        1.535444   
6   Gradient_Boosting            0.290319       2.384512        1.544186   
7           Knn-model            0.186228       2.734255        1.653558   
8           SVM_model            0.360903       2.147350        1.465384   

   MAE_valid_set  
0       1.181490  
1       1.176368  
2       1.436246  
3       1.371818  
4       1.251002  
5       1.217777  
6       1.212868  
7       1.332979  
8       1.174567  
validation_ranked_list[['MSE_valid_set', 'RMSE_valid_set', 'MAE_valid_set']] *= -1
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.361117      -2.146633       -1.465139   
1    Ridge-Regression            0.365044      -2.133436       -1.460629   
2    Lasso-Regression           -0.011116      -3.397327       -1.843184   
3         Elastic_Net            0.111061      -2.986813       -1.728240   
4       Random_Forest            0.251436      -2.515158       -1.585925   
5  AdaBoost_Regressor            0.298332      -2.357589       -1.535444   
6   Gradient_Boosting            0.290319      -2.384512       -1.544186   
7           Knn-model            0.186228      -2.734255       -1.653558   
8           SVM_model            0.360903      -2.147350       -1.465384   

   MAE_valid_set  
0      -1.181490  
1      -1.176368  
2      -1.436246  
3      -1.371818  
4      -1.251002  
5      -1.217777  
6      -1.212868  
7      -1.332979  
8      -1.174567  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 8.0            8.0             8.0   
1    Ridge-Regression                 9.0            9.0             9.0   
2    Lasso-Regression                 1.0            1.0             1.0   
3         Elastic_Net                 2.0            2.0             2.0   
4       Random_Forest                 4.0            4.0             4.0   
5  AdaBoost_Regressor                 6.0            6.0             6.0   
6   Gradient_Boosting                 5.0            5.0             5.0   
7           Knn-model                 3.0            3.0             3.0   
8           SVM_model                 7.0            7.0             7.0   

   MAE_valid_set  
0            7.0  
1            8.0  
2            1.0  
3            2.0  
4            4.0  
5            5.0  
6            6.0  
7            3.0  
8            9.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
1    Ridge-Regression                 9.0            9.0             9.0   
0   Linear-Regression                 8.0            8.0             8.0   
8           SVM_model                 7.0            7.0             7.0   
5  AdaBoost_Regressor                 6.0            6.0             6.0   
6   Gradient_Boosting                 5.0            5.0             5.0   
4       Random_Forest                 4.0            4.0             4.0   
7           Knn-model                 3.0            3.0             3.0   
3         Elastic_Net                 2.0            2.0             2.0   
2    Lasso-Regression                 1.0            1.0             1.0   

   MAE_valid_set  Combined_Column  
1            8.0             35.0  
0            7.0             31.0  
8            9.0             30.0  
5            5.0             23.0  
6            6.0             21.0  
4            4.0             16.0  
7            3.0             12.0  
3            2.0              8.0  
2            1.0              4.0  
Selected Model - Fine_Tuning With Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Linear Regression model
Final_Model_Linear_Regression = Ridge()

# Define the parameter grid for GridSearchCV
param_grid = {
    'alpha': [0.1, 1.0, 10.0],  # Regularization strength
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'copy_X': [True, False],  # Whether to copy X data before fitting
    'max_iter': [1000, 2000],  # Maximum number of iterations
}

# Create GridSearchCV object
Dapar_Final_Model_Linear_Regression = GridSearchCV(Final_Model_Linear_Regression, param_grid, scoring='neg_mean_squared_error', cv=3)

# Fit the model with grid search
Dapar_Final_Model_Linear_Regression.fit(X_data, y_data)

# Best parameters from the grid search
best_params = Dapar_Final_Model_Linear_Regression.best_params_
print(f"Best parameters: {best_params}")

# Best model from the grid search
best_model = Dapar_Final_Model_Linear_Regression.best_estimator_

# Evaluate the best model on training data
y_pred_train_enva = best_model.predict(X_train)

# Display scatter plot for training data
sns.scatterplot(y_pred_train_enva, y_train)

# Evaluate metrics for training data
metrics_train = regressionMetrics(y_train, y_pred_train_enva)
formatted_metrics_train = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_train.items()}
print("Metrics on training data:")
print(formatted_metrics_train)

# Evaluate the best model on validation data
y_pred_valid_enva = best_model.predict(X_valid)

# Display scatter plot for validation data
#sns.scatterplot(y_pred_valid_enva, y_valid)

# Evaluate metrics for validation data
metrics_valid = regressionMetrics(y_valid, y_pred_valid_enva)
formatted_metrics_valid = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_valid.items()}
print("Metrics on validation data:")
print(formatted_metrics_valid)

# Cross-validate the best model
cv_scores = cross_validate(best_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Best parameters: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': True}
Metrics on training data:
{'r2_score': 0.4, 'MSE': 1.87, 'RMSE': 1.37, 'MAE': 1.12}
Metrics on validation data:
{'r2_score': 0.38, 'MSE': 2.08, 'RMSE': 1.44, 'MAE': 1.16}
Cross-validated RMSE scores: [1.51666242 1.37675081 1.36352941]
Mean RMSE from cross-validation: 1.4189808799220947
Cross-validated R-squared scores: [0.41879134 0.38730074 0.2767545 ]
Mean R-squared from cross-validation: 0.3609488611886736
 
Save Dapar grade selected model - Auto Set Hyper Parameters
#import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
#joblib.dump(Dapar_Final_Model_Linear_Regression, 'Dapar_Final_Model_Linear_Regression.pkl')
Final Algorithm Setting
Dapar_Final_Model_Linear_Regression = Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=True)
Dapar_Final_Model_Linear_Regression.fit(X_train, y_train)
Ridge(alpha=0.1, max_iter=1000, normalize=True)
###Model Evaluation on train envairment

y_pred_train_enva = Dapar_Final_Model_Linear_Regression.predict(X_train)

Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.41, 'MSE': 1.85, 'RMSE': 1.36, 'MAE': 1.12}
Model Evaluation on valid envairment
###Model Evaluation on valid envairment

y_pred_valid_enva = Dapar_Final_Model_Linear_Regression.predict(X_valid)

Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.37, 'MSE': 2.13, 'RMSE': 1.46, 'MAE': 1.17}
Save Dapar final grade selected model - Constant Set Hyper Parameters
import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
joblib.dump(Dapar_Final_Model_Linear_Regression, 'Dapar_Final_Model_Linear_Regression.pkl')
['Dapar_Final_Model_Linear_Regression.pkl']
Hebrew Grade Prediction Models
###Change target variable to categor type

Heb_Sofi_Gold_List["Hebrew_Target"] = Heb_Sofi_Gold_List["Hebrew_Target"].astype('category')
#Heb_Sofi_Gold_List.info()
re-orginize variables in final file
Heb_Sofi_Gold_List = pd.DataFrame(Heb_Sofi_Gold_List)
temp_cols=Heb_Sofi_Gold_List.columns.tolist()
index=Heb_Sofi_Gold_List.columns.get_loc("Hebrew_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Heb_Sofi_Gold_List=Heb_Sofi_Gold_List[new_cols]
Heb_Sofi_Gold_List = pd.DataFrame(Heb_Sofi_Gold_List)
temp_cols=Heb_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Heb_Sofi_Gold_List=Heb_Sofi_Gold_List[new_cols]
#Heb_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Heb_Sofi_Gold_List = Heb_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Heb_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Hebrew_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

#Heb_Sofi_Gold_List=Heb_Sofi_Gold_List.apply(zscore)
#Heb_Sofi_Gold_List.head()
data = pd.DataFrame(Heb_Sofi_Gold_List)
#data
#data = pd.DataFrame(Heb_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Hebrew_Target"])
X_data = data[data.columns[~data.columns.isin(['Hebrew_Target'])]]
y_data = data['Hebrew_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.4, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (288, 11)
Validation data shape: (191, 11)
Test data shape: (1, 11)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Hebrew_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Linear Regression
Algorithm Setting
Model_Linear_Regression = LinearRegression()
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Model_Linear_Regression.fit(X_train, y_train)
LinearRegression()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Linear_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.35, 'MSE': 0.59, 'RMSE': 0.77, 'MAE': 0.6}
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.49, 'MSE': 0.48, 'RMSE': 0.7, 'MAE': 0.57}
model_dict = {'model': "Linear-Regression"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.49  0.48   0.7  0.57
Cross Validation
from sklearn.model_selection import cross_validate
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Model_Linear_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.74785204 0.80023977 0.78386039]
Mean RMSE from cross-validation: 0.7773174011416608
Cross-validated R-squared scores: [0.39019394 0.34568659 0.29526926]
Mean R-squared from cross-validation: 0.34371659678162736
Linear Reggresion Regulation
ridge
Algorithm Setting
Ridge_Regression = Ridge(random_state=5)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Ridge_Regression.fit(X_train, y_train)
Ridge(random_state=5)
Model Evaluation on train envairment
y_pred_train_enva = Ridge_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.35, 'MSE': 0.59, 'RMSE': 0.77, 'MAE': 0.6}
Model Evaluation on valid envairment
y_pred_valid_enva = Ridge_Regression.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.49, 'MSE': 0.49, 'RMSE': 0.7, 'MAE': 0.56}
model_dict = {'model': "Ridge-Regression"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.49  0.48   0.7  0.57
1   Ridge-Regression      0.49  0.49   0.7  0.56
Cross Validation
# Cross-validation

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Ridge_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=5)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.73315958 0.77909752 0.79906386 0.7985663  0.68092573]
Mean RMSE from cross-validation: 0.7581625991923611
Cross-validated R-squared scores: [0.39284082 0.3961521  0.40198738 0.18327985 0.4716101 ]
Mean R-squared from cross-validation: 0.36917404871321874
Lasso
Algorithm Setting
Lasso_Hebrew = Lasso(random_state=3)
parameters :
(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Lasso_Hebrew.fit(X_train, y_train)
Lasso(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Lasso_Hebrew.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 0.9, 'RMSE': 0.95, 'MAE': 0.79}
Model Evaluation on valid envairment
y_pred_valid_enva = Lasso_Hebrew.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.0, 'MSE': 0.96, 'RMSE': 0.98, 'MAE': 0.81}
model_dict = {'model': "Lasso-Regression"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.49  0.48  0.70  0.57
1   Ridge-Regression      0.49  0.49  0.70  0.56
2   Lasso-Regression     -0.00  0.96  0.98  0.81
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Lasso_Hebrew, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.95708798 0.99107837 0.93254477]
Mean RMSE from cross-validation: 0.9602370372652681
Cross-validated R-squared scores: [ 0.00123321 -0.00360208  0.00256374]
Mean R-squared from cross-validation: 6.495775540826809e-05
Elastic_net
Algorithm Setting
Elastic_Net = ElasticNet(random_state=3)
parameters :
(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Elastic_Net.fit(X_train, y_train)
ElasticNet(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Elastic_Net.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.02, 'MSE': 0.89, 'RMSE': 0.94, 'MAE': 0.78}
Model Evaluation on valid envairment
y_pred_valid_enva = Elastic_Net.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.02, 'MSE': 0.93, 'RMSE': 0.97, 'MAE': 0.8}
model_dict = {'model': "Elastic_Net"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.49  0.48  0.70  0.57
1   Ridge-Regression      0.49  0.49  0.70  0.56
2   Lasso-Regression     -0.00  0.96  0.98  0.81
3        Elastic_Net      0.02  0.93  0.97  0.80
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Elastic_Net, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.95290904 0.97912412 0.91051253]
Mean RMSE from cross-validation: 0.9475152285997811
Cross-validated R-squared scores: [0.00993601 0.02046253 0.04913771]
Mean R-squared from cross-validation: 0.026512081837362977
Random Forrest
Algorithm Setting
Random_Forest = RandomForestRegressor(n_estimators=15,  # Number of trees in the forest
                                      max_depth=10,      # Maximum depth of the trees
                                      min_samples_split=15,  # Minimum number of samples required to split an internal node
                                      min_samples_leaf=15,   # Minimum number of samples required to be at a leaf node
                                      max_features='sqrt',  # Number of features to consider when looking for the best split
                                      random_state=10)      # Random state for reproducibility
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Random_Forest.fit(X_train, y_train)
RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=15,
                      min_samples_split=15, n_estimators=15, random_state=10)
Model Evaluation on train envairment
y_pred_train_enva = Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_train_enva,y_train)
<AxesSubplot:ylabel='Hebrew_Target'>
 
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.31, 'MSE': 0.62, 'RMSE': 0.79, 'MAE': 0.61}
Model Evaluation on valid envairment
y_pred_valid_enva = Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
sns.scatterplot(y_pred_valid_enva,y_valid)
<AxesSubplot:ylabel='Hebrew_Target'>
 
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.33, 'MSE': 0.64, 'RMSE': 0.8, 'MAE': 0.62}
model_dict = {'model': "Random_Forest"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.49  0.48  0.70  0.57
1   Ridge-Regression      0.49  0.49  0.70  0.56
2   Lasso-Regression     -0.00  0.96  0.98  0.81
3        Elastic_Net      0.02  0.93  0.97  0.80
4      Random_Forest      0.33  0.64  0.80  0.62
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Random_Forest, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.8176398  0.83091834 0.81958218]
Mean RMSE from cross-validation: 0.8227134360649839
Cross-validated R-squared scores: [0.27107237 0.29455649 0.22957426]
Mean R-squared from cross-validation: 0.26506770575397404
AdaBoost-Regressor
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Initialize DecisionTreeRegressor as the base estimator
base_estimator = DecisionTreeRegressor(max_depth=3)  # Adjust max_depth to control the complexity of the base estimator

# Initialize AdaBoostRegressor with hyperparameters to reduce overfitting
AdaBoost_Regressor = AdaBoostRegressor(base_estimator=base_estimator,
                                       n_estimators=100,     # Number of weak learners (base estimators)
                                       learning_rate=0.1,    # Learning rate shrinks the contribution of each base estimator
                                       loss='exponential',        # The loss function to use (linear, square, exponential)
                                       random_state=42)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
AdaBoost_Regressor.fit(X_train, y_train)
AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),
                  learning_rate=0.1, loss='exponential', n_estimators=100,
                  random_state=42)
Model Evaluation on train envairment
y_pred_train_enva = AdaBoost_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.42, 'MSE': 0.53, 'RMSE': 0.73, 'MAE': 0.59}
Model Evaluation on valid envairment
y_pred_valid_enva = AdaBoost_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.33, 'MSE': 0.64, 'RMSE': 0.8, 'MAE': 0.62}
model_dict = {'model': "AdaBoost_Regressor"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.49  0.48  0.70  0.57
1    Ridge-Regression      0.49  0.49  0.70  0.56
2    Lasso-Regression     -0.00  0.96  0.98  0.81
3         Elastic_Net      0.02  0.93  0.97  0.80
4       Random_Forest      0.33  0.64  0.80  0.62
5  AdaBoost_Regressor      0.33  0.64  0.80  0.62
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(AdaBoost_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.77921245 0.83127783 0.80780427]
Mean RMSE from cross-validation: 0.8060981830432737
Cross-validated R-squared scores: [0.33797843 0.29394595 0.25155815]
Mean R-squared from cross-validation: 0.2944941759829001
Gradient Boosting Machine
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
# Initialize GradientBoostingRegressor with hyperparameters to reduce overfitting
Gradient_Boosting = GradientBoostingRegressor(n_estimators=100,     # Number of boosting stages to be performed
                                              learning_rate=0.1,    # Learning rate shrinks the contribution of each tree
                                              max_depth=3,          # Maximum depth of the individual trees
                                              min_samples_split=25,  # Minimum number of samples required to split an internal node
                                              min_samples_leaf=25,   # Minimum number of samples required to be at a leaf node
                                              max_features='sqrt',  # Number of features to consider when looking for the best split
                                              loss='ls',            # Loss function to be optimized ('ls' refers to least squares regression)
                                              random_state=17)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Gradient_Boosting.fit(X_train, y_train)
GradientBoostingRegressor(loss='ls', max_features='sqrt', min_samples_leaf=25,
                          min_samples_split=25, random_state=17)
Model Evaluation on train envairment
y_pred_train_enva = Gradient_Boosting.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.5, 'MSE': 0.45, 'RMSE': 0.67, 'MAE': 0.52}
Model Evaluation on valid envairment
y_pred_valid_enva = Gradient_Boosting.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.38, 'MSE': 0.6, 'RMSE': 0.77, 'MAE': 0.59}
model_dict = {'model': "Gradient_Boosting"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.49  0.48  0.70  0.57
1    Ridge-Regression      0.49  0.49  0.70  0.56
2    Lasso-Regression     -0.00  0.96  0.98  0.81
3         Elastic_Net      0.02  0.93  0.97  0.80
4       Random_Forest      0.33  0.64  0.80  0.62
5  AdaBoost_Regressor      0.33  0.64  0.80  0.62
6   Gradient_Boosting      0.38  0.60  0.77  0.59
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Gradient_Boosting, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.79827231 0.79413542 0.81362591]
Mean RMSE from cross-validation: 0.8020112151977664
Cross-validated R-squared scores: [0.30519569 0.35563093 0.24073161]
Mean R-squared from cross-validation: 0.30051940808570904

kNN
Algorithm Setting
Knn_model = KNeighborsRegressor(n_neighbors=5,  # Number of neighbors to consider
                                weights='uniform',  # Weight function used in prediction ('uniform' for equal weights)
                                algorithm='brute',  # Algorithm used to compute the nearest neighbors ('auto' for automatic selection)
                                leaf_size=30,      # Leaf size passed to BallTree or KDTree
                                p=2)               # Power parameter for the Minkowski metric (2 for Euclidean distance)
parameters :
(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Knn_model.fit(X_train, y_train)
KNeighborsRegressor(algorithm='brute')
Model Evaluation on train envairment
y_pred_train_enva = Knn_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.42, 'MSE': 0.52, 'RMSE': 0.72, 'MAE': 0.58}
Model Evaluation on valid envairment
y_pred_valid_enva = Knn_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.19, 'MSE': 0.77, 'RMSE': 0.88, 'MAE': 0.68}
model_dict = {'model': "Knn-model"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.49  0.48  0.70  0.57
1    Ridge-Regression      0.49  0.49  0.70  0.56
2    Lasso-Regression     -0.00  0.96  0.98  0.81
3         Elastic_Net      0.02  0.93  0.97  0.80
4       Random_Forest      0.33  0.64  0.80  0.62
5  AdaBoost_Regressor      0.33  0.64  0.80  0.62
6   Gradient_Boosting      0.38  0.60  0.77  0.59
7           Knn-model      0.19  0.77  0.88  0.68
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Knn_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.91021975 0.85381497 0.94973681]
Mean RMSE from cross-validation: 0.9045905091926842
Cross-validated R-squared scores: [ 0.09665659  0.25514269 -0.03455197]
Mean R-squared from cross-validation: 0.10574910050050663
SVM Model
Algorithm Setting
from sklearn.svm import SVR
SVM_model = SVR(kernel='rbf',    # Kernel function ('rbf' for radial basis function)
                C=1.0,          # Regularization parameter
                epsilon=0.1,    # Epsilon in the epsilon-insensitive loss function
                gamma='scale', # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'
                degree=3)       # Degree of the polynomial kernel function (for 'poly' kernel)
parameters :
(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]
Training Scores
Model Fit Base on Train Set Only :
SVM_model.fit(X_train, y_train)
SVR()
Model Evaluation on train envairment
y_pred_train_enva = SVM_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.21, 'MSE': 0.71, 'RMSE': 0.84, 'MAE': 0.64}
Model Evaluation on valid envairment
y_pred_valid_enva = SVM_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.27, 'MSE': 0.7, 'RMSE': 0.83, 'MAE': 0.64}
model_dict = {'model': "SVM_model"}
Hebrew_models_list_validation = Hebrew_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Hebrew_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.49  0.48  0.70  0.57
1    Ridge-Regression      0.49  0.49  0.70  0.56
2    Lasso-Regression     -0.00  0.96  0.98  0.81
3         Elastic_Net      0.02  0.93  0.97  0.80
4       Random_Forest      0.33  0.64  0.80  0.62
5  AdaBoost_Regressor      0.33  0.64  0.80  0.62
6   Gradient_Boosting      0.38  0.60  0.77  0.59
7           Knn-model      0.19  0.77  0.88  0.68
8           SVM_model      0.27  0.70  0.83  0.64
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(SVM_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.86183549 0.84256598 0.84124512]
Mean RMSE from cross-validation: 0.8485488619085725
Cross-validated R-squared scores: [0.19014156 0.27464035 0.18830869]
Mean R-squared from cross-validation: 0.21769686662126397
Selecting the Highest-Quality Model
Hebrew_models_list_validation.sort_values('r2_score',ascending=False).round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.49  0.48  0.70  0.57
1    Ridge-Regression      0.49  0.49  0.70  0.56
6   Gradient_Boosting      0.38  0.60  0.77  0.59
4       Random_Forest      0.33  0.64  0.80  0.62
5  AdaBoost_Regressor      0.33  0.64  0.80  0.62
8           SVM_model      0.27  0.70  0.83  0.64
7           Knn-model      0.19  0.77  0.88  0.68
3         Elastic_Net      0.02  0.93  0.97  0.80
2    Lasso-Regression     -0.00  0.96  0.98  0.81
selected_columns = ['model', 'r2_score', 'MSE', 'RMSE', 'MAE']
validation_ranked_list = Hebrew_models_list_validation[selected_columns];

column_mapping = {'r2_score': 'r2_score_valid_set','MSE': 'MSE_valid_set','RMSE': 'RMSE_valid_set','MAE': 'MAE_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.493971       0.483794        0.695553   
1    Ridge-Regression            0.491205       0.486438        0.697451   
2    Lasso-Regression           -0.000118       0.956172        0.977840   
3         Elastic_Net            0.024524       0.932613        0.965719   
4       Random_Forest            0.329470       0.641066        0.800666   
5  AdaBoost_Regressor            0.326345       0.644055        0.802530   
6   Gradient_Boosting            0.375879       0.596697        0.772462   
7           Knn-model            0.194337       0.770262        0.877646   
8           SVM_model            0.271840       0.696164        0.834365   

   MAE_valid_set  
0       0.565159  
1       0.564902  
2       0.808464  
3       0.795211  
4       0.623055  
5       0.622112  
6       0.590947  
7       0.684817  
8       0.644951  
validation_ranked_list[['MSE_valid_set', 'RMSE_valid_set', 'MAE_valid_set']] *= -1
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.493971      -0.483794       -0.695553   
1    Ridge-Regression            0.491205      -0.486438       -0.697451   
2    Lasso-Regression           -0.000118      -0.956172       -0.977840   
3         Elastic_Net            0.024524      -0.932613       -0.965719   
4       Random_Forest            0.329470      -0.641066       -0.800666   
5  AdaBoost_Regressor            0.326345      -0.644055       -0.802530   
6   Gradient_Boosting            0.375879      -0.596697       -0.772462   
7           Knn-model            0.194337      -0.770262       -0.877646   
8           SVM_model            0.271840      -0.696164       -0.834365   

   MAE_valid_set  
0      -0.565159  
1      -0.564902  
2      -0.808464  
3      -0.795211  
4      -0.623055  
5      -0.622112  
6      -0.590947  
7      -0.684817  
8      -0.644951  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 9.0            9.0             9.0   
1    Ridge-Regression                 8.0            8.0             8.0   
2    Lasso-Regression                 1.0            1.0             1.0   
3         Elastic_Net                 2.0            2.0             2.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 5.0            5.0             5.0   
6   Gradient_Boosting                 7.0            7.0             7.0   
7           Knn-model                 3.0            3.0             3.0   
8           SVM_model                 4.0            4.0             4.0   

   MAE_valid_set  
0            8.0  
1            9.0  
2            1.0  
3            2.0  
4            5.0  
5            6.0  
6            7.0  
7            3.0  
8            4.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 9.0            9.0             9.0   
1    Ridge-Regression                 8.0            8.0             8.0   
6   Gradient_Boosting                 7.0            7.0             7.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 5.0            5.0             5.0   
8           SVM_model                 4.0            4.0             4.0   
7           Knn-model                 3.0            3.0             3.0   
3         Elastic_Net                 2.0            2.0             2.0   
2    Lasso-Regression                 1.0            1.0             1.0   

   MAE_valid_set  Combined_Column  
0            8.0             35.0  
1            9.0             33.0  
6            7.0             28.0  
4            5.0             23.0  
5            6.0             21.0  
8            4.0             16.0  
7            3.0             12.0  
3            2.0              8.0  
2            1.0              4.0  
Selected Model - Fine_Tuning With Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Linear Regression model
Final_Model_Linear_Regression = Ridge()

# Define the parameter grid for GridSearchCV
param_grid = {
    'alpha': [0.1, 1.0, 10.0],  # Regularization strength
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'copy_X': [True, False],  # Whether to copy X data before fitting
    'max_iter': [1000, 2000],  # Maximum number of iterations
}

# Create GridSearchCV object
Hebrew_Final_Model_Linear_Regression = GridSearchCV(Final_Model_Linear_Regression, param_grid, scoring='neg_mean_squared_error', cv=3)

# Fit the model with grid search
Hebrew_Final_Model_Linear_Regression.fit(X_data, y_data)

# Best parameters from the grid search
best_params = Hebrew_Final_Model_Linear_Regression.best_params_
print(f"Best parameters: {best_params}")

# Best model from the grid search
best_model = Hebrew_Final_Model_Linear_Regression.best_estimator_

# Evaluate the best model on training data
y_pred_train_enva = best_model.predict(X_train)

# Display scatter plot for training data
sns.scatterplot(y_pred_train_enva, y_train)

# Evaluate metrics for training data
metrics_train = regressionMetrics(y_train, y_pred_train_enva)
formatted_metrics_train = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_train.items()}
print("Metrics on training data:")
print(formatted_metrics_train)

# Evaluate the best model on validation data
y_pred_valid_enva = best_model.predict(X_valid)

# Display scatter plot for validation data
#sns.scatterplot(y_pred_valid_enva, y_valid)

# Evaluate metrics for validation data
metrics_valid = regressionMetrics(y_valid, y_pred_valid_enva)
formatted_metrics_valid = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_valid.items()}
print("Metrics on validation data:")
print(formatted_metrics_valid)

# Cross-validate the best model
cv_scores = cross_validate(best_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Best parameters: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': True}
Metrics on training data:
{'r2_score': 0.35, 'MSE': 0.59, 'RMSE': 0.77, 'MAE': 0.6}
Metrics on validation data:
{'r2_score': 0.51, 'MSE': 0.47, 'RMSE': 0.68, 'MAE': 0.55}
Cross-validated RMSE scores: [0.74615248 0.7938775  0.77772886]
Mean RMSE from cross-validation: 0.7725862805077032
Cross-validated R-squared scores: [0.39296246 0.35604941 0.30625127]
Mean R-squared from cross-validation: 0.351754380210123
 
Hebrew_Final_Model_Linear_Regression = Ridge(alpha=0.1, copy_X= True, fit_intercept= True, normalize= True, max_iter= 1000)
Model Fit Base on Train Set Only :
Hebrew_Final_Model_Linear_Regression.fit(X_train, y_train)
Ridge(alpha=0.1, max_iter=1000, normalize=True)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Hebrew_Final_Model_Linear_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.35, 'MSE': 0.59, 'RMSE': 0.77, 'MAE': 0.6}
Model Evaluation on valid envairment
y_pred_valid_enva = Hebrew_Final_Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.49, 'MSE': 0.49, 'RMSE': 0.7, 'MAE': 0.57}
Save Dapar grade selected model
import joblib

# Assuming Model_Linear_Regression is already trained

# Save the model to a file
joblib.dump(Hebrew_Final_Model_Linear_Regression, 'Hebrew_Final_Model_Linear_Regression.pkl')
['Hebrew_Final_Model_Linear_Regression.pkl']
Normot Grade Prediction Models
###Change target variable to categor type

Normot_Sofi_Gold_List["Normot_Target"] = Normot_Sofi_Gold_List["Normot_Target"].astype('category')
#Normot_Sofi_Gold_List
Model Develop based on candidates for "yeodi liba" roles
import pandas as pd

# Assuming Normot_Sofi_Gold_List is your dataframe
# Assuming Yeodi_Liba is the variable/column in the dataframe

# Filter rows where Yeodi_Liba is not equal to 0
Normot_Sofi_Gold_List = Normot_Sofi_Gold_List[Normot_Sofi_Gold_List['Yeodi_Liba'] != 0]

# Reset index after dropping rows
Normot_Sofi_Gold_List.reset_index(drop=True, inplace=True)
Normot_Sofi_Gold_List.drop(columns=['Yeodi_Liba'], inplace=True)
Normot_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 233 entries, 0 to 232
Data columns (total 10 columns):
 #   Column                                 Non-Null Count  Dtype   
---  ------                                 --------------  -----   
 0   Hebrew_Meam_Num                        233 non-null    float64 
 1   Work_Perceived_Maching_Num             233 non-null    float64 
 2   Number_of_Attempts_Num                 233 non-null    float64 
 3   Nation_Dico                            233 non-null    float64 
 4   Previous_Job_Salary_Num                233 non-null    float64 
 5   Salary_Expectations_Num                233 non-null    float64 
 6   Mental_Difficulties_Dico               233 non-null    float64 
 7   Normot_Job_Motivators_Index            233 non-null    float64 
 8   Normot_Interests_and_Activities_Index  233 non-null    float64 
 9   Normot_Target                          233 non-null    category
dtypes: category(1), float64(9)
memory usage: 17.1 KB
re-orginize variables in final file
Normot_Sofi_Gold_List = pd.DataFrame(Normot_Sofi_Gold_List)
temp_cols=Normot_Sofi_Gold_List.columns.tolist()
index=Normot_Sofi_Gold_List.columns.get_loc("Normot_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Normot_Sofi_Gold_List=Normot_Sofi_Gold_List[new_cols]
Normot_Sofi_Gold_List = pd.DataFrame(Normot_Sofi_Gold_List)
temp_cols=Normot_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Normot_Sofi_Gold_List=Normot_Sofi_Gold_List[new_cols]
#Normot_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Normot_Sofi_Gold_List = Normot_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Normot_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Normot_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

#Normot_Sofi_Gold_List=Normot_Sofi_Gold_List.apply(zscore)
#Normot_Sofi_Gold_List.head()
data = pd.DataFrame(Normot_Sofi_Gold_List)
#data
#data = pd.DataFrame(Normot_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Normot_Target"])
X_data = data[data.columns[~data.columns.isin(['Normot_Target'])]]
y_data = data['Normot_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.5, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (116, 9)
Validation data shape: (116, 9)
Test data shape: (1, 9)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Normot_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Linear Regression
Algorithm Setting
Model_Linear_Regression = LinearRegression()
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Model_Linear_Regression.fit(X_train, y_train)
LinearRegression()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Linear_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.24, 'MSE': 3.17, 'RMSE': 1.78, 'MAE': 1.28}
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.11, 'MSE': 2.96, 'RMSE': 1.72, 'MAE': 1.34}
model_dict = {'model': "Linear-Regression"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.11  2.96  1.72  1.34
Cross Validation
from sklearn.model_selection import cross_validate
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Model_Linear_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.85633744 1.86203233 1.8059312 ]
Mean RMSE from cross-validation: 1.8414336599465884
Cross-validated R-squared scores: [ 0.1235569  -0.0570369   0.21764175]
Mean R-squared from cross-validation: 0.09472058615806052
Linear Reggresion Regulation
ridge
Algorithm Setting
Ridge_Regression = Ridge(random_state=5)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Ridge_Regression.fit(X_train, y_train)
Ridge(random_state=5)
Model Evaluation on train envairment
y_pred_train_enva = Ridge_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.24, 'MSE': 3.19, 'RMSE': 1.79, 'MAE': 1.29}
Model Evaluation on valid envairment
y_pred_valid_enva = Ridge_Regression.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.12, 'MSE': 2.94, 'RMSE': 1.71, 'MAE': 1.34}
model_dict = {'model': "Ridge-Regression"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.11  2.96  1.72  1.34
1   Ridge-Regression      0.12  2.94  1.71  1.34
Cross Validation
# Cross-validation

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Ridge_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.84754107 1.80311589 1.82417525]
Mean RMSE from cross-validation: 1.8249440709882683
Cross-validated R-squared scores: [0.13184339 0.00879613 0.20175467]
Mean R-squared from cross-validation: 0.11413139655763697
Lasso
Algorithm Setting
Lasso_Normot = Lasso(random_state=3)
parameters :
(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Lasso_Normot.fit(X_train, y_train)
Lasso(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Lasso_Normot.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 4.18, 'RMSE': 2.04, 'MAE': 1.52}
Model Evaluation on valid envairment
y_pred_valid_enva = Lasso_Normot.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.01, 'MSE': 3.36, 'RMSE': 1.83, 'MAE': 1.46}
model_dict = {'model': "Lasso-Regression"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.11  2.96  1.72  1.34
1   Ridge-Regression      0.12  2.94  1.71  1.34
2   Lasso-Regression     -0.01  3.36  1.83  1.46
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Lasso_Normot, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.98384131 1.82089083 2.04570465]
Mean RMSE from cross-validation: 1.9501455977611357
Cross-validated R-squared scores: [-0.00097616 -0.01084258 -0.00389693]
Mean R-squared from cross-validation: -0.0052385591602426285
Elastic_net
Algorithm Setting
Elastic_Net = ElasticNet(random_state=3)
parameters :
(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Elastic_Net.fit(X_train, y_train)
ElasticNet(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Elastic_Net.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.0, 'MSE': 4.18, 'RMSE': 2.04, 'MAE': 1.52}
Model Evaluation on valid envairment
y_pred_valid_enva = Elastic_Net.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.01, 'MSE': 3.36, 'RMSE': 1.83, 'MAE': 1.46}
model_dict = {'model': "Elastic_Net"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.11  2.96  1.72  1.34
1   Ridge-Regression      0.12  2.94  1.71  1.34
2   Lasso-Regression     -0.01  3.36  1.83  1.46
3        Elastic_Net     -0.01  3.36  1.83  1.46
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Elastic_Net, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.98384131 1.82089083 2.04570465]
Mean RMSE from cross-validation: 1.9501455977611357
Cross-validated R-squared scores: [-0.00097616 -0.01084258 -0.00389693]
Mean R-squared from cross-validation: -0.0052385591602426285
Random Forrest
Algorithm Setting
Random_Forest = RandomForestRegressor(n_estimators=15,  # Number of trees in the forest
                                      max_depth=10,      # Maximum depth of the trees
                                      min_samples_split=15,  # Minimum number of samples required to split an internal node
                                      min_samples_leaf=15,   # Minimum number of samples required to be at a leaf node
                                      max_features='sqrt',  # Number of features to consider when looking for the best split
                                      random_state=10)      # Random state for reproducibility
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Random_Forest.fit(X_train, y_train)
RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=15,
                      min_samples_split=15, n_estimators=15, random_state=10)
Model Evaluation on train envairment
y_pred_train_enva = Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.13, 'MSE': 3.63, 'RMSE': 1.91, 'MAE': 1.41}
Model Evaluation on valid envairment
y_pred_valid_enva = Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.05, 'MSE': 3.17, 'RMSE': 1.78, 'MAE': 1.41}
model_dict = {'model': "Random_Forest"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.11  2.96  1.72  1.34
1   Ridge-Regression      0.12  2.94  1.71  1.34
2   Lasso-Regression     -0.01  3.36  1.83  1.46
3        Elastic_Net     -0.01  3.36  1.83  1.46
4      Random_Forest      0.05  3.17  1.78  1.41
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Random_Forest, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.87962786 1.74176801 1.95457686]
Mean RMSE from cross-validation: 1.8586575805671102
Cross-validated R-squared scores: [0.10142646 0.07509669 0.08355001]
Mean R-squared from cross-validation: 0.08669105615081281
AdaBoost-Regressor
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Initialize DecisionTreeRegressor as the base estimator
base_estimator = DecisionTreeRegressor(max_depth=3)  # Adjust max_depth to control the complexity of the base estimator

# Initialize AdaBoostRegressor with hyperparameters to reduce overfitting
AdaBoost_Regressor = AdaBoostRegressor(base_estimator=base_estimator,
                                       n_estimators=100,     # Number of weak learners (base estimators)
                                       learning_rate=0.1,    # Learning rate shrinks the contribution of each base estimator
                                       loss='exponential',        # The loss function to use (linear, square, exponential)
                                       random_state=42)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
AdaBoost_Regressor.fit(X_train, y_train)
AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),
                  learning_rate=0.1, loss='exponential', n_estimators=100,
                  random_state=42)
Model Evaluation on train envairment
y_pred_train_enva = AdaBoost_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.62, 'MSE': 1.57, 'RMSE': 1.25, 'MAE': 1.07}
Model Evaluation on valid envairment
y_pred_valid_enva = AdaBoost_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.08, 'MSE': 3.04, 'RMSE': 1.74, 'MAE': 1.37}
model_dict = {'model': "AdaBoost_Regressor"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.11  2.96  1.72  1.34
1    Ridge-Regression      0.12  2.94  1.71  1.34
2    Lasso-Regression     -0.01  3.36  1.83  1.46
3         Elastic_Net     -0.01  3.36  1.83  1.46
4       Random_Forest      0.05  3.17  1.78  1.41
5  AdaBoost_Regressor      0.08  3.04  1.74  1.37
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(AdaBoost_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.75807606 1.85080118 1.90564893]
Mean RMSE from cross-validation: 1.8381753879662057
Cross-validated R-squared scores: [ 0.21388661 -0.04432396  0.12885779]
Mean R-squared from cross-validation: 0.09947348090891417
Gradient Boosting Machine
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
# Initialize GradientBoostingRegressor with hyperparameters to reduce overfitting
Gradient_Boosting = GradientBoostingRegressor(n_estimators=100,     # Number of boosting stages to be performed
                                              learning_rate=0.1,    # Learning rate shrinks the contribution of each tree
                                              max_depth=3,          # Maximum depth of the individual trees
                                              min_samples_split=25,  # Minimum number of samples required to split an internal node
                                              min_samples_leaf=25,   # Minimum number of samples required to be at a leaf node
                                              max_features='sqrt',  # Number of features to consider when looking for the best split
                                              loss='ls',            # Loss function to be optimized ('ls' refers to least squares regression)
                                              random_state=17)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Gradient_Boosting.fit(X_train, y_train)
GradientBoostingRegressor(loss='ls', max_features='sqrt', min_samples_leaf=25,
                          min_samples_split=25, random_state=17)
Model Evaluation on train envairment
y_pred_train_enva = Gradient_Boosting.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.3, 'MSE': 2.92, 'RMSE': 1.71, 'MAE': 1.23}
Model Evaluation on valid envairment
y_pred_valid_enva = Gradient_Boosting.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.05, 'MSE': 3.48, 'RMSE': 1.87, 'MAE': 1.44}
model_dict = {'model': "Gradient_Boosting"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.11  2.96  1.72  1.34
1    Ridge-Regression      0.12  2.94  1.71  1.34
2    Lasso-Regression     -0.01  3.36  1.83  1.46
3         Elastic_Net     -0.01  3.36  1.83  1.46
4       Random_Forest      0.05  3.17  1.78  1.41
5  AdaBoost_Regressor      0.08  3.04  1.74  1.37
6   Gradient_Boosting     -0.05  3.48  1.87  1.44
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Gradient_Boosting, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [1.93203238 1.82042681 2.02890403]
Mean RMSE from cross-validation: 1.9271210741802023
Cross-validated R-squared scores: [ 0.05062306 -0.01032745  0.01252463]
Mean R-squared from cross-validation: 0.01760674467365982

kNN
Algorithm Setting
Knn_model = KNeighborsRegressor(n_neighbors=5,  # Number of neighbors to consider
                                weights='uniform',  # Weight function used in prediction ('uniform' for equal weights)
                                algorithm='brute',  # Algorithm used to compute the nearest neighbors ('auto' for automatic selection)
                                leaf_size=30,      # Leaf size passed to BallTree or KDTree
                                p=2)               # Power parameter for the Minkowski metric (2 for Euclidean distance)
parameters :
(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Knn_model.fit(X_train, y_train)
KNeighborsRegressor(algorithm='brute')
Model Evaluation on train envairment
y_pred_train_enva = Knn_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.25, 'MSE': 3.14, 'RMSE': 1.77, 'MAE': 1.29}
Model Evaluation on valid envairment
y_pred_valid_enva = Knn_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.07, 'MSE': 3.57, 'RMSE': 1.89, 'MAE': 1.42}
model_dict = {'model': "Knn-model"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.11  2.96  1.72  1.34
1    Ridge-Regression      0.12  2.94  1.71  1.34
2    Lasso-Regression     -0.01  3.36  1.83  1.46
3         Elastic_Net     -0.01  3.36  1.83  1.46
4       Random_Forest      0.05  3.17  1.78  1.41
5  AdaBoost_Regressor      0.08  3.04  1.74  1.37
6   Gradient_Boosting     -0.05  3.48  1.87  1.44
7           Knn-model     -0.07  3.57  1.89  1.42
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Knn_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [2.02433908 1.96820887 2.04380597]
Mean RMSE from cross-validation: 2.0121179748954585
Cross-validated R-squared scores: [-0.04226078 -0.18102225 -0.00203431]
Mean R-squared from cross-validation: -0.07510577791925181
SVM Model
Algorithm Setting
from sklearn.svm import SVR
SVM_model = SVR(kernel='rbf',    # Kernel function ('rbf' for radial basis function)
                C=1.0,          # Regularization parameter
                epsilon=0.1,    # Epsilon in the epsilon-insensitive loss function
                gamma='scale', # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'
                degree=3)       # Degree of the polynomial kernel function (for 'poly' kernel)
parameters :
(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)[source]
Training Scores
Model Fit Base on Train Set Only :
SVM_model.fit(X_train, y_train)
SVR()
Model Evaluation on train envairment
y_pred_train_enva = SVM_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.02, 'MSE': 4.27, 'RMSE': 2.07, 'MAE': 1.3}
Model Evaluation on valid envairment
y_pred_valid_enva = SVM_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': -0.0, 'MSE': 3.34, 'RMSE': 1.83, 'MAE': 1.23}
model_dict = {'model': "SVM_model"}
Normot_models_list_validation = Normot_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Normot_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.11  2.96  1.72  1.34
1    Ridge-Regression      0.12  2.94  1.71  1.34
2    Lasso-Regression     -0.01  3.36  1.83  1.46
3         Elastic_Net     -0.01  3.36  1.83  1.46
4       Random_Forest      0.05  3.17  1.78  1.41
5  AdaBoost_Regressor      0.08  3.04  1.74  1.37
6   Gradient_Boosting     -0.05  3.48  1.87  1.44
7           Knn-model     -0.07  3.57  1.89  1.42
8           SVM_model     -0.00  3.34  1.83  1.23
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(SVM_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [2.04442708 1.89893789 2.14023268]
Mean RMSE from cross-validation: 2.027865884816085
Cross-validated R-squared scores: [-0.06304861 -0.09935316 -0.09881669]
Mean R-squared from cross-validation: -0.08707282183303366
Selecting the Highest-Quality Model
Normot_models_list_validation.sort_values('r2_score',ascending=False).round(2)
                model  r2_score   MSE  RMSE   MAE
1    Ridge-Regression      0.12  2.94  1.71  1.34
0   Linear-Regression      0.11  2.96  1.72  1.34
5  AdaBoost_Regressor      0.08  3.04  1.74  1.37
4       Random_Forest      0.05  3.17  1.78  1.41
8           SVM_model     -0.00  3.34  1.83  1.23
2    Lasso-Regression     -0.01  3.36  1.83  1.46
3         Elastic_Net     -0.01  3.36  1.83  1.46
6   Gradient_Boosting     -0.05  3.48  1.87  1.44
7           Knn-model     -0.07  3.57  1.89  1.42
selected_columns = ['model', 'r2_score', 'MSE', 'RMSE', 'MAE']
validation_ranked_list = Normot_models_list_validation[selected_columns];

column_mapping = {'r2_score': 'r2_score_valid_set','MSE': 'MSE_valid_set','RMSE': 'RMSE_valid_set','MAE': 'MAE_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.111320       2.955245        1.719083   
1    Ridge-Regression            0.115630       2.940912        1.714909   
2    Lasso-Regression           -0.009855       3.358205        1.832540   
3         Elastic_Net           -0.009855       3.358205        1.832540   
4       Random_Forest            0.047789       3.166512        1.779470   
5  AdaBoost_Regressor            0.084705       3.043750        1.744635   
6   Gradient_Boosting           -0.046003       3.478410        1.865050   
7           Knn-model           -0.073234       3.568966        1.889171   
8           SVM_model           -0.003359       3.336601        1.826637   

   MAE_valid_set  
0       1.336782  
1       1.344722  
2       1.462842  
3       1.462842  
4       1.412449  
5       1.365904  
6       1.444552  
7       1.417241  
8       1.230292  
validation_ranked_list[['MSE_valid_set', 'RMSE_valid_set', 'MAE_valid_set']] *= -1
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.111320      -2.955245       -1.719083   
1    Ridge-Regression            0.115630      -2.940912       -1.714909   
2    Lasso-Regression           -0.009855      -3.358205       -1.832540   
3         Elastic_Net           -0.009855      -3.358205       -1.832540   
4       Random_Forest            0.047789      -3.166512       -1.779470   
5  AdaBoost_Regressor            0.084705      -3.043750       -1.744635   
6   Gradient_Boosting           -0.046003      -3.478410       -1.865050   
7           Knn-model           -0.073234      -3.568966       -1.889171   
8           SVM_model           -0.003359      -3.336601       -1.826637   

   MAE_valid_set  
0      -1.336782  
1      -1.344722  
2      -1.462842  
3      -1.462842  
4      -1.412449  
5      -1.365904  
6      -1.444552  
7      -1.417241  
8      -1.230292  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 8.0            8.0             8.0   
1    Ridge-Regression                 9.0            9.0             9.0   
2    Lasso-Regression                 3.0            3.0             3.0   
3         Elastic_Net                 3.0            3.0             3.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 7.0            7.0             7.0   
6   Gradient_Boosting                 2.0            2.0             2.0   
7           Knn-model                 1.0            1.0             1.0   
8           SVM_model                 5.0            5.0             5.0   

   MAE_valid_set  
0            8.0  
1            7.0  
2            1.0  
3            1.0  
4            5.0  
5            6.0  
6            3.0  
7            4.0  
8            9.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
1    Ridge-Regression                 9.0            9.0             9.0   
0   Linear-Regression                 8.0            8.0             8.0   
5  AdaBoost_Regressor                 7.0            7.0             7.0   
8           SVM_model                 5.0            5.0             5.0   
4       Random_Forest                 6.0            6.0             6.0   
2    Lasso-Regression                 3.0            3.0             3.0   
3         Elastic_Net                 3.0            3.0             3.0   
6   Gradient_Boosting                 2.0            2.0             2.0   
7           Knn-model                 1.0            1.0             1.0   

   MAE_valid_set  Combined_Column  
1            7.0             34.0  
0            8.0             32.0  
5            6.0             27.0  
8            9.0             24.0  
4            5.0             23.0  
2            1.0             10.0  
3            1.0             10.0  
6            3.0              9.0  
7            4.0              7.0  
Selected Model - Fine_Tuning With Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Linear Regression model
Final_Model_Linear_Regression = Ridge()

param_grid = {
    'alpha': [0.1, 1.0, 10.0],  # Regularization strength
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'copy_X': [True, False],  # Whether to copy X data before fitting
    'max_iter': [1000, 2000],  # Maximum number of iterations
}

# Create GridSearchCV object
Normot_Final_Model_Linear_Regression = GridSearchCV(Final_Model_Linear_Regression, param_grid, scoring='neg_mean_squared_error', cv=3)

# Fit the model with grid search
Normot_Final_Model_Linear_Regression.fit(X_data, y_data)

# Best parameters from the grid search
best_params = Normot_Final_Model_Linear_Regression.best_params_
print(f"Best parameters: {best_params}")

# Best model from the grid search
best_model = Normot_Final_Model_Linear_Regression.best_estimator_

# Evaluate the best model on training data
y_pred_train_enva = best_model.predict(X_train)

# Display scatter plot for training data
sns.scatterplot(y_pred_train_enva, y_train)

# Evaluate metrics for training data
metrics_train = regressionMetrics(y_train, y_pred_train_enva)
formatted_metrics_train = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_train.items()}
print("Metrics on training data:")
print(formatted_metrics_train)

# Evaluate the best model on validation data
y_pred_valid_enva = best_model.predict(X_valid)

# Display scatter plot for validation data
#sns.scatterplot(y_pred_valid_enva, y_valid)

# Evaluate metrics for validation data
metrics_valid = regressionMetrics(y_valid, y_pred_valid_enva)
formatted_metrics_valid = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_valid.items()}
print("Metrics on validation data:")
print(formatted_metrics_valid)

# Cross-validate the best model
cv_scores = cross_validate(best_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Best parameters: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': False}
Metrics on training data:
{'r2_score': 0.22, 'MSE': 3.26, 'RMSE': 1.8, 'MAE': 1.28}
Metrics on validation data:
{'r2_score': 0.18, 'MSE': 2.74, 'RMSE': 1.65, 'MAE': 1.28}
Cross-validated RMSE scores: [1.84754107 1.80311589 1.82417525]
Mean RMSE from cross-validation: 1.8249440709882683
Cross-validated R-squared scores: [0.13184339 0.00879613 0.20175467]
Mean R-squared from cross-validation: 0.11413139655763697
 
Save Normot grade selected model - - Auto Set Hyper Parameters
#import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
#joblib.dump(Normot_Final_Model_Linear_Regression, 'Normot_Final_Model_Linear_Regression.pkl')
Normot_Final_Model_Linear_Regression = Ridge(alpha= 0.1, copy_X= True, fit_intercept= True, max_iter= 1000, normalize= True)
Model Fit Base on Train Set Only :
Normot_Final_Model_Linear_Regression.fit(X_train, y_train)
Ridge(alpha=0.1, max_iter=1000, normalize=True)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Normot_Final_Model_Linear_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.24, 'MSE': 3.18, 'RMSE': 1.78, 'MAE': 1.28}
Model Evaluation on valid envairment
y_pred_valid_enva = Normot_Final_Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.13, 'MSE': 2.91, 'RMSE': 1.71, 'MAE': 1.33}
Save Normot grade selected model - - Constant Set Hyper Parameters
import joblib

# Assuming Model_Linear_Regression is already trained

# Save the model to a file
joblib.dump(Normot_Final_Model_Linear_Regression, 'Normot_Final_Model_Linear_Regression.pkl')
['Normot_Final_Model_Linear_Regression.pkl']
Personality_Disq Prediction Models
###Change target variable to categor type

Personality_Diss_Sofi_Gold_List["Personality_Dis_Safek_Target"] = Personality_Diss_Sofi_Gold_List["Personality_Dis_Safek_Target"].astype('category')
#Personality_Diss_Sofi_Gold_List.info()
re-orginize variables in final file
Personality_Diss_Sofi_Gold_List = pd.DataFrame(Personality_Diss_Sofi_Gold_List)
temp_cols=Personality_Diss_Sofi_Gold_List.columns.tolist()
index=Personality_Diss_Sofi_Gold_List.columns.get_loc("Personality_Dis_Safek_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Personality_Diss_Sofi_Gold_List=Personality_Diss_Sofi_Gold_List[new_cols]
Personality_Diss_Sofi_Gold_List = pd.DataFrame(Personality_Diss_Sofi_Gold_List)
temp_cols=Personality_Diss_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Personality_Diss_Sofi_Gold_List=Personality_Diss_Sofi_Gold_List[new_cols]
#Personality_Diss_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Gibushon_Sofi_Gold_List = Gibushon_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Personality_Diss_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Personality_Dis_Safek_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

# Get the list of columns to compute z-scores for
columns_to_compute_zscore = [col for col in Personality_Diss_Sofi_Gold_List.columns if col != 'Personality_Dis_Safek_Target']

# Compute z-scores for each column except "Personality_Dis_Safek_Target"
#Personality_Diss_Sofi_Gold_List[columns_to_compute_zscore] = Personality_Diss_Sofi_Gold_List[columns_to_compute_zscore].apply(zscore)

# Now, z-scores are computed for all variables in the DataFrame except "Personality_Dis_Safek_Target"
#Personality_Diss_Sofi_Gold_List
data = pd.DataFrame(Personality_Diss_Sofi_Gold_List)
#data
#data = pd.DataFrame(Personality_Diss_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Personality_Dis_Safek_Target"])
X_data = data[data.columns[~data.columns.isin(['Personality_Dis_Safek_Target'])]]
y_data = data['Personality_Dis_Safek_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.45, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (440, 15)
Validation data shape: (359, 15)
Test data shape: (1, 15)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Personality_Diss_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Logistic Regression
Algorithm Setting
Model_Logistic_Regression = LogisticRegression(random_state=1)
parameters :
parameters : (penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)
Model Fit Base on Train Set Only :
Model_Logistic_Regression.fit(X_train, y_train)
LogisticRegression(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Logistic_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.82,
 'Precision': 0.69,
 'Recall': 0.11,
 'f1-score': 0.18,
 'Log-loss': 6.28,
 'AUC': 0.55}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           351    4
1.0                            76    9
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Logistic_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.83,
 'Precision': 0.38,
 'Recall': 0.14,
 'f1-score': 0.21,
 'Log-loss': 5.87,
 'AUC': 0.55}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           290   13
1.0                            48    8
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Logistic_Regression.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.72,
 'Precision': 0.3,
 'Recall': 0.55,
 'f1-score': 0.39,
 'Log-loss': 9.52,
 'AUC': 0.65}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           229  74
1.0                            25  31
model_dict = {'model': "Logistic_Regression"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                 model  Accuracy  Precision  Recall  f1-score  Log-loss   AUC
0  Logistic_Regression      0.72        0.3    0.55      0.39      9.52  0.65
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Logistic_Regression is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Logistic_Regression, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Logistic_Regression.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.82022472 0.82397004 0.81954887]
Mean Accuracy from cross-validation: 0.8212478762449195
Cross-validated Precision scores: [0.44444444 0.5        0.44444444]
Mean Precision from cross-validation: 0.46296296296296297
Cross-validated Recall scores: [0.08510638 0.04255319 0.08510638]
Mean Recall from cross-validation: 0.07092198581560283
Cross-validated F1-score scores: [0.14285714 0.07843137 0.14285714]
Mean F1-score from cross-validation: 0.12138188608776844
Cross-validated Log-loss scores: [-0.45077383 -0.42100677 -0.44590087]
Mean Log-loss from cross-validation: 0.43922715681157404
Cross-validated AUC scores: [0.66252418 0.71073501 0.67706208]
Mean AUC from cross-validation: 0.6834404228822765
Accuracy: 0.7425
Precision: 0.3568281938325991
Recall: 0.574468085106383
F1 Score: 0.44021739130434784
Log-loss: 8.893880848220512
AUC: 0.6764601427049366
DecisionTree Classifier
Algorithm Setting
Model_Decision_Tree = DecisionTreeClassifier(random_state=1)
parameters : (criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)
Model Fit Base on Train Set Only :
Model_Decision_Tree.fit(X_train, y_train)
DecisionTreeClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Decision_Tree.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.98,
 'Precision': 1.0,
 'Recall': 0.92,
 'f1-score': 0.96,
 'Log-loss': 0.55,
 'AUC': 0.96}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           355    0
1.0                             7   78
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Decision_Tree.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.76,
 'Precision': 0.3,
 'Recall': 0.38,
 'f1-score': 0.33,
 'Log-loss': 8.18,
 'AUC': 0.6}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           253   50
1.0                            35   21
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Decision_Tree.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.75,
 'Precision': 0.29,
 'Recall': 0.41,
 'f1-score': 0.34,
 'Log-loss': 8.66,
 'AUC': 0.61}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           246  57
1.0                            33  23
model_dict = {'model': "DecisionTree Classifier"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   

    AUC  
0  0.65  
1  0.61  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Decision_Tree is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Decision_Tree, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Decision_Tree.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.73782772 0.73782772 0.7443609 ]
Mean Accuracy from cross-validation: 0.7400054443224166
Cross-validated Precision scores: [0.2745098  0.25531915 0.25581395]
Mean Precision from cross-validation: 0.261880968782037
Cross-validated Recall scores: [0.29787234 0.25531915 0.23404255]
Mean Recall from cross-validation: 0.2624113475177305
Cross-validated F1-score scores: [0.28571429 0.25531915 0.24444444]
Mean F1-score from cross-validation: 0.26182595969830014
Cross-validated Log-loss scores: [-8.20205403 -8.44426129 -8.72640969]
Mean Log-loss from cross-validation: 8.457575003356652
Cross-validated AUC scores: [0.56179884 0.57427466 0.53521811]
Mean AUC from cross-validation: 0.5570972034539508
Accuracy: 0.875
Precision: 0.6184971098265896
Recall: 0.7588652482269503
F1 Score: 0.6815286624203821
Log-loss: 4.317413016151828
AUC: 0.8293567515793324
Random Forest
Algorithm Setting
Model_Random_Forest = RandomForestClassifier(random_state=1)
parameters :
parameters : (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)
Model Fit Base on Train Set Only :
Model_Random_Forest.fit(X_train, y_train)
RandomForestClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.98,
 'Precision': 0.98,
 'Recall': 0.94,
 'f1-score': 0.96,
 'Log-loss': 0.55,
 'AUC': 0.97}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           353    2
1.0                             5   80
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.79,
 'Precision': 0.19,
 'Recall': 0.11,
 'f1-score': 0.14,
 'Log-loss': 7.22,
 'AUC': 0.51}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           278   25
1.0                            50    6
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Random_Forest.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.68,
 'Precision': 0.26,
 'Recall': 0.55,
 'f1-score': 0.35,
 'Log-loss': 11.06,
 'AUC': 0.63}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           213  90
1.0                            25  31
model_dict = {'model': "Random Forest"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   

    AUC  
0  0.65  
1  0.61  
2  0.63  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Random_Forest is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Random_Forest, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Random_Forest.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.76029963 0.82022472 0.78571429]
Mean Accuracy from cross-validation: 0.7887462100945246
Cross-validated Precision scores: [0.20689655 0.45454545 0.1875    ]
Mean Precision from cross-validation: 0.28298066875653083
Cross-validated Recall scores: [0.12765957 0.10638298 0.06382979]
Mean Recall from cross-validation: 0.09929078014184396
Cross-validated F1-score scores: [0.15789474 0.17241379 0.0952381 ]
Mean F1-score from cross-validation: 0.14184887506121624
Cross-validated Log-loss scores: [-0.6444269  -0.5939724  -0.62256556]
Mean Log-loss from cross-validation: 0.6203216222891422
Cross-validated AUC scores: [0.60667311 0.64569632 0.6376178 ]
Mean AUC from cross-validation: 0.6299957458584681
Accuracy: 0.84
Precision: 0.5299539170506913
Recall: 0.8156028368794326
F1 Score: 0.6424581005586593
Log-loss: 5.526306171858059
AUC: 0.8304114336142231
Adaptive Boosting (ADABoost)
Algorithm Setting
Model_ADABoost = AdaBoostClassifier(random_state=1)
parameters : (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Model_ADABoost.fit(X_train, y_train)
AdaBoostClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_ADABoost.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.82,
 'Precision': 0.6,
 'Recall': 0.14,
 'f1-score': 0.23,
 'Log-loss': 6.36,
 'AUC': 0.56}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           347    8
1.0                            73   12
Model Evaluation on valid envairment
y_pred_valid_enva = Model_ADABoost.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.83,
 'Precision': 0.43,
 'Recall': 0.21,
 'f1-score': 0.29,
 'Log-loss': 5.77,
 'AUC': 0.58}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           287   16
1.0                            44   12
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_ADABoost.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.16,
 'Precision': 0.16,
 'Recall': 1.0,
 'f1-score': 0.27,
 'Log-loss': 29.15,
 'AUC': 0.5}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           1
Personality_Dis_Safek_Target     
0.0                           303
1.0                            56
model_dict = {'model': "AdaBoostClassifier"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   

    AUC  
0  0.65  
1  0.61  
2  0.63  
3  0.50  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_ADABoost is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_ADABoost, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_ADABoost.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.80898876 0.8164794  0.81954887]
Mean Accuracy from cross-validation: 0.8150056789914862
Cross-validated Precision scores: [0.35714286 0.41666667 0.46153846]
Mean Precision from cross-validation: 0.4117826617826618
Cross-validated Recall scores: [0.10638298 0.10638298 0.12765957]
Mean Recall from cross-validation: 0.11347517730496454
Cross-validated F1-score scores: [0.16393443 0.16949153 0.2       ]
Mean F1-score from cross-validation: 0.17780865055107897
Cross-validated Log-loss scores: [-0.67618514 -0.68002518 -0.66880734]
Mean Log-loss from cross-validation: 0.6750058865206054
Cross-validated AUC scores: [0.65913926 0.72843327 0.6818226 ]
Mean AUC from cross-validation: 0.6897983772437284
Accuracy: 0.17625
Precision: 0.17625
Recall: 1.0
F1 Score: 0.2996811902231668
Log-loss: 28.451975723690797
AUC: 0.5
Gradient Boosting Machine (GBM)
Algorithm Setting
Model_Gradient= GradientBoostingClassifier(random_state=1)
parameters : (*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)
Model Fit Base on Train Set Only :
Model_Gradient.fit(X_train, y_train)
GradientBoostingClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Gradient.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.89,
 'Precision': 0.95,
 'Recall': 0.46,
 'f1-score': 0.62,
 'Log-loss': 3.77,
 'AUC': 0.73}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           353    2
1.0                            46   39
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Gradient.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.81,
 'Precision': 0.32,
 'Recall': 0.2,
 'f1-score': 0.24,
 'Log-loss': 6.54,
 'AUC': 0.56}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           280   23
1.0                            45   11
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Gradient.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.75,
 'Precision': 0.31,
 'Recall': 0.46,
 'f1-score': 0.37,
 'Log-loss': 8.56,
 'AUC': 0.63}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           244  59
1.0                            30  26
model_dict = {'model': "GBM"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   
4                      GBM      0.75       0.31    0.46      0.37      8.56   

    AUC  
0  0.65  
1  0.61  
2  0.63  
3  0.50  
4  0.63  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Gradientis your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Gradient, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Gradient.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.80898876 0.80524345 0.82330827]
Mean Accuracy from cross-validation: 0.8125134934715065
Cross-validated Precision scores: [0.375      0.35294118 0.5       ]
Mean Precision from cross-validation: 0.4093137254901961
Cross-validated Recall scores: [0.12765957 0.12765957 0.19148936]
Mean Recall from cross-validation: 0.14893617021276595
Cross-validated F1-score scores: [0.19047619 0.1875     0.27692308]
Mean F1-score from cross-validation: 0.2182997557997558
Cross-validated Log-loss scores: [-0.487728   -0.44301565 -0.44511907]
Mean Log-loss from cross-validation: 0.4586209040187641
Cross-validated AUC scores: [0.64211799 0.69211799 0.71407753]
Mean AUC from cross-validation: 0.6827711684021797
Accuracy: 0.81125
Precision: 0.4734042553191489
Recall: 0.6312056737588653
F1 Score: 0.541033434650456
Log-loss: 6.519292994721378
AUC: 0.7404890280782186
Support Vector Machine (SVM)
Algorithm Setting
from sklearn.svm import SVC
Model_SVC= SVC(random_state=1, probability=True)
parameters : (*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)
Model Fit Base on Train Set Only :
Model_SVC.fit(X_train, y_train)
SVC(probability=True, random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_SVC.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.81,
 'Precision': 0.0,
 'Recall': 0.0,
 'f1-score': 0.0,
 'Log-loss': 6.67,
 'AUC': 0.5}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0
Personality_Dis_Safek_Target     
0.0                           355
1.0                            85
Model Evaluation on valid envairment
y_pred_valid_enva = Model_SVC.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.84,
 'Precision': 0.0,
 'Recall': 0.0,
 'f1-score': 0.0,
 'Log-loss': 5.39,
 'AUC': 0.5}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0
Personality_Dis_Safek_Target     
0.0                           303
1.0                            56
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_SVC.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.77,
 'Precision': 0.31,
 'Recall': 0.39,
 'f1-score': 0.35,
 'Log-loss': 7.99,
 'AUC': 0.62}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           254  49
1.0                            34  22
model_dict = {'model': "SVM"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   
4                      GBM      0.75       0.31    0.46      0.37      8.56   
5                      SVM      0.77       0.31    0.39      0.35      7.99   

    AUC  
0  0.65  
1  0.61  
2  0.63  
3  0.50  
4  0.63  
5  0.62  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_SVCis your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_SVC, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_SVC.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.82397004 0.82397004 0.82330827]
Mean Accuracy from cross-validation: 0.8237494485276863
Cross-validated Precision scores: [0. 0. 0.]
Mean Precision from cross-validation: 0.0
Cross-validated Recall scores: [0. 0. 0.]
Mean Recall from cross-validation: 0.0
Cross-validated F1-score scores: [0. 0. 0.]
Mean F1-score from cross-validation: 0.0
Cross-validated Log-loss scores: [-0.45210496 -0.4601725  -0.4369002 ]
Mean Log-loss from cross-validation: 0.4497258852065788
Cross-validated AUC scores: [0.62229207 0.62969052 0.70076751]
Mean AUC from cross-validation: 0.650916701259167
Accuracy: 0.825
Precision: 0.5034965034965035
Recall: 0.5106382978723404
F1 Score: 0.5070422535211266
Log-loss: 6.0443568333813
AUC: 0.7014496496948954
Neural Network - MLP Classifier
from sklearn.neural_network import MLPClassifier
Algorithm Setting
MLP_Classifier = MLPClassifier(random_state=1)
parameters : (hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)
Model Fit Base on Train Set Only :
MLP_Classifier.fit(X_train, y_train)
MLPClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = MLP_Classifier.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.84,
 'Precision': 0.83,
 'Recall': 0.22,
 'f1-score': 0.35,
 'Log-loss': 5.49,
 'AUC': 0.61}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           351    4
1.0                            66   19
Model Evaluation on valid envairment
y_pred_valid_enva = MLP_Classifier.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.84,
 'Precision': 0.48,
 'Recall': 0.21,
 'f1-score': 0.3,
 'Log-loss': 5.48,
 'AUC': 0.59}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           290   13
1.0                            44   12
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = MLP_Classifier.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.73,
 'Precision': 0.29,
 'Recall': 0.5,
 'f1-score': 0.37,
 'Log-loss': 9.33,
 'AUC': 0.64}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           234  69
1.0                            28  28
model_dict = {'model': "Neural Networks"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   
4                      GBM      0.75       0.31    0.46      0.37      8.56   
5                      SVM      0.77       0.31    0.39      0.35      7.99   
6          Neural Networks      0.73       0.29    0.50      0.37      9.33   

    AUC  
0  0.65  
1  0.61  
2  0.63  
3  0.50  
4  0.63  
5  0.62  
6  0.64  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming MLP_Classifier is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(MLP_Classifier, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = MLP_Classifier.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.83146067 0.83895131 0.82706767]
Mean Accuracy from cross-validation: 0.8324932180638863
Cross-validated Precision scores: [0.5625     0.83333333 0.53333333]
Mean Precision from cross-validation: 0.6430555555555556
Cross-validated Recall scores: [0.19148936 0.10638298 0.17021277]
Mean Recall from cross-validation: 0.15602836879432624
Cross-validated F1-score scores: [0.28571429 0.18867925 0.25806452]
Mean F1-score from cross-validation: 0.2441526823754456
Cross-validated Log-loss scores: [-0.44414553 -0.4161812  -0.44621983]
Mean Log-loss from cross-validation: 0.4355155182051031
Cross-validated AUC scores: [0.68505803 0.72388781 0.68172544]
Mean AUC from cross-validation: 0.6968904286231595
Accuracy: 0.77
Precision: 0.39303482587064675
Recall: 0.5602836879432624
F1 Score: 0.4619883040935673
Log-loss: 7.944040509437562
AUC: 0.68757735231761
Gaussian Naiv Bayes
from sklearn.naive_bayes import GaussianNB
Algorithm Setting
Model_GaussianNB = GaussianNB()
parameters : (*, priors=None, var_smoothing=1e-09)
Model Fit Base on Train Set Only :
Model_GaussianNB.fit(X_train, y_train)
GaussianNB()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_GaussianNB.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.75,
 'Precision': 0.37,
 'Recall': 0.42,
 'f1-score': 0.39,
 'Log-loss': 8.71,
 'AUC': 0.62}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           293   62
1.0                            49   36
Model Evaluation on valid envairment
y_pred_valid_enva = Model_GaussianNB.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.72,
 'Precision': 0.23,
 'Recall': 0.34,
 'f1-score': 0.28,
 'Log-loss': 9.52,
 'AUC': 0.57}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           241   62
1.0                            37   19
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_GaussianNB.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.66,
 'Precision': 0.22,
 'Recall': 0.46,
 'f1-score': 0.3,
 'Log-loss': 11.64,
 'AUC': 0.58}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           212  91
1.0                            30  26
model_dict = {'model': "GaussianNB"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   
4                      GBM      0.75       0.31    0.46      0.37      8.56   
5                      SVM      0.77       0.31    0.39      0.35      7.99   
6          Neural Networks      0.73       0.29    0.50      0.37      9.33   
7               GaussianNB      0.66       0.22    0.46      0.30     11.64   

    AUC  
0  0.65  
1  0.61  
2  0.63  
3  0.50  
4  0.63  
5  0.62  
6  0.64  
7  0.58  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_GaussianNB is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_GaussianNB, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_GaussianNB.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.75280899 0.74157303 0.21052632]
Mean Accuracy from cross-validation: 0.5683027794204613
Cross-validated Precision scores: [0.33898305 0.2962963  0.18039216]
Mean Precision from cross-validation: 0.2718905013354997
Cross-validated Recall scores: [0.42553191 0.34042553 0.9787234 ]
Mean Recall from cross-validation: 0.5815602836879433
Cross-validated F1-score scores: [0.37735849 0.31683168 0.30463576]
Mean F1-score from cross-validation: 0.33294197844125284
Cross-validated Log-loss scores: [-0.83607509 -0.94083467 -4.42491326]
Mean Log-loss from cross-validation: 2.0672743388793116
Cross-validated AUC scores: [0.6733559  0.65812379 0.65743709]
Mean AUC from cross-validation: 0.6629722612307865
Accuracy: 0.68125
Precision: 0.2824427480916031
Recall: 0.524822695035461
F1 Score: 0.36724565756823824
Log-loss: 11.009422881273876
AUC: 0.6197709833295666
K-Neighbors Classifier
from sklearn.neighbors import KNeighborsClassifier
Algorithm Setting
Model_KNeighbors = KNeighborsClassifier()
parameters : (n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Model_KNeighbors.fit(X_train, y_train)
KNeighborsClassifier()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_KNeighbors.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.83,
 'Precision': 0.75,
 'Recall': 0.18,
 'f1-score': 0.29,
 'Log-loss': 5.89,
 'AUC': 0.58}
pd.crosstab(y_train, y_pred_train_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           350    5
1.0                            70   15
Model Evaluation on valid envairment
y_pred_valid_enva = Model_KNeighbors.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.85,
 'Precision': 0.52,
 'Recall': 0.2,
 'f1-score': 0.29,
 'Log-loss': 5.29,
 'AUC': 0.58}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                         0.0  1.0
Personality_Dis_Safek_Target          
0.0                           293   10
1.0                            45   11
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_KNeighbors.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.235  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.76,
 'Precision': 0.3,
 'Recall': 0.39,
 'f1-score': 0.34,
 'Log-loss': 8.27,
 'AUC': 0.61}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           251  52
1.0                            34  22
model_dict = {'model': "KNeighbors"}
Personality_Diss_models_list_validation = Personality_Diss_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Personality_Diss_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   
4                      GBM      0.75       0.31    0.46      0.37      8.56   
5                      SVM      0.77       0.31    0.39      0.35      7.99   
6          Neural Networks      0.73       0.29    0.50      0.37      9.33   
7               GaussianNB      0.66       0.22    0.46      0.30     11.64   
8               KNeighbors      0.76       0.30    0.39      0.34      8.27   

    AUC  
0  0.65  
1  0.61  
2  0.63  
3  0.50  
4  0.63  
5  0.62  
6  0.64  
7  0.58  
8  0.61  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_KNeighbors is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_KNeighbors, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_KNeighbors.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.76779026 0.82022472 0.79699248]
Mean Accuracy from cross-validation: 0.7950024874921385
Cross-validated Precision scores: [0.22222222 0.45454545 0.11111111]
Mean Precision from cross-validation: 0.2626262626262626
Cross-validated Recall scores: [0.12765957 0.10638298 0.0212766 ]
Mean Recall from cross-validation: 0.0851063829787234
Cross-validated F1-score scores: [0.16216216 0.17241379 0.03571429]
Mean F1-score from cross-validation: 0.12343008032663205
Cross-validated Log-loss scores: [-2.68286481 -2.34682163 -2.62181427]
Mean Log-loss from cross-validation: 2.550500237631013
Cross-validated AUC scores: [0.53897485 0.6184236  0.56791023]
Mean AUC from cross-validation: 0.5751028942882629
Accuracy: 0.79125
Precision: 0.42073170731707316
Recall: 0.48936170212765956
F1 Score: 0.45245901639344266
Log-loss: 7.210064524632441
AUC: 0.6726019436283214
Selecting the Highest-Quality Model
Personality_Diss_models_list_validation.sort_values('f1-score',ascending=False).round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.72       0.30    0.55      0.39      9.52   
4                      GBM      0.75       0.31    0.46      0.37      8.56   
6          Neural Networks      0.73       0.29    0.50      0.37      9.33   
2            Random Forest      0.68       0.26    0.55      0.35     11.06   
5                      SVM      0.77       0.31    0.39      0.35      7.99   
8               KNeighbors      0.76       0.30    0.39      0.34      8.27   
1  DecisionTree Classifier      0.75       0.29    0.41      0.34      8.66   
7               GaussianNB      0.66       0.22    0.46      0.30     11.64   
3       AdaBoostClassifier      0.16       0.16    1.00      0.27     29.15   

    AUC  
0  0.65  
4  0.63  
6  0.64  
2  0.63  
5  0.62  
8  0.61  
1  0.61  
7  0.58  
3  0.50  
selected_columns = ['model', 'Precision', 'Recall', 'f1-score', 'AUC']
validation_ranked_list = Personality_Diss_models_list_validation[selected_columns];

column_mapping = {'Precision': 'Precision_valid_set','Recall': 'Recall_valid_set','f1-score': 'f1-score_valid_set','AUC': 'AUC_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                     model  Precision_valid_set  Recall_valid_set  \
0      Logistic_Regression             0.295238          0.553571   
1  DecisionTree Classifier             0.287500          0.410714   
2            Random Forest             0.256198          0.553571   
3       AdaBoostClassifier             0.155989          1.000000   
4                      GBM             0.305882          0.464286   
5                      SVM             0.309859          0.392857   
6          Neural Networks             0.288660          0.500000   
7               GaussianNB             0.222222          0.464286   
8               KNeighbors             0.297297          0.392857   

   f1-score_valid_set  AUC_valid_set  
0            0.385093       0.654674  
1            0.338235       0.611298  
2            0.350282       0.628271  
3            0.269880       0.500000  
4            0.368794       0.634783  
5            0.346457       0.615570  
6            0.366013       0.636139  
7            0.300578       0.581978  
8            0.338462       0.610620  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                     model  Precision_valid_set  Recall_valid_set  \
0      Logistic_Regression                  6.0               7.0   
1  DecisionTree Classifier                  4.0               3.0   
2            Random Forest                  3.0               7.0   
3       AdaBoostClassifier                  1.0               9.0   
4                      GBM                  8.0               4.0   
5                      SVM                  9.0               1.0   
6          Neural Networks                  5.0               6.0   
7               GaussianNB                  2.0               4.0   
8               KNeighbors                  7.0               1.0   

   f1-score_valid_set  AUC_valid_set  
0                 9.0            9.0  
1                 3.0            4.0  
2                 6.0            6.0  
3                 1.0            1.0  
4                 8.0            7.0  
5                 5.0            5.0  
6                 7.0            8.0  
7                 2.0            2.0  
8                 4.0            3.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                     model  Precision_valid_set  Recall_valid_set  \
0      Logistic_Regression                  6.0               7.0   
4                      GBM                  8.0               4.0   
6          Neural Networks                  5.0               6.0   
2            Random Forest                  3.0               7.0   
5                      SVM                  9.0               1.0   
8               KNeighbors                  7.0               1.0   
1  DecisionTree Classifier                  4.0               3.0   
3       AdaBoostClassifier                  1.0               9.0   
7               GaussianNB                  2.0               4.0   

   f1-score_valid_set  AUC_valid_set  Combined_Column  
0                 9.0            9.0             31.0  
4                 8.0            7.0             27.0  
6                 7.0            8.0             26.0  
2                 6.0            6.0             22.0  
5                 5.0            5.0             20.0  
8                 4.0            3.0             15.0  
1                 3.0            4.0             14.0  
3                 1.0            1.0             12.0  
7                 2.0            2.0             10.0  
Selected Model - Fine_Tuning With Grid Search
model = LogisticRegression(random_state=1)

# Define the expanded grid of parameters to search through

param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'fit_intercept': [True, False],
    'class_weight': [None, 'balanced'],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'max_iter': [50, 100, 200],
    'multi_class': ['auto', 'ovr', 'multinomial'],
    'warm_start': [True, False]
}

# Perform GridSearchCV with 3-fold cross-validation
Personality_Diss_Final_Model_L_R = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
Personality_Diss_Final_Model_L_R.fit(X_train, y_train)
GridSearchCV(cv=3, estimator=LogisticRegression(random_state=1),
             param_grid={'class_weight': [None, 'balanced'],
                         'fit_intercept': [True, False],
                         'max_iter': [50, 100, 200],
                         'multi_class': ['auto', 'ovr', 'multinomial'],
                         'penalty': ['l1', 'l2', 'elasticnet', 'none'],
                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',
                                    'saga'],
                         'warm_start': [True, False]})
best_model_parameters = Personality_Diss_Final_Model_L_R.best_estimator_
best_model_parameters
LogisticRegression(max_iter=50, multi_class='multinomial', penalty='none',
                   random_state=1, warm_start=True)
# Predictions on the train set
y_train_pred = Personality_Diss_Final_Model_L_R.predict(X_train)

# Predictions on the valid set
y_valid_pred = Personality_Diss_Final_Model_L_R.predict(X_valid)
Model Evaluation on valid envairment
### validation Set Evaluation Metrics

print('accuracy_score :' + str(accuracy_score(y_valid, y_valid_pred)))
print('precision_score :' + str(precision_score(y_valid, y_valid_pred)))
print('recall_score :' + str(recall_score(y_valid, y_valid_pred)))
print('f1_score :' + str(f1_score(y_valid, y_valid_pred)))
accuracy_score :0.83008356545961
precision_score :0.4
recall_score :0.17857142857142858
f1_score :0.2469135802469136
confusion_matrix = metrics.confusion_matrix(y_valid, y_valid_pred)
#confusion_matrix(y_valid, y_valid_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()
 
Model Evaluation on valid envairment - Set constant cutoffs
# Get the predicted probabilities for the positive class
Personality_Diss_GrideSearch_Model = Personality_Diss_Final_Model_L_R.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.2  # Adjust this threshold as needed

# Apply the new cutoff point
Final_Personality_Diss_Pred_cut = (Personality_Diss_GrideSearch_Model >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, Final_Personality_Diss_Pred_cut)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.68,
 'Precision': 0.27,
 'Recall': 0.64,
 'f1-score': 0.39,
 'Log-loss': 11.06,
 'AUC': 0.66}
pd.crosstab(y_valid, Final_Personality_Diss_Pred_cut)
col_0                           0   1
Personality_Dis_Safek_Target         
0.0                           208  95
1.0                            20  36
Save Personality_Disq selected model - Constant Set Hyper Parameters
import joblib

# Assuming Model_Linear_Regression is already trained

# Save the model to a file

joblib.dump((Personality_Diss_Final_Model_L_R, cutoff_point), 'Personality_Diss_Final_Model_L_R.pkl')
#joblib.dump(Personality_Diss_Final_Model_L_R, 'Personality_Diss_Final_Model_L_R.pkl')
['Personality_Diss_Final_Model_L_R.pkl']
Gibushon Grade Prediction Models
###Change target variable to categor type

Gibushon_Grade_Sofi_Gold_List["Gibushon_Target"] = Gibushon_Grade_Sofi_Gold_List["Gibushon_Target"].astype('category')
#Gibushon_Grade_Sofi_Gold_List.info()
re-orginize variables in final file
Gibushon_Grade_Sofi_Gold_List = pd.DataFrame(Gibushon_Grade_Sofi_Gold_List)
temp_cols=Gibushon_Grade_Sofi_Gold_List.columns.tolist()
index=Gibushon_Grade_Sofi_Gold_List.columns.get_loc("Gibushon_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Gibushon_Grade_Sofi_Gold_List=Gibushon_Grade_Sofi_Gold_List[new_cols]
Gibushon_Grade_Sofi_Gold_List = pd.DataFrame(Gibushon_Grade_Sofi_Gold_List)
temp_cols=Gibushon_Grade_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Gibushon_Grade_Sofi_Gold_List=Gibushon_Grade_Sofi_Gold_List[new_cols]
#Gibushon_Grade_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Gibushon_Grade_Sofi_Gold_List = Gibushon_Grade_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Gibushon_Grade_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Gibushon_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

#Gibushon_Grade_Sofi_Gold_List=Gibushon_Grade_Sofi_Gold_List.apply(zscore)
#Gibushon_Grade_Sofi_Gold_List.head()
data = pd.DataFrame(Gibushon_Grade_Sofi_Gold_List)
#data
#data = pd.DataFrame(Gibushon_Grade_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Gibushon_Target"])
X_data = data[data.columns[~data.columns.isin(['Gibushon_Target'])]]
y_data = data['Gibushon_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.5, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (96, 6)
Validation data shape: (95, 6)
Test data shape: (1, 6)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Rama_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Linear Regression
Algorithm Setting
Model_Linear_Regression = LinearRegression()
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Model_Linear_Regression.fit(X_train, y_train)
LinearRegression()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Linear_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.34, 'MSE': 0.56, 'RMSE': 0.75, 'MAE': 0.6}
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Linear_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.28, 'MSE': 0.68, 'RMSE': 0.83, 'MAE': 0.68}
model_dict = {'model': "Linear-Regression"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.28  0.68  0.83  0.68
Cross Validation
from sklearn.model_selection import cross_validate
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Model_Linear_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.78481582 0.78442408 0.85848343]
Mean RMSE from cross-validation: 0.809241108799244
Cross-validated R-squared scores: [0.34231665 0.14127586 0.23440867]
Mean R-squared from cross-validation: 0.23933372743198766
Linear Reggresion Regulation
ridge
Algorithm Setting
Ridge_Regression = Ridge(alpha=1.0, max_iter=3, solver='auto', random_state=5)
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Ridge_Regression.fit(X_train, y_train)
Ridge(max_iter=3, random_state=5)
Model Evaluation on train envairment
y_pred_train_enva = Ridge_Regression.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.33, 'MSE': 0.57, 'RMSE': 0.75, 'MAE': 0.59}
Model Evaluation on valid envairment
y_pred_valid_enva = Ridge_Regression.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.27, 'MSE': 0.69, 'RMSE': 0.83, 'MAE': 0.68}
model_dict = {'model': "Ridge-Regression"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.28  0.68  0.83  0.68
1   Ridge-Regression      0.27  0.69  0.83  0.68
Cross Validation
# Cross-validation

from sklearn.model_selection import cross_validate
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Ridge_Regression, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.78955623 0.79101436 0.8556647 ]
Mean RMSE from cross-validation: 0.8120784285547448
Cross-validated R-squared scores: [0.33434763 0.12678623 0.23942788]
Mean R-squared from cross-validation: 0.2335205828741946
Lasso
Algorithm Setting
Lasso_Gibushon_Grade = Lasso(random_state=3)
parameters :
(alpha=1.0, *, fit_intercept=True, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Lasso_Gibushon_Grade.fit(X_train, y_train)
Lasso(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Lasso_Gibushon_Grade.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.05, 'MSE': 0.81, 'RMSE': 0.9, 'MAE': 0.72}
Model Evaluation on valid envairment
y_pred_valid_enva = Lasso_Gibushon_Grade.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.02, 'MSE': 0.93, 'RMSE': 0.97, 'MAE': 0.8}
model_dict = {'model': "Lasso-Regression"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.28  0.68  0.83  0.68
1   Ridge-Regression      0.27  0.69  0.83  0.68
2   Lasso-Regression      0.02  0.93  0.97  0.80
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Lasso_Gibushon_Grade, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.9741938  0.86490738 1.04222967]
Mean RMSE from cross-validation: 0.9604436126817383
Cross-validated R-squared scores: [-0.01337939 -0.04397727 -0.12839209]
Mean R-squared from cross-validation: -0.06191625101973908
Elastic_net
Algorithm Setting
Elastic_Net = ElasticNet(random_state=3)
parameters :
(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')
Model Fit Base on Train Set Only :
Elastic_Net.fit(X_train, y_train)
ElasticNet(random_state=3)
Model Evaluation on train envairment
y_pred_train_enva = Elastic_Net.predict(X_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.08, 'MSE': 0.78, 'RMSE': 0.88, 'MAE': 0.71}
Model Evaluation on valid envairment
y_pred_valid_enva = Elastic_Net.predict(X_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.03, 'MSE': 0.92, 'RMSE': 0.96, 'MAE': 0.79}
model_dict = {'model': "Elastic_Net"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.28  0.68  0.83  0.68
1   Ridge-Regression      0.27  0.69  0.83  0.68
2   Lasso-Regression      0.02  0.93  0.97  0.80
3        Elastic_Net      0.03  0.92  0.96  0.79
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Elastic_Net, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.95425139 0.86430529 1.02116527]
Mean RMSE from cross-validation: 0.9465739843005346
Cross-validated R-squared scores: [ 0.02768508 -0.0425243  -0.08324139]
Mean R-squared from cross-validation: -0.03269353625680471
Random Forrest
Algorithm Setting
Random_Forest = RandomForestRegressor(n_estimators=15,  # Number of trees in the forest
                                      max_depth=10,      # Maximum depth of the trees
                                      min_samples_split=15,  # Minimum number of samples required to split an internal node
                                      min_samples_leaf=15,   # Minimum number of samples required to be at a leaf node
                                      max_features='sqrt',  # Number of features to consider when looking for the best split
                                      random_state=10)      # Random state for reproducibility
parameters :
(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)
Model Fit Base on Train Set Only :
Random_Forest.fit(X_train, y_train)
RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=15,
                      min_samples_split=15, n_estimators=15, random_state=10)
Model Evaluation on train envairment
y_pred_train_enva = Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.21, 'MSE': 0.67, 'RMSE': 0.82, 'MAE': 0.65}
Model Evaluation on valid envairment
y_pred_valid_enva = Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.15, 'MSE': 0.81, 'RMSE': 0.9, 'MAE': 0.75}
model_dict = {'model': "Random_Forest"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
               model  r2_score   MSE  RMSE   MAE
0  Linear-Regression      0.28  0.68  0.83  0.68
1   Ridge-Regression      0.27  0.69  0.83  0.68
2   Lasso-Regression      0.02  0.93  0.97  0.80
3        Elastic_Net      0.03  0.92  0.96  0.79
4      Random_Forest      0.15  0.81  0.90  0.75
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Random_Forest, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.86727292 0.79299185 0.95944488]
Mean RMSE from cross-validation: 0.8732365537551314
Cross-validated R-squared scores: [0.19685691 0.12241479 0.04374607]
Mean R-squared from cross-validation: 0.12100592492684296
AdaBoost-Regressor
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Initialize DecisionTreeRegressor as the base estimator
base_estimator = DecisionTreeRegressor(max_depth=3)  # Adjust max_depth to control the complexity of the base estimator

# Initialize AdaBoostRegressor with hyperparameters to reduce overfitting
AdaBoost_Regressor = AdaBoostRegressor(base_estimator=base_estimator,
                                       n_estimators=100,     # Number of weak learners (base estimators)
                                       learning_rate=0.1,    # Learning rate shrinks the contribution of each base estimator
                                       loss='exponential',        # The loss function to use (linear, square, exponential)
                                       random_state=42)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
AdaBoost_Regressor.fit(X_train, y_train)
AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),
                  learning_rate=0.1, loss='exponential', n_estimators=100,
                  random_state=42)
Model Evaluation on train envairment
y_pred_train_enva = AdaBoost_Regressor.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.64, 'MSE': 0.3, 'RMSE': 0.55, 'MAE': 0.44}
Model Evaluation on valid envairment
y_pred_valid_enva = AdaBoost_Regressor.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.14, 'MSE': 0.82, 'RMSE': 0.9, 'MAE': 0.73}
model_dict = {'model': "AdaBoost_Regressor"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.28  0.68  0.83  0.68
1    Ridge-Regression      0.27  0.69  0.83  0.68
2    Lasso-Regression      0.02  0.93  0.97  0.80
3         Elastic_Net      0.03  0.92  0.96  0.79
4       Random_Forest      0.15  0.81  0.90  0.75
5  AdaBoost_Regressor      0.14  0.82  0.90  0.73
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(AdaBoost_Regressor, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.86108869 0.77657357 0.944751  ]
Mean RMSE from cross-validation: 0.8608044189458234
Cross-validated R-squared scores: [0.20826997 0.15837805 0.07281181]
Mean R-squared from cross-validation: 0.1464866108652079
Gradient Boosting Machine
Algorithm Setting
from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor
# Initialize GradientBoostingRegressor with hyperparameters to reduce overfitting
Gradient_Boosting = GradientBoostingRegressor(n_estimators=100,     # Number of boosting stages to be performed
                                              learning_rate=0.1,    # Learning rate shrinks the contribution of each tree
                                              max_depth=3,          # Maximum depth of the individual trees
                                              min_samples_split=25,  # Minimum number of samples required to split an internal node
                                              min_samples_leaf=25,   # Minimum number of samples required to be at a leaf node
                                              max_features='sqrt',  # Number of features to consider when looking for the best split
                                              loss='ls',            # Loss function to be optimized ('ls' refers to least squares regression)
                                              random_state=17)      # Random state for reproducibility
parameters :
(random_state=3, base_estimator=None, n_estimators=1000,learning_rate=0.001) (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Gradient_Boosting.fit(X_train, y_train)
GradientBoostingRegressor(loss='ls', max_features='sqrt', min_samples_leaf=25,
                          min_samples_split=25, random_state=17)
Model Evaluation on train envairment
y_pred_train_enva = Gradient_Boosting.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.35, 'MSE': 0.55, 'RMSE': 0.74, 'MAE': 0.58}
Model Evaluation on valid envairment
y_pred_valid_enva = Gradient_Boosting.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.06, 'MSE': 0.89, 'RMSE': 0.95, 'MAE': 0.76}
model_dict = {'model': "Gradient_Boosting"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.28  0.68  0.83  0.68
1    Ridge-Regression      0.27  0.69  0.83  0.68
2    Lasso-Regression      0.02  0.93  0.97  0.80
3         Elastic_Net      0.03  0.92  0.96  0.79
4       Random_Forest      0.15  0.81  0.90  0.75
5  AdaBoost_Regressor      0.14  0.82  0.90  0.73
6   Gradient_Boosting      0.06  0.89  0.95  0.76
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Gradient_Boosting, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.93420438 0.83076107 0.96701956]
Mean RMSE from cross-validation: 0.9106616697208554
Cross-validated R-squared scores: [0.06810893 0.03682741 0.0285875 ]
Mean R-squared from cross-validation: 0.04450794756670029

kNN
Algorithm Setting
Knn_model = KNeighborsRegressor(n_neighbors=5,  # Number of neighbors to consider
                                weights='uniform',  # Weight function used in prediction ('uniform' for equal weights)
                                algorithm='brute',  # Algorithm used to compute the nearest neighbors ('auto' for automatic selection)
                                leaf_size=30,      # Leaf size passed to BallTree or KDTree
                                p=2)               # Power parameter for the Minkowski metric (2 for Euclidean distance)
parameters :
(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Knn_model.fit(X_train, y_train)
KNeighborsRegressor(algorithm='brute')
Model Evaluation on train envairment
y_pred_train_enva = Knn_model.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.33, 'MSE': 0.57, 'RMSE': 0.76, 'MAE': 0.6}
Model Evaluation on valid envairment
y_pred_valid_enva = Knn_model.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.01, 'MSE': 0.94, 'RMSE': 0.97, 'MAE': 0.77}
model_dict = {'model': "Knn-model"}
Rama_models_list_validation = Rama_models_list_validation.append({**model_dict, **regressionMetrics(y_valid, y_pred_valid_enva)}, ignore_index=True)
Rama_models_list_validation.round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.28  0.68  0.83  0.68
1    Ridge-Regression      0.27  0.69  0.83  0.68
2    Lasso-Regression      0.02  0.93  0.97  0.80
3         Elastic_Net      0.03  0.92  0.96  0.79
4       Random_Forest      0.15  0.81  0.90  0.75
5  AdaBoost_Regressor      0.14  0.82  0.90  0.73
6   Gradient_Boosting      0.06  0.89  0.95  0.76
7           Knn-model      0.01  0.94  0.97  0.77
Cross Validation
# Assuming Ridge_Regression is your Ridge model
cv_scores = cross_validate(Knn_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Cross-validated RMSE scores: [0.97154838 0.87428542 1.05549159]
Mean RMSE from cross-validation: 0.9671084639075551
Cross-validated R-squared scores: [-0.00788321 -0.06673935 -0.1572914 ]
Mean R-squared from cross-validation: -0.07730465560159816
Selecting the Highest-Quality Model
Rama_models_list_validation.sort_values('r2_score',ascending=False).round(2)
                model  r2_score   MSE  RMSE   MAE
0   Linear-Regression      0.28  0.68  0.83  0.68
1    Ridge-Regression      0.27  0.69  0.83  0.68
4       Random_Forest      0.15  0.81  0.90  0.75
5  AdaBoost_Regressor      0.14  0.82  0.90  0.73
6   Gradient_Boosting      0.06  0.89  0.95  0.76
3         Elastic_Net      0.03  0.92  0.96  0.79
2    Lasso-Regression      0.02  0.93  0.97  0.80
7           Knn-model      0.01  0.94  0.97  0.77
selected_columns = ['model', 'r2_score', 'MSE', 'RMSE', 'MAE']
validation_ranked_list = Rama_models_list_validation[selected_columns];

column_mapping = {'r2_score': 'r2_score_valid_set','MSE': 'MSE_valid_set','RMSE': 'RMSE_valid_set','MAE': 'MAE_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.279049       0.684125        0.827118   
1    Ridge-Regression            0.273657       0.689241        0.830205   
2    Lasso-Regression            0.018157       0.931690        0.965241   
3         Elastic_Net            0.028302       0.922064        0.960241   
4       Random_Forest            0.150130       0.806458        0.898030   
5  AdaBoost_Regressor            0.138882       0.817132        0.903953   
6   Gradient_Boosting            0.057419       0.894433        0.945745   
7           Knn-model            0.010620       0.938842        0.968939   

   MAE_valid_set  
0       0.680181  
1       0.679093  
2       0.795998  
3       0.792987  
4       0.748641  
5       0.732435  
6       0.759835  
7       0.769474  
validation_ranked_list[['MSE_valid_set', 'RMSE_valid_set', 'MAE_valid_set']] *= -1
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression            0.279049      -0.684125       -0.827118   
1    Ridge-Regression            0.273657      -0.689241       -0.830205   
2    Lasso-Regression            0.018157      -0.931690       -0.965241   
3         Elastic_Net            0.028302      -0.922064       -0.960241   
4       Random_Forest            0.150130      -0.806458       -0.898030   
5  AdaBoost_Regressor            0.138882      -0.817132       -0.903953   
6   Gradient_Boosting            0.057419      -0.894433       -0.945745   
7           Knn-model            0.010620      -0.938842       -0.968939   

   MAE_valid_set  
0      -0.680181  
1      -0.679093  
2      -0.795998  
3      -0.792987  
4      -0.748641  
5      -0.732435  
6      -0.759835  
7      -0.769474  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 8.0            8.0             8.0   
1    Ridge-Regression                 7.0            7.0             7.0   
2    Lasso-Regression                 2.0            2.0             2.0   
3         Elastic_Net                 3.0            3.0             3.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 5.0            5.0             5.0   
6   Gradient_Boosting                 4.0            4.0             4.0   
7           Knn-model                 1.0            1.0             1.0   

   MAE_valid_set  
0            7.0  
1            8.0  
2            1.0  
3            2.0  
4            5.0  
5            6.0  
6            4.0  
7            3.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                model  r2_score_valid_set  MSE_valid_set  RMSE_valid_set  \
0   Linear-Regression                 8.0            8.0             8.0   
1    Ridge-Regression                 7.0            7.0             7.0   
4       Random_Forest                 6.0            6.0             6.0   
5  AdaBoost_Regressor                 5.0            5.0             5.0   
6   Gradient_Boosting                 4.0            4.0             4.0   
3         Elastic_Net                 3.0            3.0             3.0   
2    Lasso-Regression                 2.0            2.0             2.0   
7           Knn-model                 1.0            1.0             1.0   

   MAE_valid_set  Combined_Column  
0            7.0             31.0  
1            8.0             29.0  
4            5.0             23.0  
5            6.0             21.0  
6            4.0             16.0  
3            2.0             11.0  
2            1.0              7.0  
7            3.0              6.0  
Selected Model - Fine_Tuning With Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import make_scorer, mean_squared_error, r2_score

# Linear Regression model
Final_Model_Linear_Regression = Ridge()

# Define the parameter grid for GridSearchCV
param_grid = {
    'alpha': [0.1, 1.0, 10.0],  # Regularization strength
    'fit_intercept': [True, False],
    'normalize': [True, False],
    'copy_X': [True, False],  # Whether to copy X data before fitting
    'max_iter': [1000, 2000],  # Maximum number of iterations
}

# Create GridSearchCV object
Gibushon_Grade_Model_Linear_Regression = GridSearchCV(Final_Model_Linear_Regression, param_grid, scoring='neg_mean_squared_error', cv=3)

# Fit the model with grid search
Gibushon_Grade_Model_Linear_Regression.fit(X_data, y_data)

# Best parameters from the grid search
best_params = Gibushon_Grade_Model_Linear_Regression.best_params_
print(f"Best parameters: {best_params}")

# Best model from the grid search
best_model = Gibushon_Grade_Model_Linear_Regression.best_estimator_

# Evaluate the best model on training data
y_pred_train_enva = best_model.predict(X_train)

# Display scatter plot for training data
sns.scatterplot(y_pred_train_enva, y_train)

# Evaluate metrics for training data
metrics_train = regressionMetrics(y_train, y_pred_train_enva)
formatted_metrics_train = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_train.items()}
print("Metrics on training data:")
print(formatted_metrics_train)

# Evaluate the best model on validation data
y_pred_valid_enva = best_model.predict(X_valid)

# Display scatter plot for validation data
#sns.scatterplot(y_pred_valid_enva, y_valid)

# Evaluate metrics for validation data
metrics_valid = regressionMetrics(y_valid, y_pred_valid_enva)
formatted_metrics_valid = {key: value.round(2) if isinstance(value, float) else value for key, value in metrics_valid.items()}
print("Metrics on validation data:")
print(formatted_metrics_valid)

# Cross-validate the best model
cv_scores = cross_validate(best_model, X_data, y_data, scoring=["neg_mean_squared_error", "r2"], cv=3)

# Extract RMSE scores
cv_rmse_scores = np.sqrt(-cv_scores["test_neg_mean_squared_error"])
print(f"Cross-validated RMSE scores: {cv_rmse_scores}")
print(f"Mean RMSE from cross-validation: {np.mean(cv_rmse_scores)}")

# Extract R-squared scores
cv_r2_scores = cv_scores["test_r2"]
print(f"Cross-validated R-squared scores: {cv_r2_scores}")
print(f"Mean R-squared from cross-validation: {np.mean(cv_r2_scores)}")
Best parameters: {'alpha': 0.1, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'normalize': True}
Metrics on training data:
{'r2_score': 0.32, 'MSE': 0.58, 'RMSE': 0.76, 'MAE': 0.6}
Metrics on validation data:
{'r2_score': 0.33, 'MSE': 0.63, 'RMSE': 0.8, 'MAE': 0.66}
Cross-validated RMSE scores: [0.78612462 0.77687094 0.86120729]
Mean RMSE from cross-validation: 0.8080676179972727
Cross-validated R-squared scores: [0.34012125 0.15773337 0.22954271]
Mean R-squared from cross-validation: 0.24246577459711213
 
Save Gibushon grade selected model - Auto Set Hyper Parameters
#import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
#joblib.dump(Gibushon_Grade_Model_Linear_Regression, 'Gibushon_Grade_Model_Linear_Regression.pkl')
Final Algorithm Setting
Gibushon_Grade_Model_Linear_Regression = Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000, normalize=True)
Gibushon_Grade_Model_Linear_Regression.fit(X_train, y_train)
Ridge(alpha=0.1, max_iter=1000, normalize=True)
###Model Evaluation on train envairment

y_pred_train_enva = Gibushon_Grade_Model_Linear_Regression.predict(X_train)

Metrics = regressionMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.34, 'MSE': 0.56, 'RMSE': 0.75, 'MAE': 0.59}
Model Evaluation on valid envairment
###Model Evaluation on valid envairment

y_pred_valid_enva = Gibushon_Grade_Model_Linear_Regression.predict(X_valid)

Metrics = regressionMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'r2_score': 0.28, 'MSE': 0.68, 'RMSE': 0.83, 'MAE': 0.68}
Save Gibushon final grade selected model - Constant Set Hyper Parameters
import joblib

### Assuming Model_Linear_Regression is already trained

### Save the model to a file
joblib.dump(Gibushon_Grade_Model_Linear_Regression, 'Gibushon_Grade_Model_Linear_Regression.pkl')
['Gibushon_Grade_Model_Linear_Regression.pkl']
Gius Result Prediction Models
###Change target variable to categor type

Gius_Sofi_Gold_List["Gius_Target"] = Gius_Sofi_Gold_List["Gius_Target"].astype('category')
#Gius_Sofi_Gold_List.info()
re-orginize variables in final file
Gius_Sofi_Gold_List = pd.DataFrame(Gius_Sofi_Gold_List)
temp_cols=Gius_Sofi_Gold_List.columns.tolist()
index=Gius_Sofi_Gold_List.columns.get_loc("Gius_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Gius_Sofi_Gold_List=Gius_Sofi_Gold_List[new_cols]
Gius_Sofi_Gold_List = pd.DataFrame(Gius_Sofi_Gold_List)
temp_cols=Gius_Sofi_Gold_List.columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Gius_Sofi_Gold_List=Gius_Sofi_Gold_List[new_cols]
#Gius_Sofi_Gold_List.head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Gibushon_Sofi_Gold_List = Gibushon_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Gius_Sofi_Gold_List)
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Gius_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

# Get the list of columns to compute z-scores for
columns_to_compute_zscore = [col for col in Gius_Sofi_Gold_List.columns if col != 'Gius_Target']

# Compute z-scores for each column except "Gius_Target"
#Gius_Sofi_Gold_List[columns_to_compute_zscore] = Gius_Sofi_Gold_List[columns_to_compute_zscore].apply(zscore)

# Now, z-scores are computed for all variables in the DataFrame except "Gius_Target"
#Gius_Sofi_Gold_List
data = pd.DataFrame(Gius_Sofi_Gold_List)
#data
#data = pd.DataFrame(Gius_Sofi_Gold_List, columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Gius_Target"])
X_data = data[data.columns[~data.columns.isin(['Gius_Target'])]]
y_data = data['Gius_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.4, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (329, 11)
Validation data shape: (219, 11)
Test data shape: (1, 11)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Gius_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Logistic Regression
Algorithm Setting
Model_Logistic_Regression = LogisticRegression(random_state=1)
parameters :
parameters : (penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)
Model Fit Base on Train Set Only :
Model_Logistic_Regression.fit(X_train, y_train)
LogisticRegression(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Logistic_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.69,
 'Precision': 0.61,
 'Recall': 0.37,
 'f1-score': 0.46,
 'Log-loss': 10.71,
 'AUC': 0.62}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          184   28
1.0           74   43
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Logistic_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.64,
 'Precision': 0.75,
 'Recall': 0.38,
 'f1-score': 0.5,
 'Log-loss': 12.46,
 'AUC': 0.63}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0          100   13
1.0           66   40
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Logistic_Regression.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.65,
 'Precision': 0.69,
 'Recall': 0.5,
 'f1-score': 0.58,
 'Log-loss': 12.14,
 'AUC': 0.64}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          89  24
1.0          53  53
model_dict = {'model': "Logistic_Regression"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                 model  Accuracy  Precision  Recall  f1-score  Log-loss   AUC
0  Logistic_Regression      0.65       0.69     0.5      0.58     12.14  0.64
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Logistic_Regression is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Logistic_Regression, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Logistic_Regression.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.6557377  0.64480874 0.6557377 ]
Mean Accuracy from cross-validation: 0.6520947176684881
Cross-validated Precision scores: [0.58571429 0.59615385 0.6       ]
Mean Precision from cross-validation: 0.593956043956044
Cross-validated Recall scores: [0.54666667 0.41333333 0.44594595]
Mean Recall from cross-validation: 0.4686486486486487
Cross-validated F1-score scores: [0.56551724 0.48818898 0.51162791]
Mean F1-score from cross-validation: 0.5217780415780024
Cross-validated Log-loss scores: [-0.57217458 -0.57300378 -0.6137035 ]
Mean Log-loss from cross-validation: 0.5862939535225724
Cross-validated AUC scores: [0.75469136 0.75296296 0.7081577 ]
Mean AUC from cross-validation: 0.7386040066570138
Accuracy: 0.6612021857923497
Precision: 0.6079545454545454
Recall: 0.47767857142857145
F1 Score: 0.535
Log-loss: 11.701762443854408
AUC: 0.6326854395604395
DecisionTree Classifier
Algorithm Setting
Model_Decision_Tree = DecisionTreeClassifier(random_state=1)
parameters : (criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)
Model Fit Base on Train Set Only :
Model_Decision_Tree.fit(X_train, y_train)
DecisionTreeClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Decision_Tree.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 1.0,
 'Precision': 1.0,
 'Recall': 1.0,
 'f1-score': 1.0,
 'Log-loss': 0.0,
 'AUC': 1.0}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          212    0
1.0            0  117
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Decision_Tree.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.51,
 'Precision': 0.49,
 'Recall': 0.38,
 'f1-score': 0.43,
 'Log-loss': 17.03,
 'AUC': 0.5}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           71   42
1.0           66   40
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Decision_Tree.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.51,
 'Precision': 0.49,
 'Recall': 0.38,
 'f1-score': 0.43,
 'Log-loss': 17.03,
 'AUC': 0.5}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          71  42
1.0          66  40
model_dict = {'model': "DecisionTree Classifier"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   

    AUC  
0  0.64  
1  0.50  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Decision_Tree is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Decision_Tree, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Decision_Tree.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.54098361 0.6284153  0.54098361]
Mean Accuracy from cross-validation: 0.5701275045537341
Cross-validated Precision scores: [0.44705882 0.54545455 0.4375    ]
Mean Precision from cross-validation: 0.47667112299465236
Cross-validated Recall scores: [0.50666667 0.56       0.47297297]
Mean Recall from cross-validation: 0.5132132132132133
Cross-validated F1-score scores: [0.475      0.55263158 0.45454545]
Mean F1-score from cross-validation: 0.49405901116427436
Cross-validated Log-loss scores: [-15.85406994 -12.83423377 -15.8540612 ]
Mean Log-loss from cross-validation: 14.847454969166767
Cross-validated AUC scores: [0.53574074 0.61796296 0.53006447]
Mean AUC from cross-validation: 0.5612560572805222
Accuracy: 0.8032786885245902
Precision: 0.79
Recall: 0.7053571428571429
F1 Score: 0.7452830188679247
Log-loss: 6.79457456055086
AUC: 0.7880631868131868
Random Forest
Algorithm Setting
Model_Random_Forest = RandomForestClassifier(random_state=1)
parameters :
parameters : (n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)
Model Fit Base on Train Set Only :
Model_Random_Forest.fit(X_train, y_train)
RandomForestClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Random_Forest.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 1.0,
 'Precision': 1.0,
 'Recall': 1.0,
 'f1-score': 1.0,
 'Log-loss': 0.0,
 'AUC': 1.0}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          212    0
1.0            0  117
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Random_Forest.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.58,
 'Precision': 0.63,
 'Recall': 0.35,
 'f1-score': 0.45,
 'Log-loss': 14.35,
 'AUC': 0.58}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           91   22
1.0           69   37
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Random_Forest.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.58,
 'Precision': 0.6,
 'Recall': 0.4,
 'f1-score': 0.48,
 'Log-loss': 14.51,
 'AUC': 0.57}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          85  28
1.0          64  42
model_dict = {'model': "Random Forest"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   

    AUC  
0  0.64  
1  0.50  
2  0.57  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Random_Forest is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Random_Forest, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Random_Forest.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.6557377  0.63934426 0.6010929 ]
Mean Accuracy from cross-validation: 0.6320582877959927
Cross-validated Precision scores: [0.59090909 0.57377049 0.50793651]
Mean Precision from cross-validation: 0.5575386968829591
Cross-validated Recall scores: [0.52       0.46666667 0.43243243]
Mean Recall from cross-validation: 0.47303303303303307
Cross-validated F1-score scores: [0.55319149 0.51470588 0.46715328]
Mean F1-score from cross-validation: 0.5116835521287254
Cross-validated Log-loss scores: [-0.60848989 -0.64513678 -0.71317263]
Mean Log-loss from cross-validation: 0.6555997651338663
Cross-validated AUC scores: [0.71037037 0.66271605 0.60004959]
Mean AUC from cross-validation: 0.6577120035427885
Accuracy: 0.8324225865209471
Precision: 0.851063829787234
Recall: 0.7142857142857143
F1 Score: 0.7766990291262136
Log-loss: 5.7879595939158985
AUC: 0.8140659340659342
Adaptive Boosting (ADABoost)
Algorithm Setting
Model_ADABoost = AdaBoostClassifier(random_state=1)
parameters : (estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None, base_estimator='deprecated')
Model Fit Base on Train Set Only :
Model_ADABoost.fit(X_train, y_train)
AdaBoostClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_ADABoost.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.77,
 'Precision': 0.72,
 'Recall': 0.59,
 'f1-score': 0.65,
 'Log-loss': 7.87,
 'AUC': 0.73}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          185   27
1.0           48   69
Model Evaluation on valid envairment
y_pred_valid_enva = Model_ADABoost.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.59,
 'Precision': 0.65,
 'Recall': 0.33,
 'f1-score': 0.44,
 'Log-loss': 14.19,
 'AUC': 0.58}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           94   19
1.0           71   35
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_ADABoost.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.52,
 'Precision': 0.5,
 'Recall': 0.96,
 'f1-score': 0.66,
 'Log-loss': 16.72,
 'AUC': 0.53}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0    1
Gius_Target         
0.0          11  102
1.0           4  102
model_dict = {'model': "AdaBoostClassifier"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   

    AUC  
0  0.64  
1  0.50  
2  0.57  
3  0.53  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_ADABoost is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_ADABoost, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_ADABoost.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.67759563 0.67213115 0.64480874]
Mean Accuracy from cross-validation: 0.6648451730418944
Cross-validated Precision scores: [0.6025641  0.61904762 0.58490566]
Mean Precision from cross-validation: 0.6021724606630268
Cross-validated Recall scores: [0.62666667 0.52       0.41891892]
Mean Recall from cross-validation: 0.5218618618618619
Cross-validated F1-score scores: [0.61437908 0.56521739 0.48818898]
Mean F1-score from cross-validation: 0.5559284842165403
Cross-validated Log-loss scores: [-0.67995665 -0.68324423 -0.67622022]
Mean Log-loss from cross-validation: 0.6798070334577888
Cross-validated AUC scores: [0.73783951 0.72654321 0.67394   ]
Mean AUC from cross-validation: 0.7127742370300983
Accuracy: 0.46083788706739526
Precision: 0.4296875
Recall: 0.9821428571428571
F1 Score: 0.5978260869565216
Log-loss: 18.62242494598029
AUC: 0.5418406593406594
Gradient Boosting Machine (GBM)
Algorithm Setting
Model_Gradient= GradientBoostingClassifier(random_state=1)
parameters : (*, loss='log_loss', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)
Model Fit Base on Train Set Only :
Model_Gradient.fit(X_train, y_train)
GradientBoostingClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Gradient.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.9,
 'Precision': 0.95,
 'Recall': 0.77,
 'f1-score': 0.85,
 'Log-loss': 3.36,
 'AUC': 0.87}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          207    5
1.0           27   90
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Gradient.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.6,
 'Precision': 0.68,
 'Recall': 0.32,
 'f1-score': 0.44,
 'Log-loss': 13.88,
 'AUC': 0.59}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           97   16
1.0           72   34
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Gradient.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.61,
 'Precision': 0.67,
 'Recall': 0.38,
 'f1-score': 0.48,
 'Log-loss': 13.56,
 'AUC': 0.6}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          93  20
1.0          66  40
model_dict = {'model': "GBM"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   
4                      GBM      0.61       0.67    0.38      0.48     13.56   

    AUC  
0  0.64  
1  0.50  
2  0.57  
3  0.53  
4  0.60  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Gradientis your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Gradient, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Gradient.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.6557377  0.64480874 0.59562842]
Mean Accuracy from cross-validation: 0.6320582877959927
Cross-validated Precision scores: [0.58823529 0.57142857 0.5       ]
Mean Precision from cross-validation: 0.5532212885154061
Cross-validated Recall scores: [0.53333333 0.53333333 0.39189189]
Mean Recall from cross-validation: 0.4861861861861862
Cross-validated F1-score scores: [0.55944056 0.55172414 0.43939394]
Mean F1-score from cross-validation: 0.5168528789218444
Cross-validated Log-loss scores: [-0.62413576 -0.65214168 -0.72477752]
Mean Log-loss from cross-validation: 0.6670183198881552
Cross-validated AUC scores: [0.69444444 0.67592593 0.63067196]
Mean AUC from cross-validation: 0.6670141089101334
Accuracy: 0.8032786885245902
Precision: 0.8295454545454546
Recall: 0.6517857142857143
F1 Score: 0.73
Log-loss: 6.794557083011402
AUC: 0.779739010989011
Support Vector Machine (SVM)
Algorithm Setting
from sklearn.svm import SVC
Model_SVC= SVC(random_state=1, probability=True)
parameters : (*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)
Model Fit Base on Train Set Only :
Model_SVC.fit(X_train, y_train)
SVC(probability=True, random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_SVC.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.64,
 'Precision': 0.0,
 'Recall': 0.0,
 'f1-score': 0.0,
 'Log-loss': 12.28,
 'AUC': 0.5}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0
Gius_Target     
0.0          212
1.0          117
Model Evaluation on valid envairment
y_pred_valid_enva = Model_SVC.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.52,
 'Precision': 0.0,
 'Recall': 0.0,
 'f1-score': 0.0,
 'Log-loss': 16.72,
 'AUC': 0.5}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0
Gius_Target     
0.0          113
1.0          106
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_SVC.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.56,
 'Precision': 0.74,
 'Recall': 0.13,
 'f1-score': 0.22,
 'Log-loss': 15.3,
 'AUC': 0.54}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0          0   1
Gius_Target         
0.0          108   5
1.0           92  14
model_dict = {'model': "SVM"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   
4                      GBM      0.61       0.67    0.38      0.48     13.56   
5                      SVM      0.56       0.74    0.13      0.22     15.30   

    AUC  
0  0.64  
1  0.50  
2  0.57  
3  0.53  
4  0.60  
5  0.54  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_SVCis your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_SVC, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_SVC.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.59016393 0.59016393 0.59562842]
Mean Accuracy from cross-validation: 0.5919854280510018
Cross-validated Precision scores: [0. 0. 0.]
Mean Precision from cross-validation: 0.0
Cross-validated Recall scores: [0. 0. 0.]
Mean Recall from cross-validation: 0.0
Cross-validated F1-score scores: [0. 0. 0.]
Mean F1-score from cross-validation: 0.0
Cross-validated Log-loss scores: [-0.6178847  -0.67942219 -0.66389602]
Mean Log-loss from cross-validation: 0.6537343008105178
Cross-validated AUC scores: [0.70814815 0.69185185 0.63265559]
Mean AUC from cross-validation: 0.6775518637903959
Accuracy: 0.6320582877959927
Precision: 0.775
Recall: 0.13839285714285715
F1 Score: 0.23484848484848486
Log-loss: 12.708269632329381
AUC: 0.5553502747252748
Neural Network - MLP Classifier
from sklearn.neural_network import MLPClassifier
Algorithm Setting
MLP_Classifier = MLPClassifier(random_state=1)
parameters : (hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)
Model Fit Base on Train Set Only :
MLP_Classifier.fit(X_train, y_train)
MLPClassifier(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = MLP_Classifier.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.69,
 'Precision': 0.6,
 'Recall': 0.41,
 'f1-score': 0.49,
 'Log-loss': 10.6,
 'AUC': 0.63}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          180   32
1.0           69   48
Model Evaluation on valid envairment
y_pred_valid_enva = MLP_Classifier.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.65,
 'Precision': 0.73,
 'Recall': 0.45,
 'f1-score': 0.56,
 'Log-loss': 11.99,
 'AUC': 0.65}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           95   18
1.0           58   48
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = MLP_Classifier.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.67,
 'Precision': 0.69,
 'Recall': 0.58,
 'f1-score': 0.63,
 'Log-loss': 11.51,
 'AUC': 0.66}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          85  28
1.0          45  61
model_dict = {'model': "Neural Networks"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   
4                      GBM      0.61       0.67    0.38      0.48     13.56   
5                      SVM      0.56       0.74    0.13      0.22     15.30   
6          Neural Networks      0.67       0.69    0.58      0.63     11.51   

    AUC  
0  0.64  
1  0.50  
2  0.57  
3  0.53  
4  0.60  
5  0.54  
6  0.66  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming MLP_Classifier is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(MLP_Classifier, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = MLP_Classifier.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.67759563 0.64480874 0.63387978]
Mean Accuracy from cross-validation: 0.6520947176684883
Cross-validated Precision scores: [0.59756098 0.58928571 0.55932203]
Mean Precision from cross-validation: 0.5820562412645919
Cross-validated Recall scores: [0.65333333 0.44       0.44594595]
Mean Recall from cross-validation: 0.5130930930930931
Cross-validated F1-score scores: [0.62420382 0.50381679 0.4962406 ]
Mean F1-score from cross-validation: 0.5414204056843134
Cross-validated Log-loss scores: [-0.583552   -0.58293541 -0.60901433]
Mean Log-loss from cross-validation: 0.5918339143475886
Cross-validated AUC scores: [0.74271605 0.73802469 0.7136127 ]
Mean AUC from cross-validation: 0.7314511453349374
Accuracy: 0.6867030965391621
Precision: 0.6326530612244898
Recall: 0.5535714285714286
F1 Score: 0.5904761904761905
Log-loss: 10.820996559088545
AUC: 0.6660164835164836
Gaussian Naiv Bayes
from sklearn.naive_bayes import GaussianNB
Algorithm Setting
Model_GaussianNB = GaussianNB()
parameters : (*, priors=None, var_smoothing=1e-09)
Model Fit Base on Train Set Only :
Model_GaussianNB.fit(X_train, y_train)
GaussianNB()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_GaussianNB.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.65,
 'Precision': 0.51,
 'Recall': 0.63,
 'f1-score': 0.56,
 'Log-loss': 11.97,
 'AUC': 0.65}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          141   71
1.0           43   74
Model Evaluation on valid envairment
y_pred_valid_enva = Model_GaussianNB.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.63,
 'Precision': 0.61,
 'Recall': 0.68,
 'f1-score': 0.64,
 'Log-loss': 12.62,
 'AUC': 0.64}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           67   46
1.0           34   72
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_GaussianNB.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.64,
 'Precision': 0.6,
 'Recall': 0.75,
 'f1-score': 0.67,
 'Log-loss': 12.46,
 'AUC': 0.64}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          60  53
1.0          26  80
model_dict = {'model': "GaussianNB"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   
4                      GBM      0.61       0.67    0.38      0.48     13.56   
5                      SVM      0.56       0.74    0.13      0.22     15.30   
6          Neural Networks      0.67       0.69    0.58      0.63     11.51   
7               GaussianNB      0.64       0.60    0.75      0.67     12.46   

    AUC  
0  0.64  
1  0.50  
2  0.57  
3  0.53  
4  0.60  
5  0.54  
6  0.66  
7  0.64  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_GaussianNB is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_GaussianNB, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_GaussianNB.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.54644809 0.63934426 0.68306011]
Mean Accuracy from cross-validation: 0.6229508196721313
Cross-validated Precision scores: [0.47142857 0.54054054 0.59756098]
Mean Precision from cross-validation: 0.536510029192956
Cross-validated Recall scores: [0.88       0.8        0.66216216]
Mean Recall from cross-validation: 0.7807207207207209
Cross-validated F1-score scores: [0.61395349 0.64516129 0.62820513]
Mean F1-score from cross-validation: 0.6291066356332673
Cross-validated Log-loss scores: [-1.29511691 -1.12026326 -1.0061078 ]
Mean Log-loss from cross-validation: 1.140495991600286
Cross-validated AUC scores: [0.68506173 0.71901235 0.73022564]
Mean AUC from cross-validation: 0.7114332375188644
Accuracy: 0.6557377049180327
Precision: 0.5585284280936454
Recall: 0.7455357142857143
F1 Score: 0.638623326959847
Log-loss: 11.890590683968862
AUC: 0.6696909340659342
K-Neighbors Classifier
from sklearn.neighbors import KNeighborsClassifier
Algorithm Setting
Model_KNeighbors = KNeighborsClassifier()
parameters : (n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
Model Fit Base on Train Set Only :
Model_KNeighbors.fit(X_train, y_train)
KNeighborsClassifier()
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_KNeighbors.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.72,
 'Precision': 0.65,
 'Recall': 0.48,
 'f1-score': 0.55,
 'Log-loss': 9.55,
 'AUC': 0.67}
pd.crosstab(y_train, y_pred_train_enva)
col_0        0.0  1.0
Gius_Target          
0.0          182   30
1.0           61   56
Model Evaluation on valid envairment
y_pred_valid_enva = Model_KNeighbors.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.53,
 'Precision': 0.52,
 'Recall': 0.31,
 'f1-score': 0.39,
 'Log-loss': 16.24,
 'AUC': 0.52}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0        0.0  1.0
Gius_Target          
0.0           83   30
1.0           73   33
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_KNeighbors.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.53,
 'Precision': 0.52,
 'Recall': 0.31,
 'f1-score': 0.39,
 'Log-loss': 16.24,
 'AUC': 0.52}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0         0   1
Gius_Target        
0.0          83  30
1.0          73  33
model_dict = {'model': "KNeighbors"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   
4                      GBM      0.61       0.67    0.38      0.48     13.56   
5                      SVM      0.56       0.74    0.13      0.22     15.30   
6          Neural Networks      0.67       0.69    0.58      0.63     11.51   
7               GaussianNB      0.64       0.60    0.75      0.67     12.46   
8               KNeighbors      0.53       0.52    0.31      0.39     16.24   

    AUC  
0  0.64  
1  0.50  
2  0.57  
3  0.53  
4  0.60  
5  0.54  
6  0.66  
7  0.64  
8  0.52  
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_KNeighbors is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_KNeighbors, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_KNeighbors.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.53551913 0.58469945 0.46994536]
Mean Accuracy from cross-validation: 0.5300546448087432
Cross-validated Precision scores: [0.42857143 0.49315068 0.31147541]
Mean Precision from cross-validation: 0.4110658411130004
Cross-validated Recall scores: [0.4        0.48       0.25675676]
Mean Recall from cross-validation: 0.3789189189189189
Cross-validated F1-score scores: [0.4137931  0.48648649 0.28148148]
Mean F1-score from cross-validation: 0.39392035713874796
Cross-validated Log-loss scores: [-1.09031556 -1.56995213 -2.58270252]
Mean Log-loss from cross-validation: 1.7476567372386984
Cross-validated AUC scores: [0.53932099 0.60530864 0.47167121]
Mean AUC from cross-validation: 0.53876694737551
Accuracy: 0.6466302367941712
Precision: 0.6
Recall: 0.4017857142857143
F1 Score: 0.4812834224598931
Log-loss: 12.205046623785947
AUC: 0.6085851648351649
Selecting the Highest-Quality Model
Gius_models_list_validation.sort_values('Accuracy',ascending=False).round(2)
                     model  Accuracy  Precision  Recall  f1-score  Log-loss  \
6          Neural Networks      0.67       0.69    0.58      0.63     11.51   
0      Logistic_Regression      0.65       0.69    0.50      0.58     12.14   
7               GaussianNB      0.64       0.60    0.75      0.67     12.46   
4                      GBM      0.61       0.67    0.38      0.48     13.56   
2            Random Forest      0.58       0.60    0.40      0.48     14.51   
5                      SVM      0.56       0.74    0.13      0.22     15.30   
8               KNeighbors      0.53       0.52    0.31      0.39     16.24   
3       AdaBoostClassifier      0.52       0.50    0.96      0.66     16.72   
1  DecisionTree Classifier      0.51       0.49    0.38      0.43     17.03   

    AUC  
6  0.66  
0  0.64  
7  0.64  
4  0.60  
2  0.57  
5  0.54  
8  0.52  
3  0.53  
1  0.50  
selected_columns = ['model', 'Precision', 'Recall', 'f1-score', 'AUC']
validation_ranked_list = Gius_models_list_validation[selected_columns];

column_mapping = {'Precision': 'Precision_valid_set','Recall': 'Recall_valid_set','f1-score': 'f1-score_valid_set','AUC': 'AUC_valid_set'};
validation_ranked_list.rename(columns=column_mapping, inplace=True);

validation_ranked_list
                     model  Precision_valid_set  Recall_valid_set  \
0      Logistic_Regression             0.688312          0.500000   
1  DecisionTree Classifier             0.487805          0.377358   
2            Random Forest             0.600000          0.396226   
3       AdaBoostClassifier             0.500000          0.962264   
4                      GBM             0.666667          0.377358   
5                      SVM             0.736842          0.132075   
6          Neural Networks             0.685393          0.575472   
7               GaussianNB             0.601504          0.754717   
8               KNeighbors             0.523810          0.311321   

   f1-score_valid_set  AUC_valid_set  
0            0.579235       0.643805  
1            0.425532       0.502839  
2            0.477273       0.574219  
3            0.658065       0.529805  
4            0.481928       0.600184  
5            0.224000       0.543914  
6            0.625641       0.663842  
7            0.669456       0.642845  
8            0.390533       0.522917  
for col in validation_ranked_list.columns[1:]:
    validation_ranked_list[col] = validation_ranked_list[col].rank(axis=0, method='min');
    
validation_ranked_list
                     model  Precision_valid_set  Recall_valid_set  \
0      Logistic_Regression                  8.0               6.0   
1  DecisionTree Classifier                  1.0               3.0   
2            Random Forest                  4.0               5.0   
3       AdaBoostClassifier                  2.0               9.0   
4                      GBM                  6.0               3.0   
5                      SVM                  9.0               1.0   
6          Neural Networks                  7.0               7.0   
7               GaussianNB                  5.0               8.0   
8               KNeighbors                  3.0               2.0   

   f1-score_valid_set  AUC_valid_set  
0                 6.0            8.0  
1                 3.0            1.0  
2                 4.0            5.0  
3                 8.0            3.0  
4                 5.0            6.0  
5                 1.0            4.0  
6                 7.0            9.0  
7                 9.0            7.0  
8                 2.0            2.0  
validation_ranked_list['Combined_Column'] = validation_ranked_list.sum(axis=1);
validation_ranked_list = validation_ranked_list.sort_values(by='Combined_Column', ascending=False);
validation_ranked_list
                     model  Precision_valid_set  Recall_valid_set  \
6          Neural Networks                  7.0               7.0   
7               GaussianNB                  5.0               8.0   
0      Logistic_Regression                  8.0               6.0   
3       AdaBoostClassifier                  2.0               9.0   
4                      GBM                  6.0               3.0   
2            Random Forest                  4.0               5.0   
5                      SVM                  9.0               1.0   
8               KNeighbors                  3.0               2.0   
1  DecisionTree Classifier                  1.0               3.0   

   f1-score_valid_set  AUC_valid_set  Combined_Column  
6                 7.0            9.0             30.0  
7                 9.0            7.0             29.0  
0                 6.0            8.0             28.0  
3                 8.0            3.0             22.0  
4                 5.0            6.0             20.0  
2                 4.0            5.0             18.0  
5                 1.0            4.0             15.0  
8                 2.0            2.0              9.0  
1                 3.0            1.0              8.0  
Selected Model - Fine_Tuning With Grid Search
model = LogisticRegression(random_state=1)

# Define the expanded grid of parameters to search through

param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', 'none'],
    'fit_intercept': [True, False],
    'class_weight': [None, 'balanced'],
    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
    'max_iter': [50, 100, 200],
    'multi_class': ['auto', 'ovr', 'multinomial'],
    'warm_start': [True, False]
}

# Perform GridSearchCV with 3-fold cross-validation
Gius_Final_Model_L_R = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)
Gius_Final_Model_L_R.fit(X_train, y_train)
GridSearchCV(cv=3, estimator=LogisticRegression(random_state=1),
             param_grid={'class_weight': [None, 'balanced'],
                         'fit_intercept': [True, False],
                         'max_iter': [50, 100, 200],
                         'multi_class': ['auto', 'ovr', 'multinomial'],
                         'penalty': ['l1', 'l2', 'elasticnet', 'none'],
                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',
                                    'saga'],
                         'warm_start': [True, False]})
best_model_parameters = Gius_Final_Model_L_R.best_estimator_
best_model_parameters
LogisticRegression(max_iter=50, penalty='l1', random_state=1,
                   solver='liblinear', warm_start=True)
Model Evaluation on valid envairment
# Predictions on the train set
y_train_pred = Gius_Final_Model_L_R.predict(X_train)

# Predictions on the valid set
y_valid_pred = Gius_Final_Model_L_R.predict(X_valid)
### validation Set Evaluation Metrics

print('accuracy_score :' + str(accuracy_score(y_valid, y_valid_pred)))
print('precision_score :' + str(precision_score(y_valid, y_valid_pred)))
print('recall_score :' + str(recall_score(y_valid, y_valid_pred)))
print('f1_score :' + str(f1_score(y_valid, y_valid_pred)))
accuracy_score :0.6438356164383562
precision_score :0.75
recall_score :0.39622641509433965
f1_score :0.5185185185185185
confusion_matrix = metrics.confusion_matrix(y_valid, y_valid_pred)
#confusion_matrix(y_valid, y_valid_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()
 
Set Constant Cutoffs
# Get the predicted probabilities for the positive class
Gius_GrideSearch_Model = Gius_Final_Model_L_R.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.4  # Adjust this threshold as needed

# Apply the new cutoff point
Final_Gius_Pred_cut = (Gius_GrideSearch_Model >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, Final_Gius_Pred_cut)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.67,
 'Precision': 0.67,
 'Recall': 0.64,
 'f1-score': 0.65,
 'Log-loss': 11.36,
 'AUC': 0.67}
pd.crosstab(y_valid, Final_Gius_Pred_cut)
col_0         0   1
Gius_Target        
0.0          79  34
1.0          38  68
Save Personality_Disq selected model - Constant Set Hyper Parameters
import joblib

# Assuming Model_Linear_Regression is already trained

# Save the model to a file
joblib.dump((Gius_Final_Model_L_R, cutoff_point), 'Gius_Final_Model_L_R.pkl')
['Gius_Final_Model_L_R.pkl']
Final Gibushon Dico Prediction Models
###Change target variable to categor type

Final_Gibushon_Dico_Gold_List ["Evaluation_Center_Dico_Target"] = Final_Gibushon_Dico_Gold_List ["Evaluation_Center_Dico_Target"].astype('category')
#Final_Gibushon_Dico_Gold_List .info()
re-orginize variables in final file
Final_Gibushon_Dico_Gold_List  = pd.DataFrame(Final_Gibushon_Dico_Gold_List )
temp_cols=Final_Gibushon_Dico_Gold_List .columns.tolist()
index=Final_Gibushon_Dico_Gold_List .columns.get_loc("Evaluation_Center_Dico_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
Final_Gibushon_Dico_Gold_List =Final_Gibushon_Dico_Gold_List [new_cols]
Final_Gibushon_Dico_Gold_List  = pd.DataFrame(Final_Gibushon_Dico_Gold_List )
temp_cols=Final_Gibushon_Dico_Gold_List .columns.tolist()
new_cols=temp_cols[1:] + temp_cols[0:1]
Final_Gibushon_Dico_Gold_List =Final_Gibushon_Dico_Gold_List [new_cols]
#Final_Gibushon_Dico_Gold_List .head()
id to first column
#df_final_train2 = pd.DataFrame(df_final_train1)
#temp_cols=df_final_train2.columns.tolist()
#index=df_final_train2.columns.get_loc("id")
#new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
#df_final_train2=df_final_train2[new_cols]
Random Reorder All Raws in Dataframe
import pandas as pd
import numpy as np

# Assuming Gibushon_Sofi_Gold_List is your DataFrame
# Set a fixed seed value for reproducibility
np.random.seed(42)

# Apply random reorder of all rows
Gibushon_Sofi_Gold_List = Gibushon_Sofi_Gold_List.sample(frac=1).reset_index(drop=True)
Splitting Data to Train  Validation  Test Sets
#data = pd.DataFrame(Final_Gibushon_Dico_Gold_List )
#data = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Evaluation_Center_Dico_Target"])
# Shuffle the values in the original DataFrame
#for column in data.columns:
#    data[column] = np.random.permutation(data[column].values)
import pickle
from sklearn.model_selection import train_test_split 
from scipy.stats import zscore

# Get the list of columns to compute z-scores for
columns_to_compute_zscore = [col for col in Final_Gibushon_Dico_Gold_List .columns if col != 'Evaluation_Center_Dico_Target']

# Compute z-scores for each column except "Evaluation_Center_Dico_Target"
#Final_Gibushon_Dico_Gold_List [columns_to_compute_zscore] = Final_Gibushon_Dico_Gold_List [columns_to_compute_zscore].apply(zscore)

# Now, z-scores are computed for all variables in the DataFrame except "Evaluation_Center_Dico_Target"
#Final_Gibushon_Dico_Gold_List 
data = pd.DataFrame(Final_Gibushon_Dico_Gold_List )
#data
#data = pd.DataFrame(Final_Gibushon_Dico_Gold_List , columns=["Job_Motivators_Index", "Misconduct_Index", "United_Employment_Problems" , "Interests_and_Activities_Index", "Age_Num", "Temp_Mean_Num", "Evaluation_Center_Dico_Target"])
X_data = data[data.columns[~data.columns.isin(['Evaluation_Center_Dico_Target'])]]
y_data = data['Evaluation_Center_Dico_Target']
60% of data observations - train-set | 20% of data observations - validation-set | 20% of data observations - test-set
# Split the data into train, validation and test sets

X_train, X_temp, y_train, y_temp = train_test_split(X_data, y_data, test_size=0.45, random_state=10)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.00001, random_state=5)
print("Train data shape:", X_train.shape)
print("Validation data shape:", X_valid.shape)
print("Test data shape:", X_test.shape)
Train data shape: (106, 11)
Validation data shape: (86, 11)
Test data shape: (1, 11)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", y_train)
#print("Real values in y_valid:", X_valid)
#print("Real values in y_valid:", y_valid)
#print("Real values in y_valid:", X_test)
#print("Real values in y_valid:", y_test)
###only if needed
#lab = preprocessing.LabelEncoder()
#y_train = lab.fit_transform(y_train)
#y_test = lab.fit_transform(y_test)
#y_valid = lab.fit_transform(y_valid)
Metrics Table
Gius_models_list_validation = pd.DataFrame()
#Rama_models_list_train = pd.DataFrame()
#Rama_models_list_test = pd.DataFrame()
Models Development
Cross Validation imports
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
Logistic Regression
Algorithm Setting
Model_Logistic_Regression = LogisticRegression(random_state=1)
parameters :
parameters : (penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)
Model Fit Base on Train Set Only :
Model_Logistic_Regression.fit(X_train, y_train)
LogisticRegression(random_state=1)
Model Evaluation on train envairment
# predictions --not-- based on cross validation test
y_pred_train_enva = Model_Logistic_Regression.predict(X_train)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_train_enva,y_train)
Metrics = classificationMetrics(y_train, y_pred_train_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.79,
 'Precision': 0.79,
 'Recall': 0.91,
 'f1-score': 0.85,
 'Log-loss': 7.17,
 'AUC': 0.75}
pd.crosstab(y_train, y_pred_train_enva)
col_0                          0.0  1.0
Evaluation_Center_Dico_Target          
0.0                             24   16
1.0                              6   60
Model Evaluation on valid envairment
y_pred_valid_enva = Model_Logistic_Regression.predict(X_valid)
###Model evaluation figrue - train enva 
#sns.scatterplot(y_pred_valid_enva,y_valid)
Metrics = classificationMetrics(y_valid, y_pred_valid_enva)

formatted_Metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Metrics.items()}
formatted_Metrics
{'Accuracy': 0.7,
 'Precision': 0.67,
 'Recall': 0.73,
 'f1-score': 0.7,
 'Log-loss': 10.44,
 'AUC': 0.7}
pd.crosstab(y_valid, y_pred_valid_enva)
col_0                          0.0  1.0
Evaluation_Center_Dico_Target          
0.0                             30   15
1.0                             11   30
Changing Cut-Off - Validation Set Scores
# Get the predicted probabilities for the positive class
y_pred_prob = Model_Logistic_Regression.predict_proba(X_valid)[:, 1]
# Define a new cutoff point (threshold)
cutoff_point = 0.45  # Adjust this threshold as needed

# Apply the new cutoff point
y_pred_valid_cutoff = (y_pred_prob >= cutoff_point).astype(int)
# Calculate classification metrics and confusion matrix with the new threshold
Cutoff_metrics = classificationMetrics(y_valid, y_pred_valid_cutoff)
Formatted_cutoff_metrics = {key: value.round(2) if isinstance(value, float) else value for key, value in Cutoff_metrics.items()}
Formatted_cutoff_metrics
{'Accuracy': 0.65,
 'Precision': 0.61,
 'Recall': 0.76,
 'f1-score': 0.67,
 'Log-loss': 12.05,
 'AUC': 0.66}
pd.crosstab(y_valid, y_pred_valid_cutoff)
col_0                           0   1
Evaluation_Center_Dico_Target        
0.0                            25  20
1.0                            10  31
model_dict = {'model': "Logistic_Regression"}
Gius_models_list_validation = Gius_models_list_validation.append({**model_dict, **classificationMetrics(y_valid, y_pred_valid_cutoff)}, ignore_index=True)
Gius_models_list_validation.round(2)
                 model  Accuracy  Precision  Recall  f1-score  Log-loss   AUC
0  Logistic_Regression      0.65       0.61    0.76      0.67     12.05  0.66
Cross Validation (Base on above cutoff)
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score
from sklearn.model_selection import cross_validate

# Assuming Model_Logistic_Regression is your logistic regression model
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1_score': 'f1',
    'log_loss': 'neg_log_loss',
    'roc_auc': 'roc_auc'
}

# Perform cross-validation with multiple scoring metrics
cv_scores = cross_validate(Model_Logistic_Regression, X_data, y_data, scoring=scoring, cv=3)

# Extract and print the results
print(f"Cross-validated Accuracy scores: {cv_scores['test_accuracy']}")
print(f"Mean Accuracy from cross-validation: {np.mean(cv_scores['test_accuracy'])}")

print(f"Cross-validated Precision scores: {cv_scores['test_precision']}")
print(f"Mean Precision from cross-validation: {np.mean(cv_scores['test_precision'])}")

print(f"Cross-validated Recall scores: {cv_scores['test_recall']}")
print(f"Mean Recall from cross-validation: {np.mean(cv_scores['test_recall'])}")

print(f"Cross-validated F1-score scores: {cv_scores['test_f1_score']}")
print(f"Mean F1-score from cross-validation: {np.mean(cv_scores['test_f1_score'])}")

print(f"Cross-validated Log-loss scores: {cv_scores['test_log_loss']}")
print(f"Mean Log-loss from cross-validation: {np.mean(cv_scores['test_log_loss']) * -1}")  # Log-loss is negated in cross-validation

print(f"Cross-validated AUC scores: {cv_scores['test_roc_auc']}")
print(f"Mean AUC from cross-validation: {np.mean(cv_scores['test_roc_auc'])}")

# Adjust predicted probabilities based on the cutoff point
y_pred_prob = Model_Logistic_Regression.predict_proba(X_data)[:, 1]
y_pred_adjusted = (y_pred_prob >= cutoff_point).astype(int)

# Calculate metrics using adjusted predictions
accuracy = accuracy_score(y_data, y_pred_adjusted)
precision = precision_score(y_data, y_pred_adjusted)
recall = recall_score(y_data, y_pred_adjusted)
f1 = f1_score(y_data, y_pred_adjusted)
logloss = log_loss(y_data, y_pred_adjusted)
auc = roc_auc_score(y_data, y_pred_adjusted)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Log-loss: {logloss}")
print(f"AUC: {auc}")
Cross-validated Accuracy scores: [0.55384615 0.734375   0.78125   ]
Mean Accuracy from cross-validation: 0.6898237179487179
Cross-validated Precision scores: [0.59459459 0.73684211 0.76190476]
Mean Precision from cross-validation: 0.6977804872541714
Cross-validated Recall scores: [0.61111111 0.8        0.88888889]
Mean Recall from cross-validation: 0.7666666666666666
Cross-validated F1-score scores: [0.60273973 0.76712329 0.82051282]
Mean F1-score from cross-validation: 0.7301252780704836
Cross-validated Log-loss scores: [-0.71891085 -0.47819112 -0.51610117]
Mean Log-loss from cross-validation: 0.5710677110221584
Cross-validated AUC scores: [0.64272031 0.86108374 0.82440476]
Mean AUC from cross-validation: 0.7760696040868454
Accuracy: 0.7357512953367875
Precision: 0.7121212121212122
Recall: 0.8785046728971962
F1 Score: 0.7866108786610878
Log-loss: 9.126984356698406
AUC: 0.7183221038904586
confusion_matrix = metrics.confusion_matrix(y_data, y_pred_adjusted)
#confusion_matrix(y_valid, y_valid_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()
 
Save Personality_Disq selected model - Constant Set Hyper Parameters
import joblib

# Assuming Model_Linear_Regression is already trained

# Save the model to a file
joblib.dump((Model_Logistic_Regression, cutoff_point), 'Model_Logistic_Regression.pkl')
['Model_Logistic_Regression.pkl']

