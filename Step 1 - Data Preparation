Data Prep
Schedule refresh for models
#pip install schedule
#import sys

#sys.path.append("/past/the/path/you/copied/here")
import time
import threading
import schedule
from IPython.display import display, Javascript
#def run_notebook_code():
#    ### Your notebook code to be executed every 24 hours
#    display(Javascript('IPython.notebook.execute_cell_range(IPython.notebook.get_selected_index()+1, IPython.notebook.ncells())'))

#def run_scheduler():
    ### Schedule the code to run every 24 hours
#    schedule.every(24).hours.do(run_notebook_code)

    ### Run the scheduler
#    while True:
#        schedule.run_pending()
#        time.sleep(1)

### Create a thread for the scheduler and start it
#scheduler_thread = threading.Thread(target=run_scheduler)
#scheduler_thread.daemon = True  # Daemonize the thread so it exits when the main thread exits
#scheduler_thread.start()
Import Packages
#pip install factor_analyzer
#pip install --upgrade numpy
#pip install --user numpy
#pip install statsmodels
import pandas as pd
import numpy as np

import scipy.stats as ss
import researchpy as rp
import scipy.stats as stats
import csv
import pandas as pd
import numpy as np

from scipy import stats
from scipy.stats import chisquare
from scipy.stats import chi2_contingency
from scipy.stats import f_oneway
from scipy.stats import kstest
from scipy.stats import ks_2samp
from scipy.stats import norm
from scipy.stats import iqr

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.datasets import load_boston
from sklearn import linear_model

import matplotlib.pyplot as plt
plt.style.use('classic')
import seaborn as sns
plt.style.use('seaborn')
get_ipython().run_line_magic('matplotlib', 'inline')

#from ydata_profiling import ProfileReport
import statsmodels.api as sm
import statsmodels.formula.api as smf

import chart_studio
import re
import cv2

import missingno as msno
import warnings
warnings.filterwarnings("ignore")

import cloudinary
import cloudinary.uploader
import cloudinary.api

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#port csv
#mport pyodbc
#.read_sql_query

import researchpy as rp
import scipy.stats as stats


import pandas as pd
import numpy as np
import scipy.stats as ss
from math import log
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
Breathing Models Development
Import and converting models Data to a flat file
from datetime import datetime
import os
import pandas as pd

# Path to the folder
folder_path = rfolder_path = r'C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/1Input Basic Files/1.1Project questionnaire'

# Get a list of all Excel files in the folder
excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]

# Extract dates from the file names and create a dictionary mapping files to dates
file_dates = {file: pd.to_datetime(file.split('_')[-1].split('.')[0], errors='coerce') for file in excel_files}

# Sort the files by date
sorted_files = sorted(file_dates.keys(), key=file_dates.get, reverse=True)

# Select the 350 most recent files
recent_files = sorted_files[:350]

# Initialize an empty DataFrame to store the loaded data
loaded_data = pd.DataFrame()

# Load data from the "Main" tab of the selected files
for file in recent_files:
    file_path = os.path.join(folder_path, file)
    df_main = pd.read_excel(file_path, sheet_name='Main')  # Load only from the "Main" tab
    loaded_data = pd.concat([loaded_data, df_main])

# Reset the index of the merged DataFrame
loaded_data.reset_index(drop=True, inplace=True)

# Save "Main" tab to dataframe
df_Main_tab = loaded_data.copy()

# Rename the first column to '#Parent'
df_Main_tab = df_Main_tab.rename(columns={df_Main_tab.columns[0]: '#Parent'})

#df_Main_tab.head()
import os
import pandas as pd

# Path to the folder
folder_path = rfolder_path = r'C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/1Input Basic Files/1.1Project questionnaire'

# Get a list of all Excel files in the folder
excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]

# Extract dates from the file names and create a dictionary mapping files to dates
file_dates = {file: pd.to_datetime(file.split('_')[-1].split('.')[0], errors='coerce') for file in excel_files}

# Sort the files by date
sorted_files = sorted(file_dates.keys(), key=file_dates.get, reverse=True)

# Select the 350 most recent files
recent_files = sorted_files[:350]

# Initialize an empty DataFrame to store the loaded data from the specified tab
loaded_data_gIYUS4 = pd.DataFrame()

# Load data from the "Data.root.pAGE3.gIYUS4" tab of the selected files
for file in recent_files:
    file_path = os.path.join(folder_path, file)
    df_gIYUS4 = pd.read_excel(file_path, sheet_name='Data.root.pAGE3.gIYUS4')  # Load only from the specified tab
    loaded_data_gIYUS4 = pd.concat([loaded_data_gIYUS4, df_gIYUS4])

# Reset the index of the merged DataFrame
loaded_data_gIYUS4.reset_index(drop=True, inplace=True)

# Select only the first 6 categories for each participant
df_gIYUS4_tab_first_6 = loaded_data_gIYUS4.groupby('#Parent').head(6).copy()

# Create a new column for counting occurrences within each group
df_gIYUS4_tab_first_6['count'] = df_gIYUS4_tab_first_6.groupby('#Parent').cumcount()

# Pivot the table to create columns for each category
pivoted_gIYUS4_tab = df_gIYUS4_tab_first_6.pivot_table(index='#Parent', columns='dataText', values='count', aggfunc='count', fill_value=0)

# Reset the index to make #Parent a regular column
pivoted_gIYUS4_tab.reset_index(inplace=True)

# Display the transformed DataFrame
#pivoted_gIYUS4_tab.head()
import os
import pandas as pd

# Path to the folder
folder_path = rfolder_path = r'C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/1Input Basic Files/1.1Project questionnaire'

# Get a list of all Excel files in the folder
excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]

# Extract dates from the file names and create a dictionary mapping files to dates
file_dates = {file: pd.to_datetime(file.split('_')[-1].split('.')[0], errors='coerce') for file in excel_files}

# Sort the files by date
sorted_files = sorted(file_dates.keys(), key=file_dates.get, reverse=True)

# Select the 350 most recent files
recent_files = sorted_files[:350]

# Initialize an empty DataFrame to store the loaded data from the specified tab
loaded_data_gIYUS3 = pd.DataFrame()

# Load data from the "Data.root.pAGE3.gIYUS3" tab of the selected files
for file in recent_files:
    file_path = os.path.join(folder_path, file)
    df_gIYUS3 = pd.read_excel(file_path, sheet_name='Data.root.pAGE3.gIYUS3')  # Load only from the specified tab
    loaded_data_gIYUS3 = pd.concat([loaded_data_gIYUS3, df_gIYUS3])

# Select only the first 6 categories for each participant
df_gIYUS3_tab_first_6 = loaded_data_gIYUS3.groupby('#Parent').head(6).copy()

# Create a new column for counting occurrences within each group
df_gIYUS3_tab_first_6['count'] = df_gIYUS3_tab_first_6.groupby('#Parent').cumcount()

# Pivot the table to create columns for each category
pivoted_gIYUS3_tab = df_gIYUS3_tab_first_6.pivot_table(index='#Parent', columns='dataText', values='count', aggfunc='count', fill_value=0)

# Reset the index to make #Parent a regular column
pivoted_gIYUS3_tab.reset_index(inplace=True)

# Display the transformed DataFrame
#pivoted_gIYUS3_tab.head()
# Assuming df_Main_tab, pivoted_gIYUS4_tab, and pivoted_gIYUS3_tab are your DataFrames
# If not, replace them with your actual DataFrame variables

# Merge df_Main_tab with pivoted_gIYUS4_tab based on #Parent
merged_df = pd.merge(df_Main_tab, pivoted_gIYUS4_tab, on='#Parent', how='left')

# Merge the result with pivoted_gIYUS3_tab based on #Parent
merged_df = pd.merge(merged_df, pivoted_gIYUS3_tab, on='#Parent', how='left')

#merged_df.head()
#merged_df.info()
# Select columns that do not end with 'dataText'
filtered_columns = [col for col in merged_df.columns if not col.endswith('dataText')]

# Create a new DataFrame with the selected columns
filtered_dataframe = merged_df[filtered_columns]
#merged_df.dtypes
# Assuming your DataFrame is named 'filtered_dataframe'
# Replace 'filtered_dataframe' with the actual name of your DataFrame

# List of columns to be dropped
columns_to_drop = [
    'ProcessInstance.BiztalkID',
    'ProcessInstance.FormID',
    'ProcessInstance.UserID',
    'ProcessInstance.PhaseID',
    'ProcessInstance.UpdateDate',
    'ProcessInstance.StageStatus',
    'ProcessInstance.FlowID',
    'Data.root.MetaData.referenceNumber',
    'Data.root.MetaData.sentDate',
    'Data.root.MetaData.processId',
    'Data.root.MetaData.language',
    'Data.root.pAGE3.gIYUS3',
    'Data.root.pAGE3.gIYUS4',
    'Data.root.Files',
    'Data.root.pAGE3.gIYUS10',
    'Data.root.pAGE3.gIYUS11',
    'Data.root.pAGE2.jOBB',
    'Data.root.pAGE2.aRMY4'
]

# Drop the specified columns
filtered_dataframe = filtered_dataframe.drop(columns=columns_to_drop, errors='ignore')
#filtered_dataframe.head(5)
Change columns names
# Assuming filtered_dataframe is your DataFrame

column_mapping = {
    "ProcessInstance.CreationDate": "Date",
    "#Parent": "Index",
    "Data.root.pAGE2.aGE": "Age_Num",
    "Data.root.pAGE2.gender.dataCode": "Gender_Dico",
    "Data.root.pAGE2.familystatus.dataCode": "Married_Dico",
    "Data.root.pAGE2.nATIO.dataCode": "Nation_Dico",
    "Data.root.pAGE2.mIUT1.dataCode": "Minority_Dico",
    "Data.root.pAGE2.mIUT.dataCode": "Minority_Type_Categor",
    "Data.root.pAGE2.eDU.dataCode": "Current_Education_Categor",
    "Data.root.pAGE2.eDU2.dataCode": "Graduation_Average_Num",
    "Data.root.pAGE2.eDU3.dataCode": "Math_Units_Num",
    "Data.root.pAGE2.eDU4.dataCode": "English_Units_Num",
    "Data.root.pAGE2.eDU5.dataCode": "Bachelors_Degree_Average_Num",
    "Data.root.pAGE2.eDU6.dataCode": "Masters_Degree_Average_Num",
    "Data.root.pAGE2.eDU7.dataCode": "Relevant_Education_Dico",
    "Data.root.pAGE2.eDU8.dataCode": "Do_Psyc_Test_Dico",
    "Data.root.pAGE2.eDU9.dataCode": "Psyc_Test_Grade_Num",
    "Data.root.pAGE2.eDU10.dataCode": "Achievements_Preception_Num",
    "Data.root.pAGE2.eDU11.dataCode": "Hebrew_1_Expressive_Num",
    "Data.root.pAGE2.eDU12.dataCode": "Hebrew_2_Vocabulary_Num",
    "Data.root.pAGE2.eDU13.dataCode": "Hebrew_3_Writing_Num",
    "Data.root.pAGE2.eDU14.dataCode": "Hebrew_4_Reading_Num",
    "Data.root.pAGE2.eDU15.dataCode": "Hebrew_5_Proofreading_Num",
    "Data.root.pAGE2.eDU16.dataCode": "Psych_Tests_Subjective_Num",
    "Data.root.pAGE2.eDU17.dataCode": "Temper_Control_1_Num",
    "Data.root.pAGE2.eDU18.dataCode": "Temper_Control_2_Num",
    "Data.root.pAGE2.eDU19.dataCode": "Temper_Control_3_Num",
    "Data.root.pAGE2.eDU20.dataCode": "Temper_Control_4_Num",
    "Data.root.pAGE2.eDU21.dataCode": "Temper_Control_5_Num",
    "Data.root.pAGE2.aRMY1.dataCode": "Service_Type_Categor",
    "Data.root.pAGE2.aRMY.dataCode": "Commander_Army_Dico",
    "Data.root.pAGE2.aRMY2.dataCode": "Kazin_Army_Dico",
    "Data.root.pAGE2.aRMY3.dataCode": "Special_Unit_Army_Dico",
    "Data.root.pAGE2.aRMY5.dataCode": "Role_Type_Army_Categor",
    "Data.root.pAGE2.aRMY6.dataCode": "Kaba_Grade_Army_Num",
    "Data.root.pAGE2.aRMY7.dataCode": "Dapar_Grade_Army_Num",
    "Data.root.pAGE2.aRMY20.dataCode": "Mechina_Before_Army_Dico",
    "Data.root.pAGE2.aRMY21.dataCode": "Year_of_Service_Army_Dico",
    "Data.root.pAGE2.aRMY8.dataCode": "Army_Disciplinary_Dico",
    "Data.root.pAGE2.aRMY9.dataCode": "Conviction_Army_Dico",
    "Data.root.pAGE2.aRMY10.dataCode": "Arrest_Prison_Army_Dico",
    "Data.root.pAGE2.aRMY11.dataCode": "Mental_Treatment_Army_Dico",
    "Data.root.pAGE2.aRMY12.dataCode": "Mental_Release_Army_Dico",
    "Data.root.pAGE2.jOB.dataCode": "Relevant_Job_Experience_Dico",
    "Data.root.pAGE2.jOB2.dataCode": "Previous_Gius_Attempts_Dico",
    "Data.root.pAGE2.jOB3.dataCode": "Number_of_Attempts_Num",
    "Data.root.pAGE2.jOB4.dataCode": "Miun_Stop_Reason_Categor",
    "Data.root.pAGE2.jOB5.dataCode": "Past_Permanent_Officer_Dico",
    "Data.root.pAGE2.jOB6.dataCode": "Saham_Officer_Past_Dico",
    "Data.root.pAGE2.jOB7.dataCode": "Past_Layoffs_Dico",
    "Data.root.pAGE2.jOB8.dataCode": "Another_Job_Nomination_Dico",
    "Data.root.pAGE2.jOB9.dataCode": "Max_Procedure_Duration_Num",
    "Data.root.pAGE2.tAS1.dataCode": "Previous_Arrest_Dico",
    "Data.root.pAGE2.tAS2.dataCode": "Criminal_Record_Dico",
    "Data.root.pAGE2.tAS3.dataCode": "Conviction_in_Court_Dico",
    "Data.root.pAGE2.tAS4.dataCode": "Light_Drugs_Dico",
    "Data.root.pAGE2.tAS5.dataCode": "Light_Drugs_Last_Use_Num",
    "Data.root.pAGE2.tAS6.dataCode": "Hard_Drugs_Dico",
    "Data.root.pAGE2.tAS7.dataCode": "Hard_Drugs_Last_Use_Num",
    "Data.root.pAGE2.tAS8.dataCode": "Drinking_Alcohol_Frequ_Num",
    "Data.root.pAGE2.tAS9.dataCode": "Gambling_Frequ_Num",
    "Data.root.pAGE2.tAS10.dataCode": "Unemployment_Dico",
    "Data.root.pAGE2.tAS11.dataCode": "Financial_Difficulties_Dico",
    "Data.root.pAGE2.tAS12.dataCode": "Pshitat_Regel_Dico",
    "Data.root.pAGE2.tAS13.dataCode": "Debts_Dico",
    "Data.root.pAGE2.bRI.dataCode": "Physical_Fitness_Frequ_Num",
    "Data.root.pAGE2.bRI2": "Height_Num",
    "Data.root.pAGE2.bRI3": "Weight_Num",
    "Data.root.pAGE2.bRI4.dataCode": "Cigarettes_Dico",
    "Data.root.pAGE2.bRI5.dataCode": "Cigarettes_Num",
    "Data.root.pAGE2.bRI6.dataCode": "Chronic_Disease_Dico",
    "Data.root.pAGE2.bRI7.dataCode": "Physical_Limitations_Dico",
    "Data.root.pAGE2.bRI8.dataCode": "Mental_Difficulties_Dico",
    "Data.root.pAGE2.bRI9.dataCode": "Psychiatric_Drugs_Dico",
    "Data.root.pAGE3.gIYUS1.dataCode": "Volunteering_Dico",
    "Data.root.pAGE3.gIYUS12.dataCode": "Volunteering_Scope_Num",
    "Data.root.pAGE3.gIYUS.dataCode": "Service_Period_Commitment_Dico",
    "Data.root.pAGE3.gIYUS2.dataCode": "Work_Perceived_Maching_Num",
    "Data.root.pAGE3.gIYUS5.dataCode": "Previous_Job_Salary_Num",
    "Data.root.pAGE3.gIYUS6.dataCode": "Salary_Expectations_Num",
    "ProcessInstance.ReferenceNumber": "Index_Real",
    "Data.root.pAGE2.mAIL": "Mail",
    "Data.root.pAGE2.iD": "ID_Num",
    "Data.root.pAGE2.lN": "First_Name",
    "Data.root.pAGE2.fN": "Last_Name",
    "ספורט":"Sport",
    "בריאות":"Healt",
    "בישול":"Cooking",
    "פעילות התנדבותית":"Volunteering",
    "פסיכולוגיה":"Psychology",
    "ספרות":"Literature",
    "פוליטיקה":"Politics",
    "קולנוע":"Theater",
    "ניהול אורח חיים בריא":"Managing_Healthy_Lifestyle",
    "גלישה באינטרנט":"Web_Surfing",
    "הימורים":"Gambling",
    "טבע וטיולים":"Nature_and_Trips",
    "טכנולוגיה ומחשבים":"Technology_and_Computers",
    "כלכלה":"Economy",
    "מדע":"Science",
    "מפגשים עם חברים":"Meeting_With_Friends",
    "בילויים במסיבות / ברים":"Enjoy_Parties_Bars",
    "אקטואליה":"Actuavlia",
    "אומנות":"Art",
    "כלי נשק":"Weapons",
    "פעילות מיסטית":"Mistic_Activity",
    "מוזיקה":"Music",
    "ספורט אתגרי":"Extreme_Sports",
    "אפשרות לנהל ולפקד":"Manage_and_Command",
    "האתגר והעניין בעבודה":"Challenging_Interesting_Work",
    "תנאים סוציאליים ורווחה":"Social_Benefits",
    "רצון להעניק סיוע לאזרחים":"Assistance_to_Citizens",
    "אפשרות להתפתחות מקצועית ואישית":"Personal_Development",
    "תרומת עבודת השוטר לחברה":"Contribution_to_Society",
    "המלצה של המשפחה/חברים":"Recommendation",
    "יציבות תעסוקתית":"Employment_Stability",
    "השכר":"Good_Salary",
    "אפשרות להילחם בפשיעה":"Crime_Fighting",
    "אינטראקציה עם אנשים":"Interaction_With_People",
    "הגיוון בעבודה":"Diversity_at_Work",
    "מיקום נוח של יחידת השירות":"Convenient_Work_Location",
    "ברירת מחדל":"Default_Employment",
    "אפשרות ללבוש מדים":"Wearing_Uniform",
    "האפשרות להפעיל כוח במסגרת התפקיד":"Use_Force",
    "היכולת להפעיל סמכות":"Exercise_Authority",
    "התדמית המכובדת של העבודה":"Respectable_Job",
    "שעות עבודה נוחות":"Convenient_Working_Hours",
    "תפקיד המאפשר עצמאות ואוטונומיה":"Independence_and_Autonomy",
    
}

filtered_dataframe.rename(columns=column_mapping, inplace=True)
# Get the current column names
column_names = filtered_dataframe.columns.tolist()

# Update the name of the 6th column from the right
column_names[-6] = "The_Action_In_Work"

# Assign the updated column names back to the DataFrame
filtered_dataframe.columns = column_names
Recode variables categories
# Assuming filtered_dataframe is your DataFrame

# Replace values in the "Married_Dico" column
replacement_dict = {"widower": "0", "Single": "0", "divorcee": "0", "Married": "1"}
filtered_dataframe["Married_Dico"].replace(replacement_dict, inplace=True)
import pandas as pd

# Assuming your DataFrame is named filtered_dataframe
# Identify columns ending with "Dico" and replace 2 with 0
dico_columns = [col for col in filtered_dataframe.columns if col.endswith("Dico")]

# Replace 2 with 0 in identified columns
filtered_dataframe[dico_columns] = filtered_dataframe[dico_columns].replace(2, 0)
import pandas as pd

# Assuming your DataFrame is named filtered_dataframe
# Check for columns with only values 1 or 2
selected_columns = filtered_dataframe.columns[(filtered_dataframe.eq(1) | filtered_dataframe.eq(2)).all()]

# Print the names of columns meeting the condition
if not selected_columns.empty:
    print("Columns with only values 1 or 2:", selected_columns.tolist())
else:
    print("No columns with only values 1 or 2 found.")
No columns with only values 1 or 2 found.
import pandas as pd

# Assuming your DataFrame is named filtered_dataframe
# Check for columns with words starting with a lowercase letter
selected_columns = filtered_dataframe.columns[filtered_dataframe.columns.str.contains(r'\b[a-z]', regex=True)]

# Print the names of columns meeting the condition
if not selected_columns.empty:
    print("Columns with words starting with a lowercase letter:", selected_columns.tolist())
else:
    print("No columns with words starting with a lowercase letter found.")
No columns with words starting with a lowercase letter found.
#filtered_dataframe.head()
Recode variables categories for handling missing values
df = pd.DataFrame(filtered_dataframe)
# Assuming df_train is your data frame
df = df.drop_duplicates(subset='ID_Num')
df["Number_of_Attempts_Num"].fillna(0, inplace=True)
import pandas as pd

# Create a new variable "Masters_Degree_Average_up_84" based on conditions

df["Masters_Degree_Average_up_84_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Masters_Degree_Average_Num"].between(9, 12, inclusive=True), "Masters_Degree_Average_up_84_Dico"] = 1
import pandas as pd

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Psyc_Test_600_Up_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Psyc_Test_Grade_Num"].between(9, 12, inclusive=True), "Psyc_Test_600_Up_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Psyc_Test_450_600_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Psyc_Test_Grade_Num"].between(6, 8, inclusive=True), "Psyc_Test_450_600_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Psyc_Test_450_Less_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Psyc_Test_Grade_Num"].between(1, 5, inclusive=True), "Psyc_Test_450_Less_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Kaba_Grade_55_UP_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Kaba_Grade_Army_Num"].between(15, 17, inclusive=True), "Kaba_Grade_55_UP_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Kaba_Grade_51_54_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Kaba_Grade_Army_Num"].between(12, 14, inclusive=True), "Kaba_Grade_51_54_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Kaba_Grade_46_Less_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Kaba_Grade_Army_Num"].between(1, 6, inclusive=True), "Kaba_Grade_46_Less_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Kaba_Grade_51_Up_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Kaba_Grade_Army_Num"].between(12, 17, inclusive=True), "Kaba_Grade_51_Up_Dico"] = 1

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Dapar_Grade_7_Up_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Dapar_Grade_Army_Num"].between(8, 10, inclusive=True), "Dapar_Grade_7_Up_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Dapar_Grade_3_Less_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Dapar_Grade_Army_Num"].between(2, 4, inclusive=True), "Dapar_Grade_3_Less_Dico"] = 1

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Light_Drugs_Last_Year_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Light_Drugs_Last_Use_Num"].between(1, 1, inclusive=True), "Light_Drugs_Last_Year_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Graduation_Average_60_Less_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Graduation_Average_Num"].between(2, 5, inclusive=True), "Graduation_Average_60_Less_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Graduation_Average_85_Up_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Graduation_Average_Num"].between(9, 11, inclusive=True), "Graduation_Average_85_Up_Dico"] = 1

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["English_4_5_Units_Num_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["English_Units_Num"].between(3, 4, inclusive=True), "English_4_5_Units_Num_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["English_3_Units_Num_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["English_Units_Num"].between(2, 2, inclusive=True), "English_3_Units_Num_Dico"] = 1

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Math_4_5_Units_Num_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Math_Units_Num"].between(3, 4, inclusive=True), "Math_4_5_Units_Num_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Math_3_Units_Num_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Math_Units_Num"].between(2, 2, inclusive=True), "Math_3_Units_Num_Dico"] = 1

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Volunteering_3_Up_Week_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Volunteering_Scope_Num"].between(4, 6, inclusive=True), "Volunteering_3_Up_Week_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Cigarettes_11_Up_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Cigarettes_Num"].between(3, 4, inclusive=True), "Cigarettes_11_Up_Dico"] = 1

# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Academic_Education_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Current_Education_Categor"].between(3, 6, inclusive=True), "Academic_Education_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["No_Bagrut_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Current_Education_Categor"].between(1, 1, inclusive=True), "No_Bagrut_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Arab_Muslim_Cristian"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Minority_Type_Categor"].between(4, 5, inclusive=True), "Arab_Muslim_Cristian"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Prisha_Yezoma_on_Last_Attempt_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Miun_Stop_Reason_Categor"].between(7, 7, inclusive=True), "Prisha_Yezoma_on_Last_Attempt_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Evaluation_Center_Filed_Last_Attempt_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Miun_Stop_Reason_Categor"].between(2, 2, inclusive=True), "Evaluation_Center_Filed_Last_Attempt_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Dapar_Hebrew_Failurer_Last_Attempt_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Miun_Stop_Reason_Categor"].between(1, 1, inclusive=True), "Dapar_Hebrew_Failurer_Last_Attempt_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Sacham_officer_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Service_Type_Categor"].between(2, 2, inclusive=True), "Sacham_officer_Dico"] = 1
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Combat_Service_Army_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Role_Type_Army_Categor"].between(1, 1, inclusive=True), "Combat_Service_Army_Dico"] = 1
# List of variables
variable_list = {"Date", "Index", "ID_Num", "First_Name", "Last_Name", "Gender_Dico", "Married_Dico", "Nation_Dico", "Minority_Dico", "Relevant_Job_Experience_Dico", "Previous_Gius_Attempts_Dico", "Past_Permanent_Officer_Dico", "Saham_Officer_Past_Dico", "Past_Layoffs_Dico", "Another_Job_Nomination_Dico", "Previous_Arrest_Dico", "Criminal_Record_Dico", "Conviction_in_Court_Dico", "Light_Drugs_Dico", "Hard_Drugs_Dico", "Unemployment_Dico", "Financial_Difficulties_Dico", "Pshitat_Regel_Dico", "Debts_Dico", "Cigarettes_Dico", "Chronic_Disease_Dico", "Physical_Limitations_Dico", "Mental_Difficulties_Dico", "Psychiatric_Drugs_Dico", "Volunteering_Dico", "Service_Period_Commitment_Dico", "Do_Psyc_Test_Dico", "Relevant_Education_Dico", "Commander_Army_Dico", "Kazin_Army_Dico", "Special_Unit_Army_Dico", "Mechina_Before_Army_Dico", "Year_of_Service_Army_Dico", "Army_Disciplinary_Dico", "Conviction_Army_Dico", "Arrest_Prison_Army_Dico", "Mental_Treatment_Army_Dico", "Mental_Release_Army_Dico", "Age_Num", "Achievements_Preception_Num", "Hebrew_1_Expressive_Num", "Hebrew_2_Vocabulary_Num", "Hebrew_3_Writing_Num", "Hebrew_4_Reading_Num", "Hebrew_5_Proofreading_Num", "Psych_Tests_Subjective_Num", "Temper_Control_1_Num", "Temper_Control_2_Num", "Temper_Control_3_Num", "Temper_Control_4_Num", "Temper_Control_5_Num", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Gambling_Frequ_Num", "Physical_Fitness_Frequ_Num", "Height_Num", "Weight_Num", "Work_Perceived_Maching_Num", "Previous_Job_Salary_Num", "Salary_Expectations_Num", "Number_of_Attempts_Num", "Masters_Degree_Average_up_84_Dico", "Psyc_Test_600_Up_Dico", "Psyc_Test_450_600_Dico", "Psyc_Test_450_Less_Dico", "Kaba_Grade_55_UP_Dico", "Kaba_Grade_51_54_Dico", "Kaba_Grade_46_Less_Dico", "Dapar_Grade_7_Up_Dico", "Dapar_Grade_3_Less_Dico", "Light_Drugs_Last_Year_Dico", "Graduation_Average_60_Less_Dico", "Graduation_Average_85_Up_Dico", "English_4_5_Units_Num_Dico", "English_3_Units_Num_Dico", "Math_4_5_Units_Num_Dico", "Math_3_Units_Num_Dico", "Volunteering_3_Up_Week_Dico", "Cigarettes_11_Up_Dico", "Academic_Education_Dico", "No_Bagrut_Dico", "Arab_Muslim_Cristian", "Sacham_officer_Dico", "Combat_Service_Army_Dico", "Prisha_Yezoma_on_Last_Attempt_Dico", "Evaluation_Center_Filed_Last_Attempt_Dico", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Role_Type_Army_Categor", "Service_Type_Categor", "Miun_Stop_Reason_Categor", "Minority_Type_Categor", "Current_Education_Categor", "Graduation_Average_Num", "Math_Units_Num", "English_Units_Num", "Bachelors_Degree_Average_Num", "Volunteering_Scope_Num", "Cigarettes_Num", "Hard_Drugs_Last_Use_Num", "Light_Drugs_Last_Use_Num", "Dapar_Grade_Army_Num", "Kaba_Grade_Army_Num", "Psyc_Test_Grade_Num", "Masters_Degree_Average_Num", "Mail"}

# Check variables that do not exist in df
missing_variables = variable_list - set(df.columns)
print("Variables that do not exist in df:", missing_variables)

# Check variables in df that do not appear in the list
extra_variables = set(df.columns) - variable_list
###print("Variables in df that do not appear in the list:", extra_variables)
Variables that do not exist in df: set()
# List of variables in the desired order
desired_order = ["Date", "Index", "Index_Real", "ID_Num", "First_Name", "Last_Name", "Mail", "Gender_Dico", "Married_Dico", "Nation_Dico", "Minority_Dico", "Relevant_Job_Experience_Dico", "Previous_Gius_Attempts_Dico", "Past_Permanent_Officer_Dico", "Saham_Officer_Past_Dico", "Past_Layoffs_Dico", "Another_Job_Nomination_Dico", "Previous_Arrest_Dico", "Criminal_Record_Dico", "Conviction_in_Court_Dico", "Light_Drugs_Dico", "Hard_Drugs_Dico", "Unemployment_Dico", "Financial_Difficulties_Dico", "Pshitat_Regel_Dico", "Debts_Dico", "Cigarettes_Dico", "Chronic_Disease_Dico", "Physical_Limitations_Dico", "Mental_Difficulties_Dico", "Psychiatric_Drugs_Dico", "Volunteering_Dico", "Service_Period_Commitment_Dico", "Do_Psyc_Test_Dico", "Relevant_Education_Dico", "Commander_Army_Dico", "Kazin_Army_Dico", "Special_Unit_Army_Dico", "Mechina_Before_Army_Dico", "Year_of_Service_Army_Dico", "Army_Disciplinary_Dico", "Conviction_Army_Dico", "Arrest_Prison_Army_Dico", "Mental_Treatment_Army_Dico", "Mental_Release_Army_Dico", "Age_Num", "Achievements_Preception_Num", "Hebrew_1_Expressive_Num", "Hebrew_2_Vocabulary_Num", "Hebrew_3_Writing_Num", "Hebrew_4_Reading_Num", "Hebrew_5_Proofreading_Num", "Psych_Tests_Subjective_Num", "Temper_Control_1_Num", "Temper_Control_2_Num", "Temper_Control_3_Num", "Temper_Control_4_Num", "Temper_Control_5_Num", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Gambling_Frequ_Num", "Physical_Fitness_Frequ_Num", "Height_Num", "Weight_Num", "Work_Perceived_Maching_Num", "Previous_Job_Salary_Num", "Salary_Expectations_Num", "Number_of_Attempts_Num", "Masters_Degree_Average_up_84_Dico", "Psyc_Test_600_Up_Dico", "Psyc_Test_450_600_Dico", "Psyc_Test_450_Less_Dico", "Kaba_Grade_55_UP_Dico", "Kaba_Grade_51_54_Dico", "Kaba_Grade_46_Less_Dico", "Dapar_Grade_7_Up_Dico", "Dapar_Grade_3_Less_Dico", "Light_Drugs_Last_Year_Dico", "Graduation_Average_60_Less_Dico", "Graduation_Average_85_Up_Dico", "English_4_5_Units_Num_Dico", "English_3_Units_Num_Dico", "Math_4_5_Units_Num_Dico", "Math_3_Units_Num_Dico", "Volunteering_3_Up_Week_Dico", "Cigarettes_11_Up_Dico", "Academic_Education_Dico", "No_Bagrut_Dico", "Arab_Muslim_Cristian", "Sacham_officer_Dico", "Combat_Service_Army_Dico", "Prisha_Yezoma_on_Last_Attempt_Dico", "Evaluation_Center_Filed_Last_Attempt_Dico", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", 'Healt', 'Crime_Fighting', 'Convenient_Working_Hours', 'The_Action_In_Work', 'Music', 'Use_Force', 'Default_Employment', 'Psychology', 'Diversity_at_Work', 'Manage_and_Command', 'Sport', 'Art', 'Extreme_Sports', 'Meeting_With_Friends', 'Contribution_to_Society', 'Web_Surfing', 'Economy', 'Assistance_to_Citizens', 'Independence_and_Autonomy', 'Recommendation', 'Wearing_Uniform', 'Challenging_Interesting_Work', 'Volunteering', 'Actuavlia', 'Cooking', 'Enjoy_Parties_Bars', 'Personal_Development', 'Employment_Stability', 'Science', 'Nature_and_Trips', 'Good_Salary', 'Politics', 'Managing_Healthy_Lifestyle', 'Convenient_Work_Location', 'Gambling', 'Literature', 'Respectable_Job', 'Exercise_Authority', 'Technology_and_Computers', 'Interaction_With_People', 'Theater', 'Social_Benefits', "Role_Type_Army_Categor", "Service_Type_Categor", "Miun_Stop_Reason_Categor", "Minority_Type_Categor", "Current_Education_Categor", "Graduation_Average_Num", "Math_Units_Num", "English_Units_Num", "Bachelors_Degree_Average_Num", "Volunteering_Scope_Num", "Cigarettes_Num", "Hard_Drugs_Last_Use_Num", "Light_Drugs_Last_Use_Num", "Dapar_Grade_Army_Num", "Kaba_Grade_Army_Num", "Psyc_Test_Grade_Num", "Masters_Degree_Average_Num"]

# Reorder columns in df
df = df[desired_order]
# Identify columns ending with '_Dico'
dico_columns = [col for col in df.columns if col.endswith('_Dico')]

# Replace missing values with 0 in _Dico columns
df[dico_columns] = df[dico_columns].fillna(0)
# Identify columns ending with '_Num'
num_columns = [col for col in df.columns if col.endswith('_Num')]

# Replace missing values with the mean in _Num columns
df[num_columns] = df[num_columns].fillna(df[num_columns].mean())

# Display the modified data frame
#print(df)
# Identify columns ending with '_Categor'
Categor_columns = [col for col in df.columns if col.endswith('_Categor')]

# Replace missing values with 999 in Categor columns
df[Categor_columns] = df[Categor_columns].fillna(999)
# Check for missing values in each column of df
missing_values = df.isnull().sum()

# Extract variables with missing values
variables_with_missing_values = missing_values[missing_values > 0]

# Print the variables with missing values and their respective counts
for variable, count in variables_with_missing_values.items():
    print(f"Variable: {variable}, Missing Values: {count}")
# List of variables for which to calculate the average
expressive_variables = [
    'Hebrew_1_Expressive_Num',
    'Hebrew_2_Vocabulary_Num',
    'Hebrew_3_Writing_Num',
    'Hebrew_4_Reading_Num',
    'Hebrew_5_Proofreading_Num'
]

# Create a new column 'Hebrew_Meam_Num' containing the average of the specified variables
df['Hebrew_Meam_Num'] = df[expressive_variables].mean(axis=1)

# Display the modified data frame
#print(df)
# List of variables for which to calculate the average
Temp_variables = [
    'Temper_Control_1_Num',
    'Temper_Control_2_Num',
    'Temper_Control_3_Num',
    'Temper_Control_4_Num',
    'Temper_Control_5_Num'
]

# Create a new column 'Temp_Mean_Num' containing the average of the specified variables
df['Temp_Mean_Num'] = df[Temp_variables].mean(axis=1)

# Display the modified data frame
#print(df)
# Set the minimum and maximum limits
min_limit = 100
max_limit = 220

# Replace values outside the specified range with 173
df['Height_Num'] = df['Height_Num'].apply(lambda x: 173 if x < min_limit or x > max_limit else x)

# Display the modified data frame
#print(df)
# Set the minimum and maximum limits
min_limit = 30
max_limit = 150

# Replace values outside the specified range with 73
df['Weight_Num'] = df['Weight_Num'].apply(lambda x: 73 if x < min_limit or x > max_limit else x)

# Display the modified data frame
#print(df)
# Set the minimum and maximum limits
min_limit = 18
max_limit = 65

# Replace values outside the specified range with 27
df['Age_Num'] = df['Age_Num'].apply(lambda x: 27 if x < min_limit or x > max_limit else x)

# Display the modified data frame
#print(df)
df['BMI_Num'] = df.Weight_Num / (df.Height_Num * df.Height_Num / 10000)
# Set the minimum and maximum limits
min_limit = 15
max_limit = 50

# Replace values outside the specified range with 50
df['BMI_Num'] = df['BMI_Num'].apply(lambda x: 50 if x > max_limit else max(min(x, max_limit), min_limit))
# Set the thresholds for different BMI categories
underweight_threshold = 18.5
normal_threshold = 24.9999999
overweight_threshold = 29.9999999

# Create new dichotomous variables
df['Underweight_BMI_Dumi'] = (df['BMI_Num'] <= underweight_threshold).astype(int)
df['Normal_BMI_Dumi'] = ((df['BMI_Num'] >= underweight_threshold) & (df['BMI_Num'] <= normal_threshold)).astype(int)
df['Overweight_BMI_Dumi'] = ((df['BMI_Num'] >= normal_threshold) & (df['BMI_Num'] <= overweight_threshold)).astype(int)
df['Obesity_BMI_Dumi'] = (df['BMI_Num'] >= overweight_threshold).astype(int)

# Display the modified data frame
#print(df)
Target Variables Merge
df_Targets = pd.read_excel("C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/3Target Variables/Targets Variables - All.XLSX")
df_Targets.head()
      ID_Num  RAMA_Target  Evaluation_Center_Target  Hebrew_Target  \
0   37107588          NaN                       NaN            NaN   
1  211968169          5.0                       NaN            NaN   
2  340866375          3.5                       2.5            NaN   
3  206253106          3.0                       4.5            NaN   
4  310274717          3.0                       2.5            NaN   

   Dapar_Target  Normot_Target  Personality_Dis_Target  \
0           NaN            NaN                     NaN   
1           NaN            1.0                     0.0   
2           NaN            2.0                     0.0   
3           NaN            6.0                     1.0   
4           NaN            1.0                     0.0   

   Personality_Dis_Safek_Target  Gibushon_Target  Gius_Target  \
0                           NaN              NaN          NaN   
1                           0.0              NaN          1.0   
2                           0.0              2.5          0.0   
3                           1.0              NaN          0.0   
4                           0.0              2.5          0.0   

   Evaluation_Center_Dico_Target  Yeodi_Liba  
0                            NaN           0  
1                            NaN           1  
2                            0.0           1  
3                            1.0           1  
4                            0.0           1  
# Assuming df and df_Targets are your data frames
merged_df = pd.merge(df, df_Targets, on='ID_Num', how='left')

# Display the merged data frame
#print(merged_df)
df=pd.DataFrame(merged_df)
Normot Variable Minus 1 (Return to the original scale of the variable)
df['Normot_Target'] = df.Normot_Target - 1
Step 1 - General Feature Engineering & Enrichment
Variables Measures Scales
###checking datatype before conversions
###df.dtypes
### change variables decimal to only one after dot 
df.round(1)
###df.head()
                  Date  Index  Index_Real     ID_Num First_Name Last_Name  \
0     21/08/2023 14:44      1        5439   37107588          *         *   
1     17/08/2023 09:37      2        5403  211968169          *         *   
2     17/08/2023 07:43      3        5395  340866375          *         *   
3     17/08/2023 07:34      4        5391  206253106          *         *   
4     17/08/2023 07:45      5        5398  310274717          *         *   
...                ...    ...         ...        ...        ...       ...   
2244  16/03/2023 17:39   2487      121740  318256278    אלגלאוי      מחמד   
2245  21/02/2023 22:00   2489      121200   21939632       בלקן      אריה   
2246  21/02/2023 17:58   2493      121020   39169396        משה     שירלי   
2247  21/02/2023 14:38   2494      120900   57501777         אב        גד   
2248  21/02/2023 14:06   2495      120810   32794695        און      תומר   

                           Mail  Gender_Dico Married_Dico  Nation_Dico  ...  \
0     Roei.zami@mail.huji.ac.il          1.0            1            1  ...   
1        Adimarko6309@gmail.com          1.0            0            1  ...   
2          ntordjman3@gmail.com          1.0            1            1  ...   
3          Yardenza26@gmail.com          1.0            0            1  ...   
4         basalov1710@yahoo.com          1.0            1            1  ...   
...                         ...          ...          ...          ...  ...   
2244   mohamd.ggg1997@gmail.com          1.0            0            0  ...   
2245              efg@walla.com          1.0            1            0  ...   
2246         mshahar4@gmail.com          0.0            1            1  ...   
2247          aaa@bbb.gmail.com          1.0            0            1  ...   
2248         Tomerohn@gmail.com          1.0            1            1  ...   

      Evaluation_Center_Target  Hebrew_Target  Dapar_Target  Normot_Target  \
0                          NaN            NaN           NaN            NaN   
1                          NaN            NaN           NaN            0.0   
2                          2.5            NaN           NaN            1.0   
3                          4.5            NaN           NaN            5.0   
4                          2.5            NaN           NaN            0.0   
...                        ...            ...           ...            ...   
2244                       NaN            NaN           NaN            NaN   
2245                       NaN            NaN           NaN            NaN   
2246                       NaN            NaN           NaN            NaN   
2247                       NaN            NaN           NaN            NaN   
2248                       NaN            NaN           NaN            NaN   

      Personality_Dis_Target  Personality_Dis_Safek_Target  Gibushon_Target  \
0                        NaN                           NaN              NaN   
1                        0.0                           0.0              NaN   
2                        0.0                           0.0              2.5   
3                        1.0                           1.0              NaN   
4                        0.0                           0.0              2.5   
...                      ...                           ...              ...   
2244                     NaN                           NaN              NaN   
2245                     NaN                           NaN              NaN   
2246                     NaN                           NaN              NaN   
2247                     NaN                           NaN              NaN   
2248                     NaN                           NaN              NaN   

      Gius_Target  Evaluation_Center_Dico_Target  Yeodi_Liba  
0             NaN                            NaN           0  
1             1.0                            NaN           1  
2             0.0                            0.0           1  
3             0.0                            1.0           1  
4             0.0                            0.0           1  
...           ...                            ...         ...  
2244          NaN                            NaN           0  
2245          NaN                            NaN           0  
2246          NaN                            NaN           0  
2247          NaN                            NaN           0  
2248          NaN                            NaN           0  

[2249 rows x 171 columns]
Coding variables type as category
#df = df.astype({"Designated_Role":'category', "Education_Level":'category', "Educational_Institution":'category', "Rovaee_Training_Type":'category',"Assessment_Center_Grade":'category', "Filling_Instruction_Test":'category', "Emotional_Inteligence_Test":'category', "Age":'category', "Commander_in_Army_Dico":'category', "Honesty_Test":'category', "Population_Type_Dico":'category', "Heberew_Test_Grade":'category'})
Coding variables type as object
#df=df.astype({"Population_Type_Dico":'object',"Gender_Dico":'object',"Commander_in_Army_Dico":'object', "M2a_dico":'object',"ASP_dico":'object',"R_dico":'object',"CYN_dico":'object',"BIM_dico":'object',"PA_dico":'object',"AAS_dico":'object',"tcent_dico":'object',"OBS_dico":'object',"MDS_dico":'object',"TPA_dico":'object',"BIZ_dico":'object',"ANG_dico":'object',"DEP_dico":'object',"K_dico":'object',"SOD_dico":'object',"D_dico":'object',"D1_dico":'object',"D2_dico":'object',"D3_dico":'object',"D4_dico":'object',"D5_dico":'object',"MF_dico":'object',"DOE_dico":'object',"PT_dico":'object',"ANX_dico":'object',"F_dico":'object',"SC_dico":'object',"HEA_dico":'object',"FP_dico":'object',"HS_dico":'object',"FAM_dico":'object',"PD_dico":'object',"ES_dico":'object',"FCENT_dico":'object',"WRK_dico":'object',"HY_dico":'object',"L_dico":'object',"MA_dico":'object',"SE_dico":'object',"RT_dico":'object',"TRIN_dico":'object',"VRIN_dico":'object',"CR_dico":'object',"RS_dico":'object',"Honesty_Test":'object'})
Step 2 - Specific Feature Engineering & Feature Selection
---- Gibushon Final Grade ----
------------Feature Engineering-------------
Education_and_Inteligence_Index - Combine Variable
df['Dapar_Grade_3_Less_Dico_Rotated'] = df['Dapar_Grade_3_Less_Dico'] ^ 1
class_counts_D_G_3_L_D = df['Dapar_Grade_3_Less_Dico_Rotated'].value_counts()
###print(class_counts_D_G_3_L_D)
df['Education_and_Inteligence_Index'] = df.Dapar_Grade_3_Less_Dico_Rotated + df.Academic_Education_Dico + df.Graduation_Average_85_Up_Dico + df.Graduation_Average_60_Less_Dico
class_counts_E_I_I = df['Education_and_Inteligence_Index'].value_counts()
###print(class_counts_E_I_I)
Volunteering_Dico_Index - Combine Variable
df['Volunteering_Dico_Index'] = df.Volunteering_Dico + (df.Volunteering_3_Up_Week_Dico * 2)
class_counts_V_D_I = df['Volunteering_Dico_Index'].value_counts()
###print(class_counts_V_D_I)
Psyc_Test_Index - Combine Variable
df['All_One'] = 1
class_counts_All_One = df['All_One'].value_counts()
###print(class_counts_All_One)
df['Psyc_Test_Index'] = df.All_One - df.Psyc_Test_450_Less_Dico + (df.Psyc_Test_450_600_Dico * 1) + (df.Psyc_Test_600_Up_Dico * 2)
class_counts_P_T_I = df['Psyc_Test_Index'].value_counts()
###print(class_counts_P_T_I)
Job_Motivators_Index - Combine Variable
Job_Motivators_Corr = pd.DataFrame(df, columns=["Use_Force", "Independence_and_Autonomy", "Wearing_Uniform", "Social_Benefits","Exercise_Authority", "Default_Employment", "Respectable_Job", "Good_Salary", "Convenient_Working_Hours", "Personal_Development", "Contribution_to_Society", "Challenging_Interesting_Work", "Crime_Fighting", "Evaluation_Center_Target"])
###Job_Motivators_Corr[Job_Motivators_Corr.columns[0:]].corr(method='spearman')['Evaluation_Center_Target']
df['Job_Motivators_Index'] = df.Use_Force *-0.0486 + df.Independence_and_Autonomy * -0.160562 + df.Wearing_Uniform *-0.174265 + df.Social_Benefits * -0.089402 + df.Exercise_Authority * -0.109815 + df.Default_Employment * -0.128360 + df.Respectable_Job * -0.129551 + df.Good_Salary * -0.133278 + df.Convenient_Working_Hours *  -0.061196 + df.Personal_Development * 0.104857 + df.Contribution_to_Society * 0.211846 + df.Challenging_Interesting_Work * 0.189582 + df.Crime_Fighting * 0.177240
Misconduct_Index - Combine Variable
df['Misconduct_Index'] = df.Conviction_in_Court_Dico + df.Criminal_Record_Dico + df.Army_Disciplinary_Dico
class_counts_M_I = df['Misconduct_Index'].value_counts()
###print(class_counts_M_I)
United_Commander_or_Kazin - Combine Variable
df['United_Commander_or_Kazin'] = df.Commander_Army_Dico + df.Kazin_Army_Dico
class_counts_U_C_O_K = df['United_Commander_or_Kazin'].value_counts()
###print(class_counts_U_C_O_K)
United_Employment_Problems - Combine Variable
df['Financial_Difficulties_Dico_Rotated'] = df['Financial_Difficulties_Dico'] ^ 1
df['United_Employment_Problems'] = df.Financial_Difficulties_Dico_Rotated + df.Unemployment_Dico
class_counts_U_E_P = df['United_Employment_Problems'].value_counts()
###print(class_counts_U_E_P)
Small_Class_Dico_Index - Combine Variable
df['Small_Class_Dico_Index'] = df.Relevant_Education_Dico + df.Kaba_Grade_51_54_Dico + df.Past_Permanent_Officer_Dico + df.Underweight_BMI_Dumi + df.Another_Job_Nomination_Dico
class_counts_S_C_D_I = df['Small_Class_Dico_Index'].value_counts()
###print(class_counts_S_C_D_I)
Interests_and_Activities_Index - Combine Variable
df['Science_Rotated'] = df['Science'] ^ 1
df['Technology_and_Computers_Rotated'] = df['Technology_and_Computers'] ^ 1
df['Art_Rotated'] = df['Art'] ^ 1
df['Interests_and_Activities_Index'] = df.Science_Rotated + df.Technology_and_Computers_Rotated + df.Art_Rotated + df.Theater + df.Cooking + df.Volunteering
class_counts_Interests_and_Activities_Index = df['Interests_and_Activities_Index'].value_counts()
###print(class_counts_Interests_and_Activities_Index)
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Gold_List = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Evaluation_Center_Target"])
Gold_List[Gold_List.columns[0:]].corr(method='spearman')['Evaluation_Center_Target'].sort_values(ascending=False)
Evaluation_Center_Target           1.000000
Job_Motivators_Index               0.402411
Hebrew_Meam_Num                    0.238250
Education_and_Inteligence_Index    0.233453
Interests_and_Activities_Index     0.233427
United_Commander_or_Kazin          0.220433
Age_Num                            0.220380
United_Employment_Problems         0.203406
Psyc_Test_Index                    0.151941
Small_Class_Dico_Index             0.136727
Volunteering_Dico_Index            0.113056
Max_Procedure_Duration_Num         0.093545
Drinking_Alcohol_Frequ_Num         0.075455
Work_Perceived_Maching_Num         0.052768
Saham_Officer_Past_Dico            0.052476
Temp_Mean_Num                     -0.072066
Misconduct_Index                  -0.123042
Name: Evaluation_Center_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_M_L_R = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Evaluation_Center_Target"])
Gold_List_M_L_R_cleaned = Gold_List_M_L_R.dropna()
#x = Gold_List_M_L_R_cleaned[Gold_List_M_L_R_cleaned.columns[~Gold_List_M_L_R_cleaned.columns.isin(['Evaluation_Center_Target'])]]
#y = Gold_List_M_L_R_cleaned['Evaluation_Center_Target']
#x = sm.add_constant(x);

#regr = linear_model.LinearRegression()
#regr.fit(x, y);
#model = sm.OLS(y, x).fit();

#predictions = model.predict(x);
### See quality measurs of the model

#print_model = model.summary().tables[0]
#print(print_model)
### See quality measurs of the model

#print_model = model.summary().tables[1]
###print(print_model)
#x = sm.add_constant(x)

#vif = pd.DataFrame()
#vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
#vif["features"] = x.columns
###print(vif.round(1))
Upon evaluation, it was determined that the VIF index for the three weighted personality scales is less than 0.5. Consequently, there exists no concern regarding multicollinearity issues among these weighted factors.
Stepwise Linear Reggression
#pip install mlxtend
from sklearn.linear_model import LinearRegression
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import pandas as pd
#import statsmodels.api as sm
### Separate predictors and target variable
#X = Gold_List_M_L_R_cleaned.drop("Evaluation_Center_Target", axis=1)
#y = Gold_List_M_L_R_cleaned["Evaluation_Center_Target"]
### Forward stepwise feature selection using Linear Regression as base model
#lr = LinearRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="r2",
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = sm.OLS(y, sm.add_constant(X_selected)).fit()

### Summary of the regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
### Display regression report using statsmodels
#X_selected = sm.add_constant(X_selected)  # Add constant term for intercept
#model_stats = sm.OLS(y, X_selected).fit()

### Print regression report
#print(model_stats.summary())
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Z_Scores = pd.DataFrame(Gold_List_M_L_R, columns=["Education_and_Inteligence_Index", "Volunteering_Dico_Index", "Saham_Officer_Past_Dico", "Psyc_Test_Index", "Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "Small_Class_Dico_Index", "Interests_and_Activities_Index", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Age_Num", "Hebrew_Meam_Num", "Temp_Mean_Num", "Evaluation_Center_Target"])
descriptive_stats = Gold_List_Z_Scores.describe()
#descriptive_stats
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Evaluation_Center_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Z_Scores[numerical_columns] = (Gold_List_Z_Scores[numerical_columns] - Gold_List_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Z_Scores[numerical_columns].max() - Gold_List_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
### Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "RAMA_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

# Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_M_L_R.columns:
#    Final_Gold_List_Stand[column] = Gold_List_M_L_R[column]  / Gold_List_M_L_R[column].abs().max()
#Final_Gold_List_Stand[Final_Gold_List_Stand.columns[1:]].corr()['Evaluation_Center_Target'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
###Gold_List_Z_Scores[Gold_List_Z_Scores.columns[0:]].corr(method='spearman')['Evaluation_Center_Target'].sort_values(ascending=False)
#Gold_List_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Z_Scores.Job_Motivators_Index * 0.390 + Gold_List_Z_Scores.Education_and_Inteligence_Index * 0.253949 + Gold_List_Z_Scores.Hebrew_Meam_Num * 0.238250 + Gold_List_Z_Scores.Interests_and_Activities_Index * 0.233427 + Gold_List_Z_Scores.United_Commander_or_Kazin * 0.220433 + Gold_List_Z_Scores.Age_Num * 0.220380 + Gold_List_Z_Scores.United_Employment_Problems * 0.203406 + Gold_List_Z_Scores.Psyc_Test_Index * 0.138320 + Gold_List_Z_Scores.Small_Class_Dico_Index * 0.135312 + Gold_List_Z_Scores.Volunteering_Dico_Index * 0.113056 + Gold_List_Z_Scores.Max_Procedure_Duration_Num * 0.093545 + Gold_List_Z_Scores.Drinking_Alcohol_Frequ_Num * 0.075455 + Gold_List_Z_Scores.Work_Perceived_Maching_Num * 0.052768 + Gold_List_Z_Scores.Saham_Officer_Past_Dico * 0.052476 + (Gold_List_Z_Scores.Temp_Mean_Num *  -0.114862) + (Gold_List_Z_Scores.Misconduct_Index * -0.123042)
#Gold_List_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Evaluation_Center_Target']).round(2)
1.8 Creating Final List
# Create the new DataFrame 'Final_list'
Final_list = pd.DataFrame(Gold_List_M_L_R)
# Drop rows with missing values
Final_list = Final_list.dropna()
for col in Final_list:
    if col in Final_list.columns:
        Final_list[col] = Final_list[col].astype(np.float64)
###Final_list.head()
------------Feature Selection - Final Gibushon Grade------------
df_num_corr = pd.DataFrame(Final_list)
corr = Final_list.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list
#for i in Final_list.columns:
#        for j in Final_list.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list[i]) & ~pd.isna(Final_list[j]) 
#                    a = stats.spearmanr(Final_list[i][mask], Final_list[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
#pip install numpy
#pip install --upgrade numpy
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import linear_model
import statsmodels.api as sm
# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list.drop('Evaluation_Center_Target', axis=1)
y = Final_list['Evaluation_Center_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the linear regression model
regr = linear_model.LinearRegression()
regr.fit(x, y)

# Create the model
model = sm.OLS(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)
                               OLS Regression Results                               
====================================================================================
Dep. Variable:     Evaluation_Center_Target   R-squared:                       0.329
Model:                                  OLS   Adj. R-squared:                  0.268
Method:                       Least Squares   F-statistic:                     5.394
Date:                      Tue, 09 Apr 2024   Prob (F-statistic):           3.07e-09
Time:                              16:16:30   Log-Likelihood:                -234.82
No. Observations:                       193   AIC:                             503.6
Df Residuals:                           176   BIC:                             559.1
Df Model:                                16                                         
Covariance Type:                  nonrobust                                         
===================================================================================================
                                      coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------
const                               0.4013      0.860      0.467      0.641      -1.296       2.099
Education_and_Inteligence_Index     0.0659      0.119      0.556      0.579      -0.168       0.300
Volunteering_Dico_Index             0.0351      0.101      0.348      0.728      -0.164       0.234
Saham_Officer_Past_Dico             0.0436      0.404      0.108      0.914      -0.754       0.841
Psyc_Test_Index                     0.1400      0.165      0.848      0.398      -0.186       0.466
Job_Motivators_Index                1.1766      0.281      4.181      0.000       0.621       1.732
Misconduct_Index                   -0.2396      0.133     -1.806      0.073      -0.502       0.022
United_Commander_or_Kazin           0.4093      0.144      2.844      0.005       0.125       0.693
United_Employment_Problems          0.2380      0.158      1.502      0.135      -0.075       0.551
Small_Class_Dico_Index              0.0131      0.127      0.103      0.918      -0.237       0.263
Interests_and_Activities_Index      0.1292      0.055      2.340      0.020       0.020       0.238
Max_Procedure_Duration_Num          0.0046      0.031      0.148      0.883      -0.057       0.066
Drinking_Alcohol_Frequ_Num          0.0462      0.075      0.617      0.538      -0.101       0.194
Work_Perceived_Maching_Num          0.0191      0.093      0.206      0.837      -0.164       0.202
Age_Num                             0.0296      0.014      2.172      0.031       0.003       0.056
Hebrew_Meam_Num                     0.1298      0.083      1.566      0.119      -0.034       0.293
Temp_Mean_Num                      -0.1557      0.118     -1.321      0.188      -0.388       0.077
==============================================================================
Omnibus:                        4.804   Durbin-Watson:                   2.201
Prob(Omnibus):                  0.091   Jarque-Bera (JB):                4.782
Skew:                           0.385   Prob(JB):                       0.0915
Kurtosis:                       2.945   Cond. No.                         395.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                          Feature
0     1.636571  Education_and_Inteligence_Index
1     1.152596          Volunteering_Dico_Index
2     1.085846          Saham_Officer_Past_Dico
3     1.322762                  Psyc_Test_Index
4     1.217089             Job_Motivators_Index
5     1.071514                 Misconduct_Index
6     1.072836        United_Commander_or_Kazin
7     1.320270       United_Employment_Problems
8     1.411405           Small_Class_Dico_Index
9     1.107787   Interests_and_Activities_Index
10    1.095800       Max_Procedure_Duration_Num
11    1.142745       Drinking_Alcohol_Frequ_Num
12    1.107246       Work_Perceived_Maching_Num
13    1.270716                          Age_Num
14    1.185536                  Hebrew_Meam_Num
15    1.077663                    Temp_Mean_Num
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"] >= 5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_List_filtered = Final_list.drop(high_vif_features, axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_List_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (193, 17)
#Final_List_filtered.info()
1.5 The Feature Selection Process
No_colinear_list = Final_List_filtered.astype({"Evaluation_Center_Target":'category'})
No_colinear_list = pd.DataFrame(No_colinear_list)
temp_cols=No_colinear_list.columns.tolist()
index=No_colinear_list.columns.get_loc("Evaluation_Center_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list=No_colinear_list[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list.columns[1:40]})
#varSel.head(50)
nm = No_colinear_list.columns[1:40]
nm = nm.append(pd.Index(['Evaluation_Center_Target']))
#nm
df2 = No_colinear_list[nm].copy()
###df2.head(50)
5.2 Multivariable Analysis
No_colinear_list = No_colinear_list.dropna()
X = No_colinear_list[No_colinear_list.columns[~No_colinear_list.columns.isin(['Evaluation_Center_Target'])]]
y = No_colinear_list['Evaluation_Center_Target']
###y.head()
###print([X.shape,y.shape])
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso
0   Education_and_Inteligence_Index      0
1           Volunteering_Dico_Index      0
2           Saham_Officer_Past_Dico      0
3                   Psyc_Test_Index      0
4              Job_Motivators_Index      0
5                  Misconduct_Index      0
6         United_Commander_or_Kazin      0
7        United_Employment_Problems      0
8            Small_Class_Dico_Index      0
9    Interests_and_Activities_Index      0
10       Max_Procedure_Duration_Num      0
11       Drinking_Alcohol_Frequ_Num      0
12       Work_Perceived_Maching_Num      0
13                          Age_Num      1
14                  Hebrew_Meam_Num      0
15                    Temp_Mean_Num      0
from sklearn.linear_model import Ridge

ridge_mod = Ridge().fit(X, y)
model = SelectFromModel(ridge_mod, prefit=True)
varSel['Ridge'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso  Ridge
0   Education_and_Inteligence_Index      0      0
1           Volunteering_Dico_Index      0      0
2           Saham_Officer_Past_Dico      0      0
3                   Psyc_Test_Index      0      0
4              Job_Motivators_Index      0      1
5                  Misconduct_Index      0      1
6         United_Commander_or_Kazin      0      1
7        United_Employment_Problems      0      1
8            Small_Class_Dico_Index      0      0
9    Interests_and_Activities_Index      0      0
10       Max_Procedure_Duration_Num      0      0
11       Drinking_Alcohol_Frequ_Num      0      0
12       Work_Perceived_Maching_Num      0      0
13                          Age_Num      1      0
14                  Hebrew_Meam_Num      0      0
15                    Temp_Mean_Num      0      0
from sklearn.linear_model import ElasticNet

elastic_net_mod = ElasticNet().fit(X, y)
model = SelectFromModel(elastic_net_mod, prefit=True)
varSel['ElasticNet'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso  Ridge  ElasticNet
0   Education_and_Inteligence_Index      0      0           0
1           Volunteering_Dico_Index      0      0           0
2           Saham_Officer_Past_Dico      0      0           0
3                   Psyc_Test_Index      0      0           0
4              Job_Motivators_Index      0      1           0
5                  Misconduct_Index      0      1           0
6         United_Commander_or_Kazin      0      1           0
7        United_Employment_Problems      0      1           0
8            Small_Class_Dico_Index      0      0           0
9    Interests_and_Activities_Index      0      0           0
10       Max_Procedure_Duration_Num      0      0           0
11       Drinking_Alcohol_Frequ_Num      0      0           0
12       Work_Perceived_Maching_Num      0      0           0
13                          Age_Num      1      0           1
14                  Hebrew_Meam_Num      0      0           0
15                    Temp_Mean_Num      0      0           0
from sklearn.tree import DecisionTreeRegressor

dt_mod = DecisionTreeRegressor().fit(X, y)
model = SelectFromModel(dt_mod, prefit=True)
varSel['DecisionTree'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree
0   Education_and_Inteligence_Index      0      0           0             1
1           Volunteering_Dico_Index      0      0           0             0
2           Saham_Officer_Past_Dico      0      0           0             0
3                   Psyc_Test_Index      0      0           0             0
4              Job_Motivators_Index      0      1           0             1
5                  Misconduct_Index      0      1           0             0
6         United_Commander_or_Kazin      0      1           0             0
7        United_Employment_Problems      0      1           0             0
8            Small_Class_Dico_Index      0      0           0             0
9    Interests_and_Activities_Index      0      0           0             0
10       Max_Procedure_Duration_Num      0      0           0             0
11       Drinking_Alcohol_Frequ_Num      0      0           0             0
12       Work_Perceived_Maching_Num      0      0           0             0
13                          Age_Num      1      0           1             1
14                  Hebrew_Meam_Num      0      0           0             1
15                    Temp_Mean_Num      0      0           0             1
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  
0                  1  
1                  0  
2                  0  
3                  0  
4                  1  
5                  0  
6                  0  
7                  0  
8                  0  
9                  1  
10                 0  
11                 0  
12                 0  
13                 1  
14                 1  
15                 1  
from sklearn.linear_model import LinearRegression

linear_mod = LinearRegression().fit(X, y)
model = SelectFromModel(linear_mod, prefit=True)
varSel['LinearRegression'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  
0                  1                 0  
1                  0                 0  
2                  0                 0  
3                  0                 0  
4                  1                 1  
5                  0                 1  
6                  0                 1  
7                  0                 1  
8                  0                 0  
9                  1                 0  
10                 0                 0  
11                 0                 0  
12                 0                 0  
13                 1                 0  
14                 1                 0  
15                 1                 0  
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

rf_mod = RandomForestRegressor().fit(X, y)
model = SelectFromModel(rf_mod, prefit=True)
varSel['RandomForest'] = model.get_support().astype('int64')

RandomForestRegressor

varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  
0                  1                 0             1  
1                  0                 0             0  
2                  0                 0             0  
3                  0                 0             0  
4                  1                 1             1  
5                  0                 1             0  
6                  0                 1             0  
7                  0                 1             0  
8                  0                 0             0  
9                  1                 0             1  
10                 0                 0             0  
11                 0                 0             0  
12                 0                 0             0  
13                 1                 0             1  
14                 1                 0             1  
15                 1                 0             1  
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  
0                  1                 0             1  
1                  0                 0             0  
2                  0                 0             0  
3                  0                 0             0  
4                  1                 1             1  
5                  0                 1             0  
6                  0                 1             0  
7                  0                 1             0  
8                  0                 0             0  
9                  1                 0             1  
10                 0                 0             0  
11                 0                 0             0  
12                 0                 0             0  
13                 1                 0             1  
14                 1                 0             1  
15                 1                 0             1  
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  AdaBoost  
0                  1                 0             1         1  
1                  0                 0             0         0  
2                  0                 0             0         0  
3                  0                 0             0         1  
4                  1                 1             1         0  
5                  0                 1             0         0  
6                  0                 1             0         0  
7                  0                 1             0         0  
8                  0                 0             0         0  
9                  1                 0             1         0  
10                 0                 0             0         0  
11                 0                 0             0         0  
12                 0                 0             0         0  
13                 1                 0             1         0  
14                 1                 0             1         0  
15                 1                 0             1         0  
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)

model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  AdaBoost  SVM  
0                  1                 0             1         1    0  
1                  0                 0             0         0    0  
2                  0                 0             0         0    0  
3                  0                 0             0         1    0  
4                  1                 1             1         0    0  
5                  0                 1             0         0    0  
6                  0                 1             0         0    0  
7                  0                 1             0         0    0  
8                  0                 0             0         0    0  
9                  1                 0             1         0    0  
10                 0                 0             0         0    0  
11                 0                 0             0         0    0  
12                 0                 0             0         0    1  
13                 1                 0             1         0    1  
14                 1                 0             1         0    0  
15                 1                 0             1         0    0  
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor().fit(X, y_transformed)

model = SelectFromModel(SGD_Regressor, prefit=True)
model.get_support()

varSel['SGD'] = model.get_support().astype('int64')
varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  AdaBoost  SVM  SGD  
0                  1                 0             1         1    0    0  
1                  0                 0             0         0    0    0  
2                  0                 0             0         0    0    1  
3                  0                 0             0         1    0    1  
4                  1                 1             1         0    0    1  
5                  0                 1             0         0    0    0  
6                  0                 1             0         0    0    0  
7                  0                 1             0         0    0    0  
8                  0                 0             0         0    0    0  
9                  1                 0             1         0    0    0  
10                 0                 0             0         0    0    0  
11                 0                 0             0         0    0    0  
12                 0                 0             0         0    1    0  
13                 1                 0             1         0    1    0  
14                 1                 0             1         0    0    0  
15                 1                 0             1         0    0    0  
import pandas as pd
import statsmodels.api as sm

# Assuming X and y are already defined and varSel is initialized

# Fit an OLS regression model
ols_model = sm.OLS(y, X).fit()

# Get p-values for each feature
p_values = ols_model.pvalues

# Perform feature selection based on a significance level (e.g., 0.05)
selected_features = p_values[p_values < 0.05].index

# Store the selected features in varSel
varSel['OLS'] = [1 if feature in selected_features else 0 for feature in X.columns]

# Print model summary
#print(ols_model.summary())

varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  AdaBoost  SVM  SGD  OLS  
0                  1                 0             1         1    0    0    0  
1                  0                 0             0         0    0    0    0  
2                  0                 0             0         0    0    1    0  
3                  0                 0             0         1    0    1    0  
4                  1                 1             1         0    0    1    1  
5                  0                 1             0         0    0    0    0  
6                  0                 1             0         0    0    0    1  
7                  0                 1             0         0    0    0    0  
8                  0                 0             0         0    0    0    0  
9                  1                 0             1         0    0    0    1  
10                 0                 0             0         0    0    0    0  
11                 0                 0             0         0    0    0    0  
12                 0                 0             0         0    1    0    0  
13                 1                 0             1         0    1    0    1  
14                 1                 0             1         0    0    0    0  
15                 1                 0             1         0    0    0    0  
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
varSel
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
1           Volunteering_Dico_Index      0      0           0             0   
2           Saham_Officer_Past_Dico      0      0           0             0   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
8            Small_Class_Dico_Index      0      0           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
10       Max_Procedure_Duration_Num      0      0           0             0   
11       Drinking_Alcohol_Frequ_Num      0      0           0             0   
12       Work_Perceived_Maching_Num      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  AdaBoost  SVM  SGD  OLS  \
0                  1                 0             1         1    0    0    0   
1                  0                 0             0         0    0    0    0   
2                  0                 0             0         0    0    1    0   
3                  0                 0             0         1    0    1    0   
4                  1                 1             1         0    0    1    1   
5                  0                 1             0         0    0    0    0   
6                  0                 1             0         0    0    0    1   
7                  0                 1             0         0    0    0    0   
8                  0                 0             0         0    0    0    0   
9                  1                 0             1         0    0    0    1   
10                 0                 0             0         0    0    0    0   
11                 0                 0             0         0    0    0    0   
12                 0                 0             0         0    1    0    0   
13                 1                 0             1         0    1    0    1   
14                 1                 0             1         0    0    0    0   
15                 1                 0             1         0    0    0    0   

    Sum  
0     4  
1     0  
2     1  
3     2  
4     7  
5     2  
6     3  
7     2  
8     0  
9     3  
10    0  
11    0  
12    1  
13    7  
14    3  
15    3  
#varSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_List_Feature_Selection = varSel[varSel['Sum']>=2]
Final_List_Feature_Selection
#varSel[varSel['Sum']>=2]
                           Variable  Lasso  Ridge  ElasticNet  DecisionTree  \
0   Education_and_Inteligence_Index      0      0           0             1   
3                   Psyc_Test_Index      0      0           0             0   
4              Job_Motivators_Index      0      1           0             1   
5                  Misconduct_Index      0      1           0             0   
6         United_Commander_or_Kazin      0      1           0             0   
7        United_Employment_Problems      0      1           0             0   
9    Interests_and_Activities_Index      0      0           0             0   
13                          Age_Num      1      0           1             1   
14                  Hebrew_Meam_Num      0      0           0             1   
15                    Temp_Mean_Num      0      0           0             1   

    GradientBoosting  LinearRegression  RandomForest  AdaBoost  SVM  SGD  OLS  \
0                  1                 0             1         1    0    0    0   
3                  0                 0             0         1    0    1    0   
4                  1                 1             1         0    0    1    1   
5                  0                 1             0         0    0    0    0   
6                  0                 1             0         0    0    0    1   
7                  0                 1             0         0    0    0    0   
9                  1                 0             1         0    0    0    1   
13                 1                 0             1         0    1    0    1   
14                 1                 0             1         0    0    0    0   
15                 1                 0             1         0    0    0    0   

    Sum  
0     4  
3     2  
4     7  
5     2  
6     3  
7     2  
9     3  
13    7  
14    3  
15    3  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"] <= 2]["Variable"].tolist()

# Omit features with high VIF from the DataFrame
Gibushon_Sofi_Gold_List = Final_List_filtered.drop(low_sig_corrs, axis=1)
Gibushon_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 193 entries, 2 to 2225
Data columns (total 8 columns):
 #   Column                           Non-Null Count  Dtype  
---  ------                           --------------  -----  
 0   Education_and_Inteligence_Index  193 non-null    float64
 1   Job_Motivators_Index             193 non-null    float64
 2   United_Commander_or_Kazin        193 non-null    float64
 3   Interests_and_Activities_Index   193 non-null    float64
 4   Age_Num                          193 non-null    float64
 5   Hebrew_Meam_Num                  193 non-null    float64
 6   Temp_Mean_Num                    193 non-null    float64
 7   Evaluation_Center_Target         193 non-null    float64
dtypes: float64(8)
memory usage: 13.6 KB
---- Rama Grade ----
------------Feature Engineering-------------
Educational achievement index - Combine Variable
df['Rama_Educational_achievement_index'] = df.Graduation_Average_85_Up_Dico + df.English_4_5_Units_Num_Dico + df.Math_4_5_Units_Num_Dico + df.Academic_Education_Dico + df.Psyc_Test_600_Up_Dico
class_counts_Rama_Educational_achievement_index = df['Rama_Educational_achievement_index'].value_counts()
###print(class_counts_Rama_Educational_achievement_index)
United_Commander_or_Kazin - Combine Variable
df['Rama_United_Commander_or_Kazin'] = df.Commander_Army_Dico + df.Kazin_Army_Dico
class_counts_Rama_United_Commander_or_Kazin = df['Rama_United_Commander_or_Kazin'].value_counts()
###print(class_counts_Rama_United_Commander_or_Kazin)
Misconduct_Index - Combine Variable
df['Rama_Index_Criminal_History'] = df.Previous_Arrest_Dico + df.Criminal_Record_Dico + df.Army_Disciplinary_Dico + df.Arrest_Prison_Army_Dico + df.Conviction_Army_Dico
class_counts_M_I = df['Rama_Index_Criminal_History'].value_counts()
###print(class_counts_M_I)
Sacham_officer_Dico
class_counts_M_I = df['Sacham_officer_Dico'].value_counts()
###print(class_counts_M_I)
Married_Dico
class_counts_M_I = df['Married_Dico'].value_counts()
###print(class_counts_M_I)
Drinking_Alcohol_Frequ_Num
class_counts_M_I = df['Drinking_Alcohol_Frequ_Num'].value_counts()
###print(class_counts_M_I)
Number_of_Attempts_Num
class_counts_M_I = df['Number_of_Attempts_Num'].value_counts()
###print(class_counts_M_I)
Work_Perceived_Maching_Num
class_counts_M_I = df['Work_Perceived_Maching_Num'].value_counts()
###print(class_counts_M_I)
Temp_Mean_Num
# List of variables for which to calculate the average
Rama_Temp_variables = [
    'Temper_Control_2_Num',
    'Temper_Control_5_Num'
]

# Create a new column 'Temp_Mean_Num' containing the average of the specified variables
df['Rama_Temp_Mean_Num'] = df[Rama_Temp_variables].mean(axis=1)

# Display the modified data frame
#print(df)
Job_Motivators_Index - Combine Variable
df['Rama_Job_Motivators_Index'] = df.Challenging_Interesting_Work - df.Wearing_Uniform - df.Good_Salary - df.Exercise_Authority - df.Respectable_Job - df.Convenient_Working_Hours + df.Contribution_to_Society - df.The_Action_In_Work + df.Personal_Development + df.Interaction_With_People 
#df['Rama_Job_Motivators_Index'] = df.Wearing_Uniform * -0.097382 + df.Good_Salary * -0.065341 + df.Exercise_Authority * -0.080737 + df.Respectable_Job * -0.112960 + df.Challenging_Interesting_Work * 0.072071 + df.Convenient_Working_Hours * -0.072850 + df.Contribution_to_Society * 0.085553 + df.The_Action_In_Work * -0.097059 + df.Personal_Development * 0.075 + df.Interaction_With_People * 0.075
Interests_and_Activities_Index - Combine Variable
df['Rama_Interests_and_Activities_Index'] = df.Meeting_With_Friends - df.Technology_and_Computers - df.Theater + df.Actuavlia + df.Psychology + df.Managing_Healthy_Lifestyle
#df['Rama_Interests_and_Activities_Index'] = df.Meeting_With_Friends * 0.097476 + df.Technology_and_Computers * -0.074211 + df.Theater * -0.095627 + df.Actuavlia * 0.066915 + df.Psychology * 0.051 + df.Managing_Healthy_Lifestyle * 0.051
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Rama_Gold_List = pd.DataFrame(df, columns=["Rama_Educational_achievement_index", "Rama_United_Commander_or_Kazin", "Rama_Index_Criminal_History", "Sacham_officer_Dico", "Married_Dico", "Drinking_Alcohol_Frequ_Num", "Number_of_Attempts_Num", "Work_Perceived_Maching_Num", "Rama_Temp_Mean_Num", "Rama_Job_Motivators_Index", "Rama_Interests_and_Activities_Index", "RAMA_Target"])
for col in Rama_Gold_List:
    if col in Rama_Gold_List.columns:
        Rama_Gold_List[col] = Rama_Gold_List[col].astype(np.float64)
Rama_Gold_List[Rama_Gold_List.columns[0:]].corr(method='spearman')['RAMA_Target'].sort_values(ascending=False)
RAMA_Target                            1.000000
Rama_Educational_achievement_index     0.269311
Rama_United_Commander_or_Kazin         0.222703
Rama_Job_Motivators_Index              0.209701
Rama_Interests_and_Activities_Index    0.149854
Drinking_Alcohol_Frequ_Num             0.109490
Married_Dico                           0.103571
Rama_Temp_Mean_Num                     0.082758
Sacham_officer_Dico                    0.070728
Work_Perceived_Maching_Num             0.057680
Number_of_Attempts_Num                -0.065980
Rama_Index_Criminal_History           -0.140421
Name: RAMA_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Rama_Linear = pd.DataFrame(Rama_Gold_List, columns=["Rama_Educational_achievement_index", "Rama_United_Commander_or_Kazin", "Rama_Index_Criminal_History", "Sacham_officer_Dico", "Married_Dico", "Drinking_Alcohol_Frequ_Num", "Number_of_Attempts_Num", "Work_Perceived_Maching_Num", "Rama_Temp_Mean_Num", "Rama_Job_Motivators_Index", "Rama_Interests_and_Activities_Index", "RAMA_Target"])
Gold_List_Rama_Linear_cleaned = Gold_List_Rama_Linear.dropna()
#x = Gold_List_Rama_Linear_cleaned[Gold_List_Rama_Linear_cleaned.columns[~Gold_List_Rama_Linear_cleaned.columns.isin(["RAMA_Target"])]]
#y = Gold_List_Rama_Linear_cleaned['RAMA_Target']
#x = sm.add_constant(x);

#regr = linear_model.LinearRegression()
#regr.fit(x, y);
#model = sm.OLS(y, x).fit();

#predictions = model.predict(x);
### See quality measurs of the model

#print_model = model.summary().tables[0]
###print(print_model)
### See quality measurs of the model

#print_model = model.summary().tables[1]
###print(print_model)
#x = sm.add_constant(x)

#vif = pd.DataFrame()
#vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
#vif["features"] = x.columns
#print(vif.round(1))
Upon evaluation, it was determined that the VIF index for the three weighted personality scales is less than 0.5. Consequently, there exists no concern regarding multicollinearity issues among these weighted factors.
Stepwise Linear Reggression
#pip install mlxtend
# Separate predictors and target variable
#X = Gold_List_Rama_Linear_cleaned.drop("RAMA_Target", axis=1)
#y = Gold_List_Rama_Linear_cleaned["RAMA_Target"]
### Forward stepwise feature selection using Linear Regression as base model
#lr = LinearRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="r2",
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = sm.OLS(y, sm.add_constant(X_selected)).fit()

### Summary of the regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
### Display regression report using statsmodels
#X_selected = sm.add_constant(X_selected)  # Add constant term for intercept
#model_stats = sm.OLS(y, X_selected).fit()

### Print regression report
#print(model_stats.summary())
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Rama_Z_Scores = pd.DataFrame(Gold_List_Rama_Linear_cleaned, columns=["Rama_Educational_achievement_index", "Rama_United_Commander_or_Kazin", "Rama_Index_Criminal_History", "Sacham_officer_Dico", "Married_Dico", "Drinking_Alcohol_Frequ_Num", "Number_of_Attempts_Num", "Work_Perceived_Maching_Num", "Rama_Temp_Mean_Num", "Rama_Job_Motivators_Index", "Rama_Interests_and_Activities_Index", "RAMA_Target"])
#descriptive_stats_Rama = Gold_List_Rama_Z_Scores.describe()
#descriptive_stats_Rama
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["RAMA_Target"]
#numerical_columns = Gold_List_Rama_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Rama_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Rama_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Rama_Z_Scores[numerical_columns] = (Gold_List_Rama_Z_Scores[numerical_columns] - Gold_List_Rama_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Rama_Z_Scores[numerical_columns].max() - Gold_List_Rama_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Rama_Z_Scores[numerical_columns] = Gold_List_Rama_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Rama_Z_Scores)
#Gold_List_Rama_Z_Scores.head()
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
### Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "RAMA_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Rama_Linear_cleaned.columns:
#    Gold_List_Rama_Linear_cleaned[column] = Gold_List_Rama_Linear_cleaned[column]  / Gold_List_Rama_Linear_cleaned[column].abs().max()
#Gold_List_Rama_Linear_cleaned[Gold_List_Rama_Linear_cleaned.columns[1:]].corr()['RAMA_Target'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
Gold_List_Rama_Z_Scores[Gold_List_Rama_Z_Scores.columns[0:]].corr(method='spearman')['RAMA_Target'].sort_values(ascending=False)
RAMA_Target                            1.000000
Rama_Educational_achievement_index     0.269311
Rama_United_Commander_or_Kazin         0.222703
Rama_Job_Motivators_Index              0.209701
Rama_Interests_and_Activities_Index    0.149854
Drinking_Alcohol_Frequ_Num             0.109490
Married_Dico                           0.103571
Rama_Temp_Mean_Num                     0.082758
Sacham_officer_Dico                    0.070728
Work_Perceived_Maching_Num             0.057680
Number_of_Attempts_Num                -0.065980
Rama_Index_Criminal_History           -0.140421
Name: RAMA_Target, dtype: float64
#Gold_List_Rama_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Rama_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Rama_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['RAMA_Target']).round(2)
1.8 Creating Final List
# Create the new DataFrame 'Final_list'
Final_list_Rama = pd.DataFrame(Gold_List_Rama_Linear_cleaned)
# Drop rows with missing values
Final_list_Rama = Final_list_Rama.dropna()
for col in Final_list_Rama:
    if col in Final_list_Rama.columns:
        Final_list_Rama[col] = Final_list_Rama[col].astype(np.float64)
------------Feature Selection - Final Gibushon Grade------------
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
df_num_corr = pd.DataFrame(Final_list_Rama)
corr = Final_list_Rama.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Rama
#for i in Final_list_Rama.columns:
#        for j in Final_list_Rama.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Rama[i]) & ~pd.isna(Final_list_Rama[j]) 
#                    a = stats.spearmanr(Final_list_Rama[i][mask], Final_list_Rama[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import linear_model

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Rama.drop('RAMA_Target', axis=1)
y = Final_list_Rama['RAMA_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the linear regression model
regr = linear_model.LinearRegression()
regr.fit(x, y)

# Create the model
model = sm.OLS(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:            RAMA_Target   R-squared:                       0.223
Model:                            OLS   Adj. R-squared:                  0.210
Method:                 Least Squares   F-statistic:                     17.24
Date:                Tue, 09 Apr 2024   Prob (F-statistic):           3.32e-30
Time:                        16:16:32   Log-Likelihood:                -658.63
No. Observations:                 672   AIC:                             1341.
Df Residuals:                     660   BIC:                             1395.
Df Model:                          11                                         
Covariance Type:            nonrobust                                         
=======================================================================================================
                                          coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------------
const                                   2.6190      0.273      9.579      0.000       2.082       3.156
Rama_Educational_achievement_index      0.0933      0.020      4.601      0.000       0.053       0.133
Rama_United_Commander_or_Kazin          0.2958      0.050      5.898      0.000       0.197       0.394
Rama_Index_Criminal_History            -0.1416      0.031     -4.538      0.000      -0.203      -0.080
Sacham_officer_Dico                     0.2231      0.079      2.807      0.005       0.067       0.379
Married_Dico                            0.1818      0.063      2.884      0.004       0.058       0.306
Drinking_Alcohol_Frequ_Num              0.0916      0.029      3.108      0.002       0.034       0.150
Number_of_Attempts_Num                 -0.0357      0.022     -1.610      0.108      -0.079       0.008
Work_Perceived_Maching_Num              0.0641      0.036      1.781      0.075      -0.007       0.135
Rama_Temp_Mean_Num                      0.0803      0.041      1.976      0.049       0.001       0.160
Rama_Job_Motivators_Index               0.0726      0.019      3.854      0.000       0.036       0.110
Rama_Interests_and_Activities_Index     0.0701      0.023      3.065      0.002       0.025       0.115
==============================================================================
Omnibus:                       10.556   Durbin-Watson:                   1.972
Prob(Omnibus):                  0.005   Jarque-Bera (JB):               12.264
Skew:                           0.213   Prob(JB):                      0.00217
Kurtosis:                       3.506   Cond. No.                         90.0
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                              Feature
0     1.100589   Rama_Educational_achievement_index
1     1.066627       Rama_United_Commander_or_Kazin
2     1.066008          Rama_Index_Criminal_History
3     1.017450                  Sacham_officer_Dico
4     1.088348                         Married_Dico
5     1.070527           Drinking_Alcohol_Frequ_Num
6     1.056164               Number_of_Attempts_Num
7     1.045185           Work_Perceived_Maching_Num
8     1.034732                   Rama_Temp_Mean_Num
9     1.059419            Rama_Job_Motivators_Index
10    1.026244  Rama_Interests_and_Activities_Index
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"] >= 5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Rama_filtered = Final_list_Rama.drop(high_vif_features, axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Rama_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (672, 12)
Final_list_Rama = Final_list_Rama_filtered.astype({"RAMA_Target":'category'})
Final_list_Rama.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 672 entries, 1 to 2242
Data columns (total 12 columns):
 #   Column                               Non-Null Count  Dtype   
---  ------                               --------------  -----   
 0   Rama_Educational_achievement_index   672 non-null    float64 
 1   Rama_United_Commander_or_Kazin       672 non-null    float64 
 2   Rama_Index_Criminal_History          672 non-null    float64 
 3   Sacham_officer_Dico                  672 non-null    float64 
 4   Married_Dico                         672 non-null    float64 
 5   Drinking_Alcohol_Frequ_Num           672 non-null    float64 
 6   Number_of_Attempts_Num               672 non-null    float64 
 7   Work_Perceived_Maching_Num           672 non-null    float64 
 8   Rama_Temp_Mean_Num                   672 non-null    float64 
 9   Rama_Job_Motivators_Index            672 non-null    float64 
 10  Rama_Interests_and_Activities_Index  672 non-null    float64 
 11  RAMA_Target                          672 non-null    category
dtypes: category(1), float64(11)
memory usage: 64.0 KB
1.5 The Feature Selection Process
No_colinear_list_Rama = Final_list_Rama.astype({"RAMA_Target":'category'})
No_colinear_list_Rama.dtypes
Rama_Educational_achievement_index      float64
Rama_United_Commander_or_Kazin          float64
Rama_Index_Criminal_History             float64
Sacham_officer_Dico                     float64
Married_Dico                            float64
Drinking_Alcohol_Frequ_Num              float64
Number_of_Attempts_Num                  float64
Work_Perceived_Maching_Num              float64
Rama_Temp_Mean_Num                      float64
Rama_Job_Motivators_Index               float64
Rama_Interests_and_Activities_Index     float64
RAMA_Target                            category
dtype: object
No_colinear_list_Rama = pd.DataFrame(No_colinear_list_Rama)
temp_cols=No_colinear_list_Rama.columns.tolist()
index=No_colinear_list_Rama.columns.get_loc("RAMA_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Rama=No_colinear_list_Rama[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Rama.columns[1:40]})
varSel.head(50)
                               Variable
0    Rama_Educational_achievement_index
1        Rama_United_Commander_or_Kazin
2           Rama_Index_Criminal_History
3                   Sacham_officer_Dico
4                          Married_Dico
5            Drinking_Alcohol_Frequ_Num
6                Number_of_Attempts_Num
7            Work_Perceived_Maching_Num
8                    Rama_Temp_Mean_Num
9             Rama_Job_Motivators_Index
10  Rama_Interests_and_Activities_Index
nm = No_colinear_list_Rama.columns[1:40]
nm = nm.append(pd.Index(['RAMA_Target']))
#nm
df2_Rama = No_colinear_list_Rama[nm].copy()
#df2_Rama.head(50)
5.2 Multivariable Analysis
No_colinear_list_Rama = No_colinear_list_Rama.dropna()
X = No_colinear_list_Rama[No_colinear_list_Rama.columns[~No_colinear_list_Rama.columns.isin(['RAMA_Target'])]]
y = No_colinear_list_Rama['RAMA_Target']
print([X.shape,y.shape])
[(672, 11), (672,)]
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import Ridge

ridge_mod = Ridge().fit(X, y)
model = SelectFromModel(ridge_mod, prefit=True)
varSel['Ridge'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import ElasticNet

elastic_net_mod = ElasticNet().fit(X, y)
model = SelectFromModel(elastic_net_mod, prefit=True)
varSel['ElasticNet'] = model.get_support().astype('int64')

###varSel
from sklearn.tree import DecisionTreeRegressor

dt_mod = DecisionTreeRegressor().fit(X, y)
model = SelectFromModel(dt_mod, prefit=True)
varSel['DecisionTree'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
import pandas as pd
import statsmodels.api as sm

# Assuming X and y are already defined and varSel is initialized

# Fit an OLS regression model
ols_model = sm.OLS(y, X).fit()

# Get p-values for each feature
p_values = ols_model.pvalues

# Perform feature selection based on a significance level (e.g., 0.05)
selected_features = p_values[p_values < 0.05].index

# Store the selected features in varSel
varSel['OLS'] = [1 if feature in selected_features else 0 for feature in X.columns]

# Print model summary
#print(ols_model.summary())

###varSel
from sklearn.linear_model import LinearRegression

linear_mod = LinearRegression().fit(X, y)
model = SelectFromModel(linear_mod, prefit=True)
varSel['LinearRegression'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

rf_mod = RandomForestRegressor().fit(X, y)
model = SelectFromModel(rf_mod, prefit=True)
varSel['RandomForest'] = model.get_support().astype('int64')

RandomForestRegressor

###varSel
sklearn.ensemble._forest.RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
###varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
###varSel
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor().fit(X, y_transformed)

model = SelectFromModel(SGD_Regressor, prefit=True)
model.get_support()

varSel['SGD'] = model.get_support().astype('int64')
###varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
#varSel
#varSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_List_RAMA_Feature_Selection = varSel[varSel['Sum']>=3]
Final_List_RAMA_Feature_Selection 
                               Variable  Lasso  Ridge  ElasticNet  \
0    Rama_Educational_achievement_index      0      0           1   
1        Rama_United_Commander_or_Kazin      0      1           1   
2           Rama_Index_Criminal_History      0      1           1   
3                   Sacham_officer_Dico      0      1           1   
4                          Married_Dico      0      1           1   
5            Drinking_Alcohol_Frequ_Num      0      0           1   
7            Work_Perceived_Maching_Num      0      0           1   
8                    Rama_Temp_Mean_Num      0      0           1   
9             Rama_Job_Motivators_Index      0      0           1   
10  Rama_Interests_and_Activities_Index      0      0           1   

    DecisionTree  GradientBoosting  OLS  LinearRegression  RandomForest  \
0              1                 1    1                 0             1   
1              0                 1    1                 1             0   
2              0                 1    1                 1             0   
3              0                 0    1                 1             0   
4              0                 0    1                 1             0   
5              1                 1    1                 0             1   
7              0                 0    1                 0             0   
8              1                 0    1                 0             1   
9              1                 1    1                 0             1   
10             1                 0    1                 0             1   

    AdaBoost  SVM  SGD  Sum  
0          0    1    0    6  
1          1    0    1    7  
2          1    0    1    7  
3          0    0    1    5  
4          0    0    1    5  
5          0    1    0    6  
7          0    1    1    4  
8          0    1    1    6  
9          0    1    0    6  
10         1    1    0    6  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"] <= 1]["Variable"].tolist()
# Omit features with high VIF from the DataFrame
Rama_Sofi_Gold_List = Final_list_Rama_filtered.drop(low_sig_corrs, axis=1)
Rama_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 672 entries, 1 to 2242
Data columns (total 11 columns):
 #   Column                               Non-Null Count  Dtype  
---  ------                               --------------  -----  
 0   Rama_Educational_achievement_index   672 non-null    float64
 1   Rama_United_Commander_or_Kazin       672 non-null    float64
 2   Rama_Index_Criminal_History          672 non-null    float64
 3   Sacham_officer_Dico                  672 non-null    float64
 4   Married_Dico                         672 non-null    float64
 5   Drinking_Alcohol_Frequ_Num           672 non-null    float64
 6   Work_Perceived_Maching_Num           672 non-null    float64
 7   Rama_Temp_Mean_Num                   672 non-null    float64
 8   Rama_Job_Motivators_Index            672 non-null    float64
 9   Rama_Interests_and_Activities_Index  672 non-null    float64
 10  RAMA_Target                          672 non-null    float64
dtypes: float64(11)
memory usage: 63.0 KB
---- Dapar Grade ----
------------Feature Engineering-------------
Educational achievement index - Combine Variable
df['Dapar_Educational_achievement_index'] = df.Graduation_Average_85_Up_Dico + df.English_4_5_Units_Num_Dico + df.Math_4_5_Units_Num_Dico + df.Academic_Education_Dico + df.Psyc_Test_600_Up_Dico
class_counts_Dapar_Educational_achievement_index = df['Dapar_Educational_achievement_index'].value_counts()
###print(class_counts_Dapar_Educational_achievement_index)
Psych_Tests_Subjective_Num
class_counts_Psych_Tests_Subjective_Num = df['Psych_Tests_Subjective_Num'].value_counts()
###print(class_counts_Psych_Tests_Subjective_Num)
Kaba_Grade_51_Up_Dico
# Assuming your DataFrame is named df
# Create a new variable "Psyc_Test_600_Up" based on conditions
df["Kaba_Grade_51_Up_Dico"] = 0  # Default value is set to 0

# Update values based on conditions
df.loc[df["Kaba_Grade_Army_Num"].between(12, 18, inclusive=True), "Kaba_Grade_51_Up_Dico"] = 1
Kaba_Grade_51_54_Dico
class_counts_Kaba_Grade_51_54_Dico = df['Kaba_Grade_51_54_Dico'].value_counts()
###print(class_counts_Kaba_Grade_51_54_Dico)
Kaba_Grade_55_UP_Dico
class_counts_Kaba_Grade_55_UP_Dico = df['Kaba_Grade_55_UP_Dico'].value_counts()
###print(class_counts_Kaba_Grade_55_UP_Dico)
Married_Dico
class_counts_Married_Dico = df['Married_Dico'].value_counts()
###print(class_counts_Married_Dico)
Age_Num
class_counts_Age_Num = df['Age_Num'].value_counts()
###print(class_counts_Age_Num)
Special_Unit_Army_Dico
class_counts_Special_Unit_Army_Dico = df['Special_Unit_Army_Dico'].value_counts()
###print(class_counts_Special_Unit_Army_Dico)
Dapar_Hebrew_Failurer_Dico
class_counts_Dapar_Hebrew_Failurer_Dico = df['Dapar_Hebrew_Failurer_Last_Attempt_Dico'].value_counts()
###print(class_counts_Dapar_Hebrew_Failurer_Dico)
Job_Motivators_Index - Combine Variable
df['Dapar_Job_Motivators_Index'] = df.Personal_Development * 0.158877 + df.The_Action_In_Work * -0.134249 + df.Diversity_at_Work * 0.139854 + df.Assistance_to_Citizens *-0.105701 + df.Wearing_Uniform * -0.107838  + df.Crime_Fighting * 0.082 + df.Challenging_Interesting_Work * 0.098153  + df.Contribution_to_Society * 0.063  + df.Good_Salary * -0.063  + df.Convenient_Working_Hours * -0.081352 + df.Exercise_Authority * -0.054  + df.Use_Force * -0.098919
Interests_and_Activities_Index - Combine Variable
df['Dapar_Interests_and_Activities_Index'] = df.Science * 0.132 + df.Economy * 0.132 + df.Cooking * -0.1278 + df.Art * -0.105 + df.Actuavlia * 0.110 + df.Meeting_With_Friends * 0.076 + df.Psychology * 0.100 + df.Web_Surfing * 0.065 + df.Technology_and_Computers * 0.077
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Dapar_Gold_List = pd.DataFrame(df, columns=["Dapar_Educational_achievement_index", "Psych_Tests_Subjective_Num", "Kaba_Grade_51_54_Dico", "Kaba_Grade_55_UP_Dico", "Married_Dico", "Special_Unit_Army_Dico", "Dapar_Job_Motivators_Index", "Dapar_Interests_and_Activities_Index", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Dapar_Target"])
for col in Dapar_Gold_List:
    if col in Dapar_Gold_List.columns:
        Dapar_Gold_List[col] = Dapar_Gold_List[col].astype(np.float64)
Dapar_Gold_List[Dapar_Gold_List.columns[0:]].corr(method='spearman')['Dapar_Target'].sort_values(ascending=False)
Dapar_Target                               1.000000
Dapar_Educational_achievement_index        0.433444
Dapar_Job_Motivators_Index                 0.314597
Psych_Tests_Subjective_Num                 0.303248
Dapar_Interests_and_Activities_Index       0.284702
Kaba_Grade_51_54_Dico                      0.259420
Kaba_Grade_55_UP_Dico                      0.258947
Special_Unit_Army_Dico                     0.208293
Married_Dico                               0.121887
Dapar_Hebrew_Failurer_Last_Attempt_Dico   -0.157332
Name: Dapar_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Dapar_Linear = pd.DataFrame(Dapar_Gold_List, columns=["Dapar_Educational_achievement_index", "Psych_Tests_Subjective_Num", "Kaba_Grade_51_54_Dico", "Kaba_Grade_55_UP_Dico", "Married_Dico", "Special_Unit_Army_Dico", "Dapar_Job_Motivators_Index", "Dapar_Interests_and_Activities_Index", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Dapar_Target"])
Gold_List_Dapar_Linear_cleaned = Gold_List_Dapar_Linear.dropna()
x = Gold_List_Dapar_Linear_cleaned[Gold_List_Dapar_Linear_cleaned.columns[~Gold_List_Dapar_Linear_cleaned.columns.isin(["Dapar_Target"])]]
y = Gold_List_Dapar_Linear_cleaned['Dapar_Target']
x = sm.add_constant(x);

regr = linear_model.LinearRegression()
regr.fit(x, y);
x.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 471 entries, 6 to 2242
Data columns (total 10 columns):
 #   Column                                   Non-Null Count  Dtype  
---  ------                                   --------------  -----  
 0   const                                    471 non-null    float64
 1   Dapar_Educational_achievement_index      471 non-null    float64
 2   Psych_Tests_Subjective_Num               471 non-null    float64
 3   Kaba_Grade_51_54_Dico                    471 non-null    float64
 4   Kaba_Grade_55_UP_Dico                    471 non-null    float64
 5   Married_Dico                             471 non-null    float64
 6   Special_Unit_Army_Dico                   471 non-null    float64
 7   Dapar_Job_Motivators_Index               471 non-null    float64
 8   Dapar_Interests_and_Activities_Index     471 non-null    float64
 9   Dapar_Hebrew_Failurer_Last_Attempt_Dico  471 non-null    float64
dtypes: float64(10)
memory usage: 40.5 KB
model = sm.OLS(y, x).fit();

predictions = model.predict(x);
### See quality measurs of the model

print_model = model.summary().tables[0]
print(print_model)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:           Dapar_Target   R-squared:                       0.395
Model:                            OLS   Adj. R-squared:                  0.383
Method:                 Least Squares   F-statistic:                     33.44
Date:                Tue, 09 Apr 2024   Prob (F-statistic):           3.30e-45
Time:                        16:16:34   Log-Likelihood:                -825.27
No. Observations:                 471   AIC:                             1671.
Df Residuals:                     461   BIC:                             1712.
Df Model:                           9                                         
Covariance Type:            nonrobust                                         
==============================================================================
### See quality measurs of the model

print_model = model.summary().tables[1]
print(print_model)
===========================================================================================================
                                              coef    std err          t      P>|t|      [0.025      0.975]
-----------------------------------------------------------------------------------------------------------
const                                       2.7173      0.266     10.222      0.000       2.195       3.240
Dapar_Educational_achievement_index         0.3804      0.060      6.335      0.000       0.262       0.498
Psych_Tests_Subjective_Num                  0.2353      0.066      3.548      0.000       0.105       0.366
Kaba_Grade_51_54_Dico                       0.8986      0.190      4.728      0.000       0.525       1.272
Kaba_Grade_55_UP_Dico                       0.8341      0.229      3.642      0.000       0.384       1.284
Married_Dico                                0.6008      0.172      3.487      0.001       0.262       0.939
Special_Unit_Army_Dico                      0.6079      0.252      2.416      0.016       0.113       1.102
Dapar_Job_Motivators_Index                  2.7605      0.415      6.647      0.000       1.944       3.577
Dapar_Interests_and_Activities_Index        1.1880      0.466      2.548      0.011       0.272       2.104
Dapar_Hebrew_Failurer_Last_Attempt_Dico    -0.5644      0.239     -2.361      0.019      -1.034      -0.095
===========================================================================================================
x = sm.add_constant(x)

vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
vif["features"] = x.columns
print(vif.round(1))
   VIF Factor                                 features
0        16.7                                    const
1         1.2      Dapar_Educational_achievement_index
2         1.1               Psych_Tests_Subjective_Num
3         1.2                    Kaba_Grade_51_54_Dico
4         1.2                    Kaba_Grade_55_UP_Dico
5         1.0                             Married_Dico
6         1.1                   Special_Unit_Army_Dico
7         1.0               Dapar_Job_Motivators_Index
8         1.1     Dapar_Interests_and_Activities_Index
9         1.0  Dapar_Hebrew_Failurer_Last_Attempt_Dico
Upon evaluation, it was determined that the VIF index for the three weighted personality scales is less than 0.5. Consequently, there exists no concern regarding multicollinearity issues among these weighted factors.
Stepwise Linear Reggression
### Separate predictors and target variable
#X = Gold_List_Dapar_Linear_cleaned.drop("Dapar_Target", axis=1)
#y = Gold_List_Dapar_Linear_cleaned["Dapar_Target"]
### Forward stepwise feature selection using Linear Regression as base model
#lr = LinearRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="r2",
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = sm.OLS(y, sm.add_constant(X_selected)).fit()

### Summary of the regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
### Display regression report using statsmodels
#X_selected = sm.add_constant(X_selected)  # Add constant term for intercept
#model_stats = sm.OLS(y, X_selected).fit()

### Print regression report
#print(model_stats.summary())
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Dapar_Z_Scores = pd.DataFrame(Gold_List_Dapar_Linear_cleaned, columns=["Dapar_Educational_achievement_index", "Psych_Tests_Subjective_Num", "Kaba_Grade_51_54_Dico", "Kaba_Grade_55_UP_Dico", "Married_Dico", "Special_Unit_Army_Dico", "Dapar_Job_Motivators_Index", "Dapar_Interests_and_Activities_Index", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Dapar_Target"])
descriptive_stats_Dapar = Gold_List_Dapar_Z_Scores.describe()
###descriptive_stats_Dapar
###import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Dapar_Target"]
#numerical_columns = Gold_List_Dapar_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Dapar_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Dapar_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Dapar_Z_Scores[numerical_columns] = (Gold_List_Dapar_Z_Scores[numerical_columns] - Gold_List_Dapar_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Dapar_Z_Scores[numerical_columns].max() - Gold_List_Dapar_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Dapar_Z_Scores[numerical_columns] = Gold_List_Dapar_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Dapar_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
### Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "Dapar_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Dapar_Linear_cleaned.columns:
#    Gold_List_Dapar_Linear_cleaned[column] = Gold_List_Dapar_Linear_cleaned[column]  / Gold_List_Dapar_Linear_cleaned[column].abs().max()
#Gold_List_Dapar_Linear_cleaned[Gold_List_Dapar_Linear_cleaned.columns[1:]].corr()['Dapar_Target'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
#Gold_List_Dapar_Z_Scores[Gold_List_Dapar_Z_Scores.columns[0:]].corr(method='spearman')['Dapar_Target'].sort_values(ascending=False)
#Gold_List_Dapar_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Dapar_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Dapar_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Dapar_Target']).round(2)
1.8 Creating Final List
# Create the new DataFrame 'Final_list'
Final_list_Dapar = pd.DataFrame(Gold_List_Dapar_Linear_cleaned)
# Drop rows with missing values
Final_list_Dapar = Final_list_Dapar.dropna()
for col in Final_list_Dapar:
    if col in Final_list_Dapar.columns:
        Final_list_Dapar[col] = Final_list_Dapar[col].astype(np.float64)
------------Feature Selection - Final Gibushon Grade------------
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
df_num_corr = pd.DataFrame(Final_list_Dapar)
corr = Final_list_Dapar.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Dapar
#for i in Final_list_Dapar.columns:
#        for j in Final_list_Dapar.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Dapar[i]) & ~pd.isna(Final_list_Dapar[j]) 
#                    a = stats.spearmanr(Final_list_Dapar[i][mask], Final_list_Dapar[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import linear_model

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Dapar.drop('Dapar_Target', axis=1)
y = Final_list_Dapar ['Dapar_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the linear regression model
regr = linear_model.LinearRegression()
regr.fit(x, y)

# Create the model
model = sm.OLS(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:           Dapar_Target   R-squared:                       0.395
Model:                            OLS   Adj. R-squared:                  0.383
Method:                 Least Squares   F-statistic:                     33.44
Date:                Tue, 09 Apr 2024   Prob (F-statistic):           3.30e-45
Time:                        16:16:34   Log-Likelihood:                -825.27
No. Observations:                 471   AIC:                             1671.
Df Residuals:                     461   BIC:                             1712.
Df Model:                           9                                         
Covariance Type:            nonrobust                                         
===========================================================================================================
                                              coef    std err          t      P>|t|      [0.025      0.975]
-----------------------------------------------------------------------------------------------------------
const                                       2.7173      0.266     10.222      0.000       2.195       3.240
Dapar_Educational_achievement_index         0.3804      0.060      6.335      0.000       0.262       0.498
Psych_Tests_Subjective_Num                  0.2353      0.066      3.548      0.000       0.105       0.366
Kaba_Grade_51_54_Dico                       0.8986      0.190      4.728      0.000       0.525       1.272
Kaba_Grade_55_UP_Dico                       0.8341      0.229      3.642      0.000       0.384       1.284
Married_Dico                                0.6008      0.172      3.487      0.001       0.262       0.939
Special_Unit_Army_Dico                      0.6079      0.252      2.416      0.016       0.113       1.102
Dapar_Job_Motivators_Index                  2.7605      0.415      6.647      0.000       1.944       3.577
Dapar_Interests_and_Activities_Index        1.1880      0.466      2.548      0.011       0.272       2.104
Dapar_Hebrew_Failurer_Last_Attempt_Dico    -0.5644      0.239     -2.361      0.019      -1.034      -0.095
==============================================================================
Omnibus:                        4.098   Durbin-Watson:                   1.842
Prob(Omnibus):                  0.129   Jarque-Bera (JB):                3.486
Skew:                          -0.123   Prob(JB):                        0.175
Kurtosis:                       2.657   Cond. No.                         31.1
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
   VIF Factor                                  Feature
0    1.249840      Dapar_Educational_achievement_index
1    1.117427               Psych_Tests_Subjective_Num
2    1.157241                    Kaba_Grade_51_54_Dico
3    1.219408                    Kaba_Grade_55_UP_Dico
4    1.029982                             Married_Dico
5    1.058085                   Special_Unit_Army_Dico
6    1.041781               Dapar_Job_Motivators_Index
7    1.104312     Dapar_Interests_and_Activities_Index
8    1.027594  Dapar_Hebrew_Failurer_Last_Attempt_Dico
In the above table we can see 0 independent variables with multicollinear relations.
In several tests, it was observed that assigning weights to the multicollinear variables did not significantly enhance the percentage of explained variance in the target factor. As a result, in the upcoming section, we will exclude the multicollinear variables. This approach ensures that within the dataset, only one variable remains among those exhibiting multicollinear relationships.
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"] >= 5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Dapar_filtered = Final_list_Dapar.drop(high_vif_features, axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Dapar_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (471, 10)
Final_list_Dapar = Final_list_Dapar_filtered.astype({"Dapar_Target":'category'})
#Final_list_Dapar.info()
1.5 The Feature Selection Process
No_colinear_list_Dapar = Final_list_Dapar.astype({"Dapar_Target":'category'})
No_colinear_list_Dapar = pd.DataFrame(No_colinear_list_Dapar)
temp_cols=No_colinear_list_Dapar.columns.tolist()
index=No_colinear_list_Dapar.columns.get_loc("Dapar_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Dapar=No_colinear_list_Dapar[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Dapar.columns[1:40]})
varSel.head(50)
                                  Variable
0      Dapar_Educational_achievement_index
1               Psych_Tests_Subjective_Num
2                    Kaba_Grade_51_54_Dico
3                    Kaba_Grade_55_UP_Dico
4                             Married_Dico
5                   Special_Unit_Army_Dico
6               Dapar_Job_Motivators_Index
7     Dapar_Interests_and_Activities_Index
8  Dapar_Hebrew_Failurer_Last_Attempt_Dico
nm = No_colinear_list_Dapar.columns[1:40]
nm = nm.append(pd.Index(['Dapar_Target']))
#nm
df2_Dapar = No_colinear_list_Dapar[nm].copy()
#df2_Dapar.head(50)
5.2 Multivariable Analysis
No_colinear_list_Dapar = No_colinear_list_Dapar.dropna()
X = No_colinear_list_Dapar[No_colinear_list_Dapar.columns[~No_colinear_list_Dapar.columns.isin(['Dapar_Target'])]]
y = No_colinear_list_Dapar['Dapar_Target']
print([X.shape,y.shape])
[(471, 9), (471,)]
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import Ridge

ridge_mod = Ridge().fit(X, y)
model = SelectFromModel(ridge_mod, prefit=True)
varSel['Ridge'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import ElasticNet

elastic_net_mod = ElasticNet().fit(X, y)
model = SelectFromModel(elastic_net_mod, prefit=True)
varSel['ElasticNet'] = model.get_support().astype('int64')

###varSel
from sklearn.tree import DecisionTreeRegressor

dt_mod = DecisionTreeRegressor().fit(X, y)
model = SelectFromModel(dt_mod, prefit=True)
varSel['DecisionTree'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
import pandas as pd
import statsmodels.api as sm

# Assuming X and y are already defined and varSel is initialized

# Fit an OLS regression model
ols_model = sm.OLS(y, X).fit()

# Get p-values for each feature
p_values = ols_model.pvalues

# Perform feature selection based on a significance level (e.g., 0.05)
selected_features = p_values[p_values < 0.05].index

# Store the selected features in varSel
varSel['OLS'] = [1 if feature in selected_features else 0 for feature in X.columns]

# Print model summary
#print(ols_model.summary())

###varSel
from sklearn.linear_model import LinearRegression

linear_mod = LinearRegression().fit(X, y)
model = SelectFromModel(linear_mod, prefit=True)
varSel['LinearRegression'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

rf_mod = RandomForestRegressor().fit(X, y)
model = SelectFromModel(rf_mod, prefit=True)
varSel['RandomForest'] = model.get_support().astype('int64')

RandomForestRegressor

###varSel
sklearn.ensemble._forest.RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
#varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
###varSel
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor().fit(X, y_transformed)

model = SelectFromModel(SGD_Regressor, prefit=True)
model.get_support()

varSel['SGD'] = model.get_support().astype('int64')
#varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
varSel
                                  Variable  Lasso  Ridge  ElasticNet  \
0      Dapar_Educational_achievement_index      0      0           1   
1               Psych_Tests_Subjective_Num      0      0           0   
2                    Kaba_Grade_51_54_Dico      0      1           0   
3                    Kaba_Grade_55_UP_Dico      0      0           0   
4                             Married_Dico      0      0           0   
5                   Special_Unit_Army_Dico      0      0           0   
6               Dapar_Job_Motivators_Index      0      1           0   
7     Dapar_Interests_and_Activities_Index      0      1           0   
8  Dapar_Hebrew_Failurer_Last_Attempt_Dico      0      0           0   

   DecisionTree  GradientBoosting  OLS  LinearRegression  RandomForest  \
0             1                 1    1                 0             1   
1             0                 0    1                 0             0   
2             0                 0    1                 1             0   
3             0                 0    1                 0             0   
4             0                 0    1                 0             0   
5             0                 0    1                 0             0   
6             1                 1    1                 1             1   
7             1                 1    1                 1             1   
8             0                 0    0                 0             0   

   AdaBoost  SVM  SGD  Sum  
0         1    1    0    7  
1         0    1    0    2  
2         0    0    1    4  
3         0    0    0    1  
4         0    0    1    2  
5         0    0    0    1  
6         1    0    1    8  
7         0    0    0    6  
8         0    0    0    0  
#varSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_List_Dapar_Feature_Selection = varSel[varSel['Sum']>=0]
Final_List_Dapar_Feature_Selection 
                                  Variable  Lasso  Ridge  ElasticNet  \
0      Dapar_Educational_achievement_index      0      0           1   
1               Psych_Tests_Subjective_Num      0      0           0   
2                    Kaba_Grade_51_54_Dico      0      1           0   
3                    Kaba_Grade_55_UP_Dico      0      0           0   
4                             Married_Dico      0      0           0   
5                   Special_Unit_Army_Dico      0      0           0   
6               Dapar_Job_Motivators_Index      0      1           0   
7     Dapar_Interests_and_Activities_Index      0      1           0   
8  Dapar_Hebrew_Failurer_Last_Attempt_Dico      0      0           0   

   DecisionTree  GradientBoosting  OLS  LinearRegression  RandomForest  \
0             1                 1    1                 0             1   
1             0                 0    1                 0             0   
2             0                 0    1                 1             0   
3             0                 0    1                 0             0   
4             0                 0    1                 0             0   
5             0                 0    1                 0             0   
6             1                 1    1                 1             1   
7             1                 1    1                 1             1   
8             0                 0    0                 0             0   

   AdaBoost  SVM  SGD  Sum  
0         1    1    0    7  
1         0    1    0    2  
2         0    0    1    4  
3         0    0    0    1  
4         0    0    1    2  
5         0    0    0    1  
6         1    0    1    8  
7         0    0    0    6  
8         0    0    0    0  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"] <= 0]["Variable"].tolist()

# Omit features with high VIF from the DataFrame
Dapar_Sofi_Gold_List = Final_list_Dapar_filtered.drop(low_sig_corrs, axis=1)
Dapar_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 471 entries, 6 to 2242
Data columns (total 9 columns):
 #   Column                                Non-Null Count  Dtype  
---  ------                                --------------  -----  
 0   Dapar_Educational_achievement_index   471 non-null    float64
 1   Psych_Tests_Subjective_Num            471 non-null    float64
 2   Kaba_Grade_51_54_Dico                 471 non-null    float64
 3   Kaba_Grade_55_UP_Dico                 471 non-null    float64
 4   Married_Dico                          471 non-null    float64
 5   Special_Unit_Army_Dico                471 non-null    float64
 6   Dapar_Job_Motivators_Index            471 non-null    float64
 7   Dapar_Interests_and_Activities_Index  471 non-null    float64
 8   Dapar_Target                          471 non-null    float64
dtypes: float64(9)
memory usage: 36.8 KB
---- Hebrew Grade ----
------------Feature Engineering-------------
Educational achievement index - Combine Variable
df['Heb_Educational_achievement_index'] = df.Graduation_Average_85_Up_Dico + df.English_4_5_Units_Num_Dico + df.Math_4_5_Units_Num_Dico + df.Academic_Education_Dico + df.Psyc_Test_600_Up_Dico
class_counts_Heb_Educational_achievement_index = df['Heb_Educational_achievement_index'].value_counts()
###print(class_counts_Heb_Educational_achievement_index)
Hebrew_Meam_Num
class_counts_Hebrew_Meam_Num = df['Hebrew_Meam_Num'].value_counts()
###print(class_counts_Hebrew_Meam_Num)
Cigarettes_Dico
class_counts_Cigarettes_Dico = df['Cigarettes_Dico'].value_counts()
###print(class_counts_Cigarettes_Dico)
Service_Period_Commitment_Dico
class_counts_Service_Period_Commitment_Dico = df['Service_Period_Commitment_Dico'].value_counts()
###print(class_counts_Service_Period_Commitment_Dico)
Married_Dico
class_counts_Married_Dico = df['Married_Dico'].value_counts()
###print(class_counts_Married_Dico)
Age_Num
class_counts_Age_Num = df['Age_Num'].value_counts()
###print(class_counts_Age_Num)
Sacham_officer_Dico
class_counts_Sacham_officer_Dico = df['Sacham_officer_Dico'].value_counts()
###print(class_counts_Sacham_officer_Dico)
Dapar_Hebrew_Failurer_Dico
class_counts_Dapar_Hebrew_Failurer_Dico = df['Dapar_Hebrew_Failurer_Last_Attempt_Dico'].value_counts()
###print(class_counts_Dapar_Hebrew_Failurer_Dico)
Evaluation_Center_Filed_Dico
class_counts_Evaluation_Center_Filed_Dico = df['Evaluation_Center_Filed_Last_Attempt_Dico'].value_counts()
###print(class_counts_Evaluation_Center_Filed_Dico)
Max_Procedure_Duration_Num
class_counts_Max_Procedure_Duration_Num = df['Max_Procedure_Duration_Num'].value_counts()
###print(class_counts_Max_Procedure_Duration_Num)
Drinking_Alcohol_Frequ_Num
class_counts_Drinking_Alcohol_Frequ_Num = df['Drinking_Alcohol_Frequ_Num'].value_counts()
###print(class_counts_Drinking_Alcohol_Frequ_Num)
Work_Perceived_Maching_Num
class_counts_Work_Perceived_Maching_Num = df['Work_Perceived_Maching_Num'].value_counts()
###print(class_counts_Work_Perceived_Maching_Num)
Job_Motivators_Index - Combine Variable
df['Heb_Job_Motivators_Index'] = df.Personal_Development * 0.206 + df.Convenient_Working_Hours * -0.199 + df.Wearing_Uniform * -0.178 + df.Good_Salary *-0.176 + df.The_Action_In_Work * -0.170  + df.Contribution_to_Society * 0.151 + df.Use_Force * -0.150  + df.Interaction_With_People * 0.125  + df.Challenging_Interesting_Work * 0.119  + df.Diversity_at_Work * 0.114 + df.Exercise_Authority * -0.054  + df.Exercise_Authority * -0.108
Interests_and_Activities_Index - Combine Variable
df['Heb_Interests_and_Activities_Index'] = df.Actuavlia * 0.24 + df.Politics * 0.175 + df.Sport * -0.128 + df.Literature * 0.109 + df.Meeting_With_Friends * 0.095 + df.Art * -0.093 + df.Psychology * 0.068 + df.Enjoy_Parties_Bars * -0.065 + df.Extreme_Sports * -0.077 + df.Gambling * -0.081
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Heb_Gold_List = pd.DataFrame(df, columns=["Heb_Educational_achievement_index", "Hebrew_Meam_Num", "Cigarettes_Dico", "Service_Period_Commitment_Dico", "Married_Dico", "Age_Num", "Sacham_officer_Dico", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Evaluation_Center_Filed_Last_Attempt_Dico", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Heb_Job_Motivators_Index", "Heb_Interests_and_Activities_Index", "Hebrew_Target"])
for col in Heb_Gold_List:
    if col in Heb_Gold_List.columns:
        Heb_Gold_List[col] = Heb_Gold_List[col].astype(np.float64)
Heb_Gold_List[Heb_Gold_List.columns[0:]].corr(method='spearman')['Hebrew_Target'].sort_values(ascending=False)
Hebrew_Target                                1.000000
Heb_Educational_achievement_index            0.422768
Heb_Job_Motivators_Index                     0.382969
Hebrew_Meam_Num                              0.316215
Heb_Interests_and_Activities_Index           0.307469
Age_Num                                      0.217967
Service_Period_Commitment_Dico               0.175387
Married_Dico                                 0.160063
Max_Procedure_Duration_Num                   0.093421
Drinking_Alcohol_Frequ_Num                   0.086492
Work_Perceived_Maching_Num                   0.062530
Evaluation_Center_Filed_Last_Attempt_Dico   -0.134982
Cigarettes_Dico                             -0.176958
Sacham_officer_Dico                         -0.181378
Dapar_Hebrew_Failurer_Last_Attempt_Dico     -0.187837
Name: Hebrew_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Heb_Linear = pd.DataFrame(Heb_Gold_List, columns=["Heb_Educational_achievement_index", "Hebrew_Meam_Num", "Cigarettes_Dico", "Service_Period_Commitment_Dico", "Married_Dico", "Age_Num", "Sacham_officer_Dico", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Evaluation_Center_Filed_Last_Attempt_Dico", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Heb_Job_Motivators_Index", "Heb_Interests_and_Activities_Index", "Hebrew_Target"])
Gold_List_Heb_Linear_cleaned = Gold_List_Heb_Linear.dropna()
#x = Gold_List_Heb_Linear_cleaned[Gold_List_Heb_Linear_cleaned.columns[~Gold_List_Heb_Linear_cleaned.columns.isin(["Hebrew_Target"])]]
#y = Gold_List_Heb_Linear_cleaned['Hebrew_Target']
#x = sm.add_constant(x);

#regr = linear_model.LinearRegression()
#regr.fit(x, y);
#x.info()
#model = sm.OLS(y, x).fit();

#predictions = model.predict(x);
### See quality measurs of the model

#print_model = model.summary().tables[0]
#print(print_model)
### See quality measurs of the model

#print_model = model.summary().tables[1]
#print(print_model)
#x = sm.add_constant(x)

#vif = pd.DataFrame()
#vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
#vif["features"] = x.columns
#print(vif.round(1))
Upon evaluation, it was determined that the VIF index for the three weighted personality scales is less than 0.5. Consequently, there exists no concern regarding multicollinearity issues among these weighted factors.
Stepwise Linear Reggression
### Separate predictors and target variable
#X = Gold_List_Heb_Linear_cleaned.drop("Hebrew_Target", axis=1)
#y = Gold_List_Heb_Linear_cleaned["Hebrew_Target"]
### Forward stepwise feature selection using Linear Regression as base model
#lr = LinearRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="r2",
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = sm.OLS(y, sm.add_constant(X_selected)).fit()

### Summary of the regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
### Display regression report using statsmodels
#X_selected = sm.add_constant(X_selected)  # Add constant term for intercept
#model_stats = sm.OLS(y, X_selected).fit()

### Print regression report
#print(model_stats.summary())
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Heb_Z_Scores = pd.DataFrame(Gold_List_Heb_Linear_cleaned, columns=["Heb_Educational_achievement_index", "Hebrew_Meam_Num", "Cigarettes_Dico", "Service_Period_Commitment_Dico", "Married_Dico", "Age_Num", "Sacham_officer_Dico", "Dapar_Hebrew_Failurer_Last_Attempt_Dico", "Evaluation_Center_Filed_Last_Attempt_Dico", "Max_Procedure_Duration_Num", "Drinking_Alcohol_Frequ_Num", "Work_Perceived_Maching_Num", "Heb_Job_Motivators_Index", "Heb_Interests_and_Activities_Index", "Hebrew_Target"])
descriptive_stats_Heb = Gold_List_Heb_Z_Scores.describe()
###descriptive_stats_Heb
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Hebrew_Target"]
#numerical_columns = Gold_List_Heb_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Heb_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Heb_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Heb_Z_Scores[numerical_columns] = (Gold_List_Heb_Z_Scores[numerical_columns] - Gold_List_Heb_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Heb_Z_Scores[numerical_columns].max() - Gold_List_Heb_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Heb_Z_Scores[numerical_columns] = Gold_List_Heb_Z_Scores[numerical_columns].round(2)

# Print the DataFrame after transformation
###print(Gold_List_Heb_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
### Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "Hebrew_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Heb_Linear_cleaned.columns:
#    Gold_List_Heb_Linear_cleaned[column] = Gold_List_Heb_Linear_cleaned[column]  / Gold_List_Heb_Linear_cleaned[column].abs().max()
#Gold_List_Heb_Linear_cleaned[Gold_List_Heb_Linear_cleaned.columns[1:]].corr()['Hebrew_Target'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
Gold_List_Heb_Z_Scores[Gold_List_Heb_Z_Scores.columns[0:]].corr(method='spearman')['Hebrew_Target'].sort_values(ascending=False)
Hebrew_Target                                1.000000
Heb_Educational_achievement_index            0.422768
Heb_Job_Motivators_Index                     0.382969
Hebrew_Meam_Num                              0.316215
Heb_Interests_and_Activities_Index           0.307469
Age_Num                                      0.217967
Service_Period_Commitment_Dico               0.175387
Married_Dico                                 0.160063
Max_Procedure_Duration_Num                   0.093421
Drinking_Alcohol_Frequ_Num                   0.086492
Work_Perceived_Maching_Num                   0.062530
Evaluation_Center_Filed_Last_Attempt_Dico   -0.134982
Cigarettes_Dico                             -0.176958
Sacham_officer_Dico                         -0.181378
Dapar_Hebrew_Failurer_Last_Attempt_Dico     -0.187837
Name: Hebrew_Target, dtype: float64
#Gold_List_Heb_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Heb_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Heb_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Hebrew_Target']).round(2)
1.8 Creating Final List
### Create the new DataFrame 'Final_list'
Final_list_Heb = pd.DataFrame(Gold_List_Heb_Linear_cleaned)
### Drop rows with missing values
Final_list_Heb = Final_list_Heb.dropna()
for col in Final_list_Heb:
    if col in Final_list_Heb.columns:
        Final_list_Heb[col] = Final_list_Heb[col].astype(np.float64)
------------Feature Selection - Final Gibushon Grade------------
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
df_num_corr = pd.DataFrame(Final_list_Heb)
corr = Final_list_Heb.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Heb
#for i in Final_list_Heb.columns:
#        for j in Final_list_Heb.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Heb[i]) & ~pd.isna(Final_list_Heb[j]) 
#                    a = stats.spearmanr(Final_list_Heb[i][mask], Final_list_Heb[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import linear_model

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Heb.drop('Hebrew_Target', axis=1)
y = Final_list_Heb['Hebrew_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the linear regression model
regr = linear_model.LinearRegression()
regr.fit(x, y)

# Create the model
model = sm.OLS(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:          Hebrew_Target   R-squared:                       0.436
Model:                            OLS   Adj. R-squared:                  0.419
Method:                 Least Squares   F-statistic:                     25.71
Date:                Tue, 09 Apr 2024   Prob (F-statistic):           2.21e-49
Time:                        16:16:53   Log-Likelihood:                -524.50
No. Observations:                 480   AIC:                             1079.
Df Residuals:                     465   BIC:                             1142.
Df Model:                          14                                         
Covariance Type:            nonrobust                                         
=============================================================================================================
                                                coef    std err          t      P>|t|      [0.025      0.975]
-------------------------------------------------------------------------------------------------------------
const                                         4.4886      0.392     11.446      0.000       3.718       5.259
Heb_Educational_achievement_index             0.1358      0.029      4.632      0.000       0.078       0.193
Hebrew_Meam_Num                               0.2089      0.044      4.792      0.000       0.123       0.295
Cigarettes_Dico                              -0.2538      0.074     -3.414      0.001      -0.400      -0.108
Service_Period_Commitment_Dico                0.6194      0.165      3.755      0.000       0.295       0.944
Married_Dico                                  0.1798      0.098      1.838      0.067      -0.012       0.372
Age_Num                                       0.0095      0.008      1.259      0.209      -0.005       0.024
Sacham_officer_Dico                          -0.4118      0.123     -3.354      0.001      -0.653      -0.171
Dapar_Hebrew_Failurer_Last_Attempt_Dico      -0.5072      0.161     -3.154      0.002      -0.823      -0.191
Evaluation_Center_Filed_Last_Attempt_Dico    -1.0374      0.283     -3.666      0.000      -1.594      -0.481
Max_Procedure_Duration_Num                    0.0264      0.018      1.492      0.136      -0.008       0.061
Drinking_Alcohol_Frequ_Num                    0.0879      0.039      2.263      0.024       0.012       0.164
Work_Perceived_Maching_Num                    0.0545      0.044      1.242      0.215      -0.032       0.141
Heb_Job_Motivators_Index                      0.9186      0.153      6.009      0.000       0.618       1.219
Heb_Interests_and_Activities_Index            0.8406      0.216      3.886      0.000       0.415       1.266
==============================================================================
Omnibus:                        2.770   Durbin-Watson:                   2.017
Prob(Omnibus):                  0.250   Jarque-Bera (JB):                2.575
Skew:                          -0.134   Prob(JB):                        0.276
Kurtosis:                       3.238   Cond. No.                         324.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                                    Feature
0     1.311456          Heb_Educational_achievement_index
1     1.250090                            Hebrew_Meam_Num
2     1.112859                            Cigarettes_Dico
3     1.062829             Service_Period_Commitment_Dico
4     1.345283                               Married_Dico
5     1.367961                                    Age_Num
6     1.028208                        Sacham_officer_Dico
7     1.053740    Dapar_Hebrew_Failurer_Last_Attempt_Dico
8     1.027599  Evaluation_Center_Filed_Last_Attempt_Dico
9     1.026552                 Max_Procedure_Duration_Num
10    1.124304                 Drinking_Alcohol_Frequ_Num
11    1.090848                 Work_Perceived_Maching_Num
12    1.206529                   Heb_Job_Motivators_Index
13    1.169868         Heb_Interests_and_Activities_Index
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"] >= 5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Heb_filtered = Final_list_Heb.drop(high_vif_features, axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Heb_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (480, 15)
In the above table we can see 0 independent variables with multicollinear relations.
In several tests, it was observed that assigning weights to the multicollinear variables did not significantly enhance the percentage of explained variance in the target factor. As a result, in the upcoming section, we will exclude the multicollinear variables. This approach ensures that within the dataset, only one variable remains among those exhibiting multicollinear relationships.
Final_list_Heb = Final_list_Heb_filtered.astype({"Hebrew_Target":'category'})
Final_list_Heb.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 480 entries, 6 to 2242
Data columns (total 15 columns):
 #   Column                                     Non-Null Count  Dtype   
---  ------                                     --------------  -----   
 0   Heb_Educational_achievement_index          480 non-null    float64 
 1   Hebrew_Meam_Num                            480 non-null    float64 
 2   Cigarettes_Dico                            480 non-null    float64 
 3   Service_Period_Commitment_Dico             480 non-null    float64 
 4   Married_Dico                               480 non-null    float64 
 5   Age_Num                                    480 non-null    float64 
 6   Sacham_officer_Dico                        480 non-null    float64 
 7   Dapar_Hebrew_Failurer_Last_Attempt_Dico    480 non-null    float64 
 8   Evaluation_Center_Filed_Last_Attempt_Dico  480 non-null    float64 
 9   Max_Procedure_Duration_Num                 480 non-null    float64 
 10  Drinking_Alcohol_Frequ_Num                 480 non-null    float64 
 11  Work_Perceived_Maching_Num                 480 non-null    float64 
 12  Heb_Job_Motivators_Index                   480 non-null    float64 
 13  Heb_Interests_and_Activities_Index         480 non-null    float64 
 14  Hebrew_Target                              480 non-null    category
dtypes: category(1), float64(14)
memory usage: 56.9 KB
1.5 The Feature Selection Process
No_colinear_list_Heb = Final_list_Heb.astype({"Hebrew_Target":'category'})
No_colinear_list_Heb = pd.DataFrame(No_colinear_list_Heb)
temp_cols=No_colinear_list_Heb.columns.tolist()
index=No_colinear_list_Heb.columns.get_loc("Hebrew_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Heb=No_colinear_list_Heb[new_cols]
No_colinear_list_Heb.head()
   Hebrew_Target  Heb_Educational_achievement_index  Hebrew_Meam_Num  \
6            6.0                                2.0              6.6   
8            8.0                                0.0              4.8   
9            7.0                                0.0              5.0   
11           6.0                                0.0              3.2   
12           7.0                                0.0              4.0   

    Cigarettes_Dico  Service_Period_Commitment_Dico  Married_Dico  Age_Num  \
6               1.0                             1.0           0.0     23.0   
8               1.0                             1.0           0.0     22.0   
9               0.0                             1.0           0.0     22.0   
11              0.0                             0.0           0.0     24.0   
12              1.0                             1.0           1.0     24.0   

    Sacham_officer_Dico  Dapar_Hebrew_Failurer_Last_Attempt_Dico  \
6                   0.0                                      0.0   
8                   0.0                                      0.0   
9                   0.0                                      0.0   
11                  0.0                                      0.0   
12                  0.0                                      0.0   

    Evaluation_Center_Filed_Last_Attempt_Dico  Max_Procedure_Duration_Num  \
6                                         0.0                         1.0   
8                                         0.0                         3.0   
9                                         0.0                         1.0   
11                                        0.0                         1.0   
12                                        0.0                         3.0   

    Drinking_Alcohol_Frequ_Num  Work_Perceived_Maching_Num  \
6                          3.0                         7.0   
8                          3.0                         6.0   
9                          3.0                         6.0   
11                         3.0                         5.0   
12                         4.0                         6.0   

    Heb_Job_Motivators_Index  Heb_Interests_and_Activities_Index  
6                     -0.312                              -0.205  
8                      0.155                               0.030  
9                      0.270                               0.049  
11                    -0.076                              -0.033  
12                     0.100                              -0.175  
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Heb.columns[1:40]})
varSel.head(50)
                                     Variable
0           Heb_Educational_achievement_index
1                             Hebrew_Meam_Num
2                             Cigarettes_Dico
3              Service_Period_Commitment_Dico
4                                Married_Dico
5                                     Age_Num
6                         Sacham_officer_Dico
7     Dapar_Hebrew_Failurer_Last_Attempt_Dico
8   Evaluation_Center_Filed_Last_Attempt_Dico
9                  Max_Procedure_Duration_Num
10                 Drinking_Alcohol_Frequ_Num
11                 Work_Perceived_Maching_Num
12                   Heb_Job_Motivators_Index
13         Heb_Interests_and_Activities_Index
nm = No_colinear_list_Heb.columns[1:40]
nm = nm.append(pd.Index(['Hebrew_Target']))
#nm
df2_Heb = No_colinear_list_Heb[nm].copy()
#df2_Heb.head(50)
5.2 Multivariable Analysis
No_colinear_list_Heb = No_colinear_list_Heb.dropna()
X = No_colinear_list_Heb[No_colinear_list_Heb.columns[~No_colinear_list_Heb.columns.isin(['Hebrew_Target'])]]
y = No_colinear_list_Heb['Hebrew_Target']
print([X.shape,y.shape])
[(480, 14), (480,)]
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import Ridge

ridge_mod = Ridge().fit(X, y)
model = SelectFromModel(ridge_mod, prefit=True)
varSel['Ridge'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import ElasticNet

elastic_net_mod = ElasticNet().fit(X, y)
model = SelectFromModel(elastic_net_mod, prefit=True)
varSel['ElasticNet'] = model.get_support().astype('int64')

###varSel
from sklearn.tree import DecisionTreeRegressor

dt_mod = DecisionTreeRegressor().fit(X, y)
model = SelectFromModel(dt_mod, prefit=True)
varSel['DecisionTree'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
import pandas as pd
import statsmodels.api as sm

# Assuming X and y are already defined and varSel is initialized

# Fit an OLS regression model
ols_model = sm.OLS(y, X).fit()

# Get p-values for each feature
p_values = ols_model.pvalues

# Perform feature selection based on a significance level (e.g., 0.05)
selected_features = p_values[p_values < 0.05].index

# Store the selected features in varSel
varSel['OLS'] = [1 if feature in selected_features else 0 for feature in X.columns]

# Print model summary
#print(ols_model.summary())

###varSel
from sklearn.linear_model import LinearRegression

linear_mod = LinearRegression().fit(X, y)
model = SelectFromModel(linear_mod, prefit=True)
varSel['LinearRegression'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

rf_mod = RandomForestRegressor().fit(X, y)
model = SelectFromModel(rf_mod, prefit=True)
varSel['RandomForest'] = model.get_support().astype('int64')

RandomForestRegressor

###varSel
sklearn.ensemble._forest.RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
###varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
###varSel
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor().fit(X, y_transformed)

model = SelectFromModel(SGD_Regressor, prefit=True)
model.get_support()

varSel['SGD'] = model.get_support().astype('int64')
###varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
#varSel
#varSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_list_Heb_Feature_Selection = varSel[varSel['Sum']>=2]
Final_list_Heb_Feature_Selection 
                                     Variable  Lasso  Ridge  ElasticNet  \
0           Heb_Educational_achievement_index      0      0           0   
1                             Hebrew_Meam_Num      0      0           0   
3              Service_Period_Commitment_Dico      0      1           0   
5                                     Age_Num      0      0           1   
6                         Sacham_officer_Dico      0      1           0   
7     Dapar_Hebrew_Failurer_Last_Attempt_Dico      0      1           0   
8   Evaluation_Center_Filed_Last_Attempt_Dico      0      1           0   
9                  Max_Procedure_Duration_Num      0      0           0   
11                 Work_Perceived_Maching_Num      0      0           0   
12                   Heb_Job_Motivators_Index      0      1           0   
13         Heb_Interests_and_Activities_Index      0      1           0   

    DecisionTree  GradientBoosting  OLS  LinearRegression  RandomForest  \
0              1                 1    1                 0             1   
1              1                 1    1                 0             1   
3              0                 0    1                 1             0   
5              1                 0    1                 0             1   
6              0                 0    1                 1             0   
7              0                 0    1                 1             0   
8              0                 0    1                 1             0   
9              0                 0    1                 0             0   
11             0                 0    1                 0             0   
12             1                 1    1                 1             1   
13             1                 1    1                 1             1   

    AdaBoost  SVM  SGD  Sum  
0          1    1    0    6  
1          1    1    0    6  
3          0    0    1    4  
5          1    1    0    6  
6          0    0    0    3  
7          0    0    1    4  
8          0    0    1    4  
9          0    1    0    2  
11         0    1    1    3  
12         1    0    0    7  
13         1    0    1    8  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"] <= 1]["Variable"].tolist()
# Omit features with high VIF from the DataFrame
Heb_Sofi_Gold_List = Final_list_Heb_filtered.drop(low_sig_corrs, axis=1)
Heb_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 480 entries, 6 to 2242
Data columns (total 12 columns):
 #   Column                                     Non-Null Count  Dtype  
---  ------                                     --------------  -----  
 0   Heb_Educational_achievement_index          480 non-null    float64
 1   Hebrew_Meam_Num                            480 non-null    float64
 2   Service_Period_Commitment_Dico             480 non-null    float64
 3   Age_Num                                    480 non-null    float64
 4   Sacham_officer_Dico                        480 non-null    float64
 5   Dapar_Hebrew_Failurer_Last_Attempt_Dico    480 non-null    float64
 6   Evaluation_Center_Filed_Last_Attempt_Dico  480 non-null    float64
 7   Max_Procedure_Duration_Num                 480 non-null    float64
 8   Work_Perceived_Maching_Num                 480 non-null    float64
 9   Heb_Job_Motivators_Index                   480 non-null    float64
 10  Heb_Interests_and_Activities_Index         480 non-null    float64
 11  Hebrew_Target                              480 non-null    float64
dtypes: float64(12)
memory usage: 48.8 KB
---- Normot Test Grade ----
------------Feature Engineering-------------
Hebrew_Meam_Num
Work_Perceived_Maching_Num
Number_of_Attempts_Num
Volunteering_Dico
Married_Dico
Nation_Dico
Previous_Job_Salary_Num
Dapar_Grade_3_Less_Dico
Salary_Expectations_Num
Mental_Difficulties_Dico
United_Employment_Problems
Job_Motivators_Index - Combine Variable
df['Normot_Job_Motivators_Index'] = df.The_Action_In_Work * 0.158 + df.Default_Employment * 0.130 + df.Employment_Stability * -0.129 + df.Exercise_Authority *0.119 + df.Contribution_to_Society * -0.105  + df.Wearing_Uniform * 0.097 + df.Good_Salary * 0.090  + df.Personal_Development * -0.089  + df.Independence_and_Autonomy * 0.075  + df.Use_Force * 0.069
Interests_and_Activities_Index - Combine Variable
df['Normot_Interests_and_Activities_Index'] = df.Literature * -0.135 + df.Enjoy_Parties_Bars * 0.108 + df.Gambling * 0.097
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Normot_Gold_List = pd.DataFrame(df, columns=["Hebrew_Meam_Num", "Work_Perceived_Maching_Num", "Number_of_Attempts_Num", "Volunteering_Dico", "Married_Dico", "Nation_Dico", "Previous_Job_Salary_Num", "Dapar_Grade_3_Less_Dico", "Salary_Expectations_Num", "Mental_Difficulties_Dico", "United_Employment_Problems", "Normot_Job_Motivators_Index", "Normot_Interests_and_Activities_Index", "Normot_Target"])
for col in Normot_Gold_List:
    if col in Normot_Gold_List.columns:
        Normot_Gold_List[col] = Normot_Gold_List[col].astype(np.float64)
Normot_Gold_List[Normot_Gold_List.columns[0:]].corr(method='spearman')['Normot_Target'].sort_values(ascending=False)
Normot_Target                            1.000000
Normot_Job_Motivators_Index              0.254128
Normot_Interests_and_Activities_Index    0.164802
Dapar_Grade_3_Less_Dico                  0.076533
Salary_Expectations_Num                  0.074772
Mental_Difficulties_Dico                 0.055330
Number_of_Attempts_Num                  -0.060725
Previous_Job_Salary_Num                 -0.085872
Work_Perceived_Maching_Num              -0.105067
Married_Dico                            -0.112085
United_Employment_Problems              -0.116745
Volunteering_Dico                       -0.124188
Nation_Dico                             -0.131325
Hebrew_Meam_Num                         -0.159104
Name: Normot_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Normot_Linear = pd.DataFrame(Normot_Gold_List, columns=["Hebrew_Meam_Num", "Work_Perceived_Maching_Num", "Number_of_Attempts_Num", "Volunteering_Dico", "Married_Dico", "Nation_Dico", "Previous_Job_Salary_Num", "Dapar_Grade_3_Less_Dico", "Salary_Expectations_Num", "Mental_Difficulties_Dico", "United_Employment_Problems", "Normot_Job_Motivators_Index", "Normot_Interests_and_Activities_Index", "Normot_Target"])
Gold_List_Normot_Linear_cleaned = Gold_List_Normot_Linear.dropna()
#x = Gold_List_Normot_Linear_cleaned[Gold_List_Normot_Linear_cleaned.columns[~Gold_List_Normot_Linear_cleaned.columns.isin(["Normot_Target"])]]
#y = Gold_List_Normot_Linear_cleaned['Normot_Target']
#x = sm.add_constant(x);

#regr = linear_model.LinearRegression()
#regr.fit(x, y);
#model = sm.OLS(y, x).fit();

#predictions = model.predict(x);
### See quality measurs of the model

#print_model = model.summary().tables[0]
#print(print_model)
### See quality measurs of the model

#print_model = model.summary().tables[1]
#print(print_model)
#x = sm.add_constant(x)

#vif = pd.DataFrame()
#vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
#vif["features"] = x.columns
#print(vif.round(1))
Upon evaluation, it was determined that the VIF index for the three weighted personality scales is less than 0.5. Consequently, there exists no concern regarding multicollinearity issues among these weighted factors.
Stepwise Linear Reggression
# Separate predictors and target variable
X = Gold_List_Normot_Linear_cleaned.drop("Normot_Target", axis=1)
y = Gold_List_Normot_Linear_cleaned["Normot_Target"]
### Forward stepwise feature selection using Linear Regression as base model
#lr = LinearRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="r2",
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = sm.OLS(y, sm.add_constant(X_selected)).fit()

### Summary of the regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
### Display regression report using statsmodels
#X_selected = sm.add_constant(X_selected)  # Add constant term for intercept
#model_stats = sm.OLS(y, X_selected).fit()

### Print regression report
#print(model_stats.summary())
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Normot_Z_Scores = pd.DataFrame(Gold_List_Normot_Linear_cleaned, columns=["Hebrew_Meam_Num", "Work_Perceived_Maching_Num", "Number_of_Attempts_Num", "Volunteering_Dico", "Married_Dico", "Nation_Dico", "Previous_Job_Salary_Num", "Dapar_Grade_3_Less_Dico", "Salary_Expectations_Num", "Mental_Difficulties_Dico", "United_Employment_Problems", "Normot_Job_Motivators_Index", "Normot_Interests_and_Activities_Index", "Normot_Target"])
descriptive_stats_Normot = Gold_List_Normot_Z_Scores.describe()
#descriptive_stats_Normot
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Normot_Target"]
#numerical_columns = Gold_List_Normot_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Normot_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Normot_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Normot_Z_Scores[numerical_columns] = (Gold_List_Normot_Z_Scores[numerical_columns] - Gold_List_Normot_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Normot_Z_Scores[numerical_columns].max() - Gold_List_Normot_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Normot_Z_Scores[numerical_columns] = Gold_List_Normot_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Normot_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
# Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "Normot_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

# Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Normot_Linear_cleaned.columns:
#    Gold_List_Normot_Linear_cleaned[column] = Gold_List_Normot_Linear_cleaned[column]  / Gold_List_Normot_Linear_cleaned[column].abs().max()
#Gold_List_Normot_Linear_cleaned[Gold_List_Normot_Linear_cleaned.columns[1:]].corr()['Normot_Target'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
Gold_List_Normot_Z_Scores[Gold_List_Normot_Z_Scores.columns[0:]].corr(method='spearman')['Normot_Target'].sort_values(ascending=False)
Normot_Target                            1.000000
Normot_Job_Motivators_Index              0.254128
Normot_Interests_and_Activities_Index    0.164802
Dapar_Grade_3_Less_Dico                  0.076533
Salary_Expectations_Num                  0.074772
Mental_Difficulties_Dico                 0.055330
Number_of_Attempts_Num                  -0.060725
Previous_Job_Salary_Num                 -0.085872
Work_Perceived_Maching_Num              -0.105067
Married_Dico                            -0.112085
United_Employment_Problems              -0.116745
Volunteering_Dico                       -0.124188
Nation_Dico                             -0.131325
Hebrew_Meam_Num                         -0.159104
Name: Normot_Target, dtype: float64
#Gold_List_Normot_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Normot_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Normot_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Normot_Target']).round(2)
1.8 Creating Final List
### Create the new DataFrame 'Final_list'
Final_list_Normot = pd.DataFrame(Gold_List_Normot_Linear_cleaned)
### Drop rows with missing values
Final_list_Normot = Final_list_Normot.dropna()
for col in Final_list_Normot:
    if col in Final_list_Normot.columns:
        Final_list_Normot[col] = Final_list_Normot[col].astype(np.float64)
Final_list_Normot.head()
   Hebrew_Meam_Num  Work_Perceived_Maching_Num  Number_of_Attempts_Num  \
1              6.0                         6.0                     0.0   
2              6.0                         5.0                     1.0   
3              4.4                         5.0                     0.0   
4              5.8                         6.0                     1.0   
5              5.2                         5.0                     2.0   

   Volunteering_Dico  Married_Dico  Nation_Dico  Previous_Job_Salary_Num  \
1                0.0           0.0          1.0                      2.0   
2                0.0           1.0          1.0                      3.0   
3                0.0           0.0          1.0                      3.0   
4                0.0           1.0          1.0                      4.0   
5                0.0           0.0          1.0                      3.0   

   Dapar_Grade_3_Less_Dico  Salary_Expectations_Num  Mental_Difficulties_Dico  \
1                      0.0                      3.0                       0.0   
2                      0.0                      3.0                       0.0   
3                      0.0                      4.0                       0.0   
4                      0.0                      4.0                       0.0   
5                      0.0                      3.0                       0.0   

   United_Employment_Problems  Normot_Job_Motivators_Index  \
1                         1.0                       -0.128   
2                         1.0                        0.277   
3                         1.0                        0.009   
4                         2.0                       -0.234   
5                         1.0                       -0.234   

   Normot_Interests_and_Activities_Index  Normot_Target  
1                                  0.000            0.0  
2                                 -0.135            1.0  
3                                  0.000            5.0  
4                                  0.000            0.0  
5                                  0.000            0.0  
------------Feature Selection - Final Gibushon Grade------------
df_num_corr = pd.DataFrame(Final_list_Normot)
corr = Final_list_Normot.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Normot
#for i in Final_list_Normot.columns:
#        for j in Final_list_Normot.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Normot[i]) & ~pd.isna(Final_list_Normot[j]) 
#                    a = stats.spearmanr(Final_list_Normot[i][mask], Final_list_Normot[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import linear_model

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Normot.drop('Normot_Target', axis=1)
y = Final_list_Normot['Normot_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the linear regression model
regr = linear_model.LinearRegression()
regr.fit(x, y)

# Create the model
model = sm.OLS(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)
                            OLS Regression Results                            
==============================================================================
Dep. Variable:          Normot_Target   R-squared:                       0.153
Model:                            OLS   Adj. R-squared:                  0.133
Method:                 Least Squares   F-statistic:                     7.831
Date:                Tue, 09 Apr 2024   Prob (F-statistic):           1.92e-14
Time:                        16:16:55   Log-Likelihood:                -1123.4
No. Observations:                 579   AIC:                             2275.
Df Residuals:                     565   BIC:                             2336.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
=========================================================================================================
                                            coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------------------------------
const                                     5.0951      0.783      6.509      0.000       3.558       6.633
Hebrew_Meam_Num                          -0.2249      0.096     -2.338      0.020      -0.414      -0.036
Work_Perceived_Maching_Num               -0.2149      0.104     -2.073      0.039      -0.418      -0.011
Number_of_Attempts_Num                   -0.0705      0.068     -1.038      0.300      -0.204       0.063
Volunteering_Dico                        -0.2349      0.181     -1.298      0.195      -0.590       0.120
Married_Dico                             -0.2987      0.179     -1.672      0.095      -0.650       0.052
Nation_Dico                              -0.9014      0.227     -3.969      0.000      -1.347      -0.455
Previous_Job_Salary_Num                  -0.0684      0.077     -0.890      0.374      -0.219       0.083
Dapar_Grade_3_Less_Dico                   0.3723      0.467      0.798      0.425      -0.544       1.289
Salary_Expectations_Num                   0.1207      0.066      1.819      0.070      -0.010       0.251
Mental_Difficulties_Dico                  1.6197      0.549      2.948      0.003       0.540       2.699
United_Employment_Problems               -0.2667      0.167     -1.595      0.111      -0.595       0.062
Normot_Job_Motivators_Index               1.8766      0.497      3.773      0.000       0.900       2.853
Normot_Interests_and_Activities_Index     3.2839      1.222      2.687      0.007       0.884       5.684
==============================================================================
Omnibus:                      177.728   Durbin-Watson:                   1.988
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              455.136
Skew:                           1.557   Prob(JB):                     1.47e-99
Kurtosis:                       6.027   Cond. No.                         166.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                                Feature
0     1.163791                        Hebrew_Meam_Num
1     1.107641             Work_Perceived_Maching_Num
2     1.048431                 Number_of_Attempts_Num
3     1.051762                      Volunteering_Dico
4     1.137004                           Married_Dico
5     1.078680                            Nation_Dico
6     1.148487                Previous_Job_Salary_Num
7     1.023794                Dapar_Grade_3_Less_Dico
8     1.131568                Salary_Expectations_Num
9     1.020840               Mental_Difficulties_Dico
10    1.122884             United_Employment_Problems
11    1.128501            Normot_Job_Motivators_Index
12    1.067517  Normot_Interests_and_Activities_Index
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"] >= 5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Normot_filtered = Final_list_Normot.drop(high_vif_features, axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Normot_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (579, 14)
Final_list_Normot
      Hebrew_Meam_Num  Work_Perceived_Maching_Num  Number_of_Attempts_Num  \
1                 6.0                         6.0                     0.0   
2                 6.0                         5.0                     1.0   
3                 4.4                         5.0                     0.0   
4                 5.8                         6.0                     1.0   
5                 5.2                         5.0                     2.0   
...               ...                         ...                     ...   
2221              5.0                         7.0                     1.0   
2224              4.2                         5.0                     1.0   
2225              4.4                         6.0                     0.0   
2231              5.2                         5.0                     2.0   
2242              5.8                         6.0                     1.0   

      Volunteering_Dico  Married_Dico  Nation_Dico  Previous_Job_Salary_Num  \
1                   0.0           0.0          1.0                      2.0   
2                   0.0           1.0          1.0                      3.0   
3                   0.0           0.0          1.0                      3.0   
4                   0.0           1.0          1.0                      4.0   
5                   0.0           0.0          1.0                      3.0   
...                 ...           ...          ...                      ...   
2221                0.0           1.0          1.0                      4.0   
2224                0.0           0.0          1.0                      4.0   
2225                0.0           0.0          1.0                      3.0   
2231                1.0           0.0          1.0                      4.0   
2242                0.0           1.0          1.0                      3.0   

      Dapar_Grade_3_Less_Dico  Salary_Expectations_Num  \
1                         0.0                      3.0   
2                         0.0                      3.0   
3                         0.0                      4.0   
4                         0.0                      4.0   
5                         0.0                      3.0   
...                       ...                      ...   
2221                      0.0                      4.0   
2224                      0.0                      4.0   
2225                      0.0                      4.0   
2231                      0.0                      3.0   
2242                      0.0                      3.0   

      Mental_Difficulties_Dico  United_Employment_Problems  \
1                          0.0                         1.0   
2                          0.0                         1.0   
3                          0.0                         1.0   
4                          0.0                         2.0   
5                          0.0                         1.0   
...                        ...                         ...   
2221                       0.0                         1.0   
2224                       0.0                         1.0   
2225                       0.0                         2.0   
2231                       0.0                         1.0   
2242                       0.0                         1.0   

      Normot_Job_Motivators_Index  Normot_Interests_and_Activities_Index  \
1                          -0.128                                  0.000   
2                           0.277                                 -0.135   
3                           0.009                                  0.000   
4                          -0.234                                  0.000   
5                          -0.234                                  0.000   
...                           ...                                    ...   
2221                       -0.218                                  0.000   
2224                       -0.128                                  0.000   
2225                        0.223                                  0.000   
2231                       -0.194                                  0.108   
2242                       -0.323                                  0.000   

      Normot_Target  
1               0.0  
2               1.0  
3               5.0  
4               0.0  
5               0.0  
...             ...  
2221            0.0  
2224            4.0  
2225            0.0  
2231            2.0  
2242            4.0  

[579 rows x 14 columns]
Final_list_Normot = Final_list_Normot_filtered.astype({"Normot_Target":'category'})
Final_list_Normot.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 579 entries, 1 to 2242
Data columns (total 14 columns):
 #   Column                                 Non-Null Count  Dtype   
---  ------                                 --------------  -----   
 0   Hebrew_Meam_Num                        579 non-null    float64 
 1   Work_Perceived_Maching_Num             579 non-null    float64 
 2   Number_of_Attempts_Num                 579 non-null    float64 
 3   Volunteering_Dico                      579 non-null    float64 
 4   Married_Dico                           579 non-null    float64 
 5   Nation_Dico                            579 non-null    float64 
 6   Previous_Job_Salary_Num                579 non-null    float64 
 7   Dapar_Grade_3_Less_Dico                579 non-null    float64 
 8   Salary_Expectations_Num                579 non-null    float64 
 9   Mental_Difficulties_Dico               579 non-null    float64 
 10  United_Employment_Problems             579 non-null    float64 
 11  Normot_Job_Motivators_Index            579 non-null    float64 
 12  Normot_Interests_and_Activities_Index  579 non-null    float64 
 13  Normot_Target                          579 non-null    category
dtypes: category(1), float64(13)
memory usage: 64.3 KB
1.5 The Feature Selection Process
No_colinear_list_Normot = Final_list_Normot.astype({"Normot_Target":'category'})
No_colinear_list_Normot = pd.DataFrame(No_colinear_list_Normot)
temp_cols=No_colinear_list_Normot.columns.tolist()
index=No_colinear_list_Normot.columns.get_loc("Normot_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Normot=No_colinear_list_Normot[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Normot.columns[1:40]})
varSel.head(50)
                                 Variable
0                         Hebrew_Meam_Num
1              Work_Perceived_Maching_Num
2                  Number_of_Attempts_Num
3                       Volunteering_Dico
4                            Married_Dico
5                             Nation_Dico
6                 Previous_Job_Salary_Num
7                 Dapar_Grade_3_Less_Dico
8                 Salary_Expectations_Num
9                Mental_Difficulties_Dico
10             United_Employment_Problems
11            Normot_Job_Motivators_Index
12  Normot_Interests_and_Activities_Index
nm = No_colinear_list_Normot.columns[1:40]
nm = nm.append(pd.Index(['Normot_Target']))
#nm
df2_Normot = No_colinear_list_Normot[nm].copy()
#df2_Normot.head(50)
5.2 Multivariable Analysis
No_colinear_list_Normot = No_colinear_list_Normot.dropna()
X = No_colinear_list_Normot[No_colinear_list_Normot.columns[~No_colinear_list_Normot.columns.isin(['Normot_Target'])]]
y = No_colinear_list_Normot['Normot_Target']
print([X.shape,y.shape])
[(579, 13), (579,)]
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

#varSel
from sklearn.linear_model import Ridge

ridge_mod = Ridge().fit(X, y)
model = SelectFromModel(ridge_mod, prefit=True)
varSel['Ridge'] = model.get_support().astype('int64')

#varSel
from sklearn.linear_model import ElasticNet

elastic_net_mod = ElasticNet().fit(X, y)
model = SelectFromModel(elastic_net_mod, prefit=True)
varSel['ElasticNet'] = model.get_support().astype('int64')

#varSel
from sklearn.tree import DecisionTreeRegressor

dt_mod = DecisionTreeRegressor().fit(X, y)
model = SelectFromModel(dt_mod, prefit=True)
varSel['DecisionTree'] = model.get_support().astype('int64')

#varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

#varSel
import pandas as pd
import statsmodels.api as sm

# Assuming X and y are already defined and varSel is initialized

# Fit an OLS regression model
ols_model = sm.OLS(y, X).fit()

# Get p-values for each feature
p_values = ols_model.pvalues

# Perform feature selection based on a significance level (e.g., 0.05)
selected_features = p_values[p_values < 0.05].index

# Store the selected features in varSel
varSel['OLS'] = [1 if feature in selected_features else 0 for feature in X.columns]

# Print model summary
#print(ols_model.summary())

#varSel
from sklearn.linear_model import LinearRegression

linear_mod = LinearRegression().fit(X, y)
model = SelectFromModel(linear_mod, prefit=True)
varSel['LinearRegression'] = model.get_support().astype('int64')

#varSel
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

rf_mod = RandomForestRegressor().fit(X, y)
model = SelectFromModel(rf_mod, prefit=True)
varSel['RandomForest'] = model.get_support().astype('int64')

RandomForestRegressor

#varSel
sklearn.ensemble._forest.RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

#varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
#varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
#varSel
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor().fit(X, y_transformed)

model = SelectFromModel(SGD_Regressor, prefit=True)
model.get_support()

varSel['SGD'] = model.get_support().astype('int64')
#varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
#varSel
#varSelvarSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_list_Normot_Feature_Selection = varSel[varSel['Sum']>=0]
Final_list_Normot_Feature_Selection 
                                 Variable  Lasso  Ridge  ElasticNet  \
0                         Hebrew_Meam_Num      0      0           1   
1              Work_Perceived_Maching_Num      0      0           1   
2                  Number_of_Attempts_Num      0      0           1   
3                       Volunteering_Dico      0      0           1   
4                            Married_Dico      0      0           1   
5                             Nation_Dico      0      1           1   
6                 Previous_Job_Salary_Num      0      0           1   
7                 Dapar_Grade_3_Less_Dico      0      0           1   
8                 Salary_Expectations_Num      0      0           1   
9                Mental_Difficulties_Dico      0      1           1   
10             United_Employment_Problems      0      0           1   
11            Normot_Job_Motivators_Index      0      1           1   
12  Normot_Interests_and_Activities_Index      0      1           1   

    DecisionTree  GradientBoosting  OLS  LinearRegression  RandomForest  \
0              1                 1    0                 0             1   
1              1                 1    1                 0             1   
2              0                 0    0                 0             0   
3              0                 0    0                 0             0   
4              0                 0    0                 0             0   
5              0                 0    0                 1             0   
6              1                 0    0                 0             1   
7              0                 0    0                 0             0   
8              1                 0    1                 0             1   
9              0                 0    1                 1             0   
10             0                 0    0                 0             0   
11             1                 1    1                 1             1   
12             0                 0    1                 1             0   

    AdaBoost  SVM  SGD  Sum  
0          1    1    0    6  
1          0    1    0    6  
2          1    1    0    3  
3          0    0    1    2  
4          0    0    1    2  
5          0    0    1    4  
6          0    1    0    4  
7          0    0    0    1  
8          0    1    0    5  
9          0    0    1    5  
10         0    0    1    2  
11         1    0    1    9  
12         0    0    0    4  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"] <= 0]["Variable"].tolist()

# Omit features with high VIF from the DataFrame
Normot_Sofi_Gold_List = Final_list_Normot_filtered.drop(low_sig_corrs, axis=1)
Normot_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 579 entries, 1 to 2242
Data columns (total 14 columns):
 #   Column                                 Non-Null Count  Dtype  
---  ------                                 --------------  -----  
 0   Hebrew_Meam_Num                        579 non-null    float64
 1   Work_Perceived_Maching_Num             579 non-null    float64
 2   Number_of_Attempts_Num                 579 non-null    float64
 3   Volunteering_Dico                      579 non-null    float64
 4   Married_Dico                           579 non-null    float64
 5   Nation_Dico                            579 non-null    float64
 6   Previous_Job_Salary_Num                579 non-null    float64
 7   Dapar_Grade_3_Less_Dico                579 non-null    float64
 8   Salary_Expectations_Num                579 non-null    float64
 9   Mental_Difficulties_Dico               579 non-null    float64
 10  United_Employment_Problems             579 non-null    float64
 11  Normot_Job_Motivators_Index            579 non-null    float64
 12  Normot_Interests_and_Activities_Index  579 non-null    float64
 13  Normot_Target                          579 non-null    float64
dtypes: float64(14)
memory usage: 67.9 KB
---- Personality Disqualification ----
------------Feature Engineering-------------
Salary_Expectations_Num
United_Commander_or_Kazin
Psyc_Test_450_Less_Dico
Psyc_Test_600_Up_Dico
Psyc_Test_450_600_Dico
Math_4_5_Units_Num_Dico
Combat_Service_Army_Dico
Chronic_Disease_Dico
Relevant_Job_Experience_Dico
Work_Perceived_Maching_Num
Gender_Dico
Actuavlia
Max_Procedure_Duration_Num
Saham_Officer_Past_Dico
Nation_Dico
Job_Motivators_Index - Combine Variable
    df['Personality_Dis_Job_Motivators_Index'] = df.Convenient_Working_Hours * -1.121235 + df.Assistance_to_Citizens * -0.619778 + df.Employment_Stability * 0.48454 + df.Interaction_With_People * 0.258155 + df.Wearing_Uniform * 0.52356  + df.Recommendation * 0.349112 + df.Contribution_to_Society * -0.206432  + df.The_Action_In_Work * -0.375042  + df.Crime_Fighting * -0.365593
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Personality_Diss_Gold_List = pd.DataFrame(df, columns=["Salary_Expectations_Num", "United_Commander_or_Kazin", "Psyc_Test_450_600_Dico", "Psyc_Test_450_Less_Dico", "Psyc_Test_600_Up_Dico", "Math_4_5_Units_Num_Dico", "Combat_Service_Army_Dico", "Chronic_Disease_Dico", "Relevant_Job_Experience_Dico", "Work_Perceived_Maching_Num", "Gender_Dico", "Actuavlia", "Max_Procedure_Duration_Num", "Saham_Officer_Past_Dico", "Nation_Dico", "Personality_Dis_Safek_Target"])
for col in Personality_Diss_Gold_List:
    if col in Personality_Diss_Gold_List.columns:
        Personality_Diss_Gold_List[col] = Personality_Diss_Gold_List[col].astype(np.float64)
Personality_Diss_Gold_List[Personality_Diss_Gold_List.columns[0:]].corr(method='spearman')['Personality_Dis_Safek_Target'].sort_values(ascending=False)
Personality_Dis_Safek_Target    1.000000
Math_4_5_Units_Num_Dico         0.122530
Combat_Service_Army_Dico        0.108455
Actuavlia                       0.104278
Psyc_Test_450_600_Dico          0.097006
Chronic_Disease_Dico            0.085512
Nation_Dico                     0.080947
Saham_Officer_Past_Dico         0.071814
Relevant_Job_Experience_Dico    0.070840
Psyc_Test_450_Less_Dico         0.063225
United_Commander_or_Kazin      -0.046374
Psyc_Test_600_Up_Dico          -0.062116
Work_Perceived_Maching_Num     -0.070846
Gender_Dico                    -0.075211
Salary_Expectations_Num        -0.093596
Max_Procedure_Duration_Num     -0.105251
Name: Personality_Dis_Safek_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Personality_Diss_Linear = pd.DataFrame(Personality_Diss_Gold_List, columns=["Salary_Expectations_Num", "United_Commander_or_Kazin", "Psyc_Test_450_600_Dico", "Psyc_Test_450_Less_Dico", "Psyc_Test_600_Up_Dico", "Math_4_5_Units_Num_Dico", "Combat_Service_Army_Dico", "Chronic_Disease_Dico", "Relevant_Job_Experience_Dico", "Work_Perceived_Maching_Num", "Gender_Dico", "Actuavlia", "Max_Procedure_Duration_Num", "Saham_Officer_Past_Dico", "Nation_Dico", "Personality_Dis_Safek_Target"])
Gold_List_Personality_Diss_Linear_cleaned = Gold_List_Personality_Diss_Linear.dropna()
#import pandas as pd
#import statsmodels.api as sm
#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import accuracy_score, classification_report

### Assuming Gold_List_Personality_Diss_Linear_cleaned is your DataFrame

### Splitting data into predictors (x) and target variable (y)
#x = Gold_List_Personality_Diss_Linear_cleaned.drop("Personality_Dis_Safek_Target", axis=1)  # Assuming this column is your target
#y = Gold_List_Personality_Diss_Linear_cleaned['Personality_Dis_Safek_Target']

### Add a constant term to the predictor variables
#x = sm.add_constant(x)

### Instantiate the logistic regression model
#log_reg = sm.Logit(y, x)

### Fit the model on the entire dataset
#result = log_reg.fit()

### Print the full model summary
#print(result.summary())

### Make predictions on the entire dataset
#y_pred = result.predict(x) > 0.5  # Thresholding probabilities to get binary predictions

### Evaluate the model
#accuracy = accuracy_score(y, y_pred)
#print("Accuracy:", accuracy)

### You can print other evaluation metrics like classification report if needed
#print(classification_report(y, y_pred))
### Fit the model on the entire dataset
#result = log_reg.fit()

### Print the full model summary
#print(result.summary())

### Calculate VIF for each feature
#vif_data = pd.DataFrame()
#vif_data["Feature"] = x.columns
#vif_data["VIF"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
#print(vif_data)

### Make predictions on the entire dataset
#y_pred = result.predict(x) > 0.5  # Thresholding probabilities to get binary predictions

### Evaluate the model
#accuracy = accuracy_score(y, y_pred)
#print("Accuracy:", accuracy)

### You can print other evaluation metrics like classification report if needed
#print(classification_report(y, y_pred))
Stepwise Linear Reggression
#pip install mlxtend
#import pandas as pd
#import statsmodels.api as sm
#from mlxtend.feature_selection import SequentialFeatureSelector as SFS
#from sklearn.linear_model import LogisticRegression
### Separate predictors and target variable
#X = Gold_List_Personality_Diss_Linear_cleaned.drop("Personality_Dis_Safek_Target", axis=1)
#y = Gold_List_Personality_Diss_Linear_cleaned["Personality_Dis_Safek_Target"]
### Forward stepwise feature selection using Logistic Regression as base model
#lr = LogisticRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="accuracy",  # Adjusted scoring method for logistic regression
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the logistic regression model with selected features
#X_selected = X[selected_features]
#model = sm.Logit(y, sm.add_constant(X_selected)).fit()

### Summary of the logistic regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Personality_Diss_Z_Scores = pd.DataFrame(Gold_List_Personality_Diss_Linear_cleaned, columns=["Salary_Expectations_Num", "United_Commander_or_Kazin", "Psyc_Test_450_600_Dico", "Psyc_Test_450_Less_Dico", "Psyc_Test_600_Up_Dico", "Math_4_5_Units_Num_Dico", "Combat_Service_Army_Dico", "Chronic_Disease_Dico", "Relevant_Job_Experience_Dico", "Work_Perceived_Maching_Num", "Gender_Dico", "Actuavlia", "Max_Procedure_Duration_Num", "Saham_Officer_Past_Dico", "Nation_Dico", "Personality_Dis_Safek_Target"])
descriptive_stats_Personality_Diss = Gold_List_Personality_Diss_Z_Scores.describe()
#descriptive_stats_Personality_Diss
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Personality_Dis_Safek_Target"]
#numerical_columns = Gold_List_Personality_Diss_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Personality_Diss_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Personality_Diss_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Personality_Diss_Z_Scores[numerical_columns] = (Gold_List_Personality_Diss_Z_Scores[numerical_columns] - Gold_List_Personality_Diss_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Personality_Diss_Z_Scores[numerical_columns].max() - Gold_List_Personality_Diss_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Personality_Diss_Z_Scores[numerical_columns] = Gold_List_Personality_Diss_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Personality_Diss_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
# Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "Personality_Dis_Safek_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

# Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Personality_Diss_Linear_cleaned.columns:
#    Gold_List_Personality_Diss_Linear_cleaned[column] = Gold_List_Personality_Diss_Linear_cleaned[column]  / Gold_List_Personality_Diss_Linear_cleaned[column].abs().max()
#Gold_List_Personality_Diss_Linear_cleaned[Gold_List_Personality_Diss_Linear_cleaned.columns[1:]].corr()['Personality_Disqualification_Dico'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
Gold_List_Personality_Diss_Z_Scores[Gold_List_Personality_Diss_Z_Scores.columns[0:]].corr(method='spearman')['Personality_Dis_Safek_Target'].sort_values(ascending=False)
Personality_Dis_Safek_Target    1.000000
Math_4_5_Units_Num_Dico         0.122530
Combat_Service_Army_Dico        0.108455
Actuavlia                       0.104278
Psyc_Test_450_600_Dico          0.097006
Chronic_Disease_Dico            0.085512
Nation_Dico                     0.080947
Saham_Officer_Past_Dico         0.071814
Relevant_Job_Experience_Dico    0.070840
Psyc_Test_450_Less_Dico         0.063225
United_Commander_or_Kazin      -0.046374
Psyc_Test_600_Up_Dico          -0.062116
Work_Perceived_Maching_Num     -0.070846
Gender_Dico                    -0.075211
Salary_Expectations_Num        -0.093596
Max_Procedure_Duration_Num     -0.105251
Name: Personality_Dis_Safek_Target, dtype: float64
#Gold_List_Personality_Diss_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Personality_Diss_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Personality_Diss_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Personality_Dis_Safek_Target']).round(2)
1.8 Creating Final List
### Create the new DataFrame 'Final_list'
Final_list_Personality_Diss = pd.DataFrame(Gold_List_Personality_Diss_Linear_cleaned)
### Drop rows with missing values
Final_list_Personality_Diss = Final_list_Personality_Diss.dropna()
for col in Final_list_Personality_Diss:
    if col in Final_list_Personality_Diss.columns:
        Final_list_Personality_Diss[col] = Final_list_Personality_Diss[col].astype(np.float64)
------------Feature Selection - Final Gibushon Grade------------
1.1 Import Packages
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
df_num_corr = pd.DataFrame(Final_list_Personality_Diss)
corr = Final_list_Personality_Diss.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Personality_Diss
#for i in Final_list_Personality_Diss.columns:
#        for j in Final_list_Personality_Diss.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Personality_Diss[i]) & ~pd.isna(Final_list_Personality_Diss[j]) 
#                    a = stats.spearmanr(Final_list_Personality_Diss[i][mask], Final_list_Personality_Diss[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Personality_Diss.drop('Personality_Dis_Safek_Target', axis=1)
y = Final_list_Personality_Diss['Personality_Dis_Safek_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the logistic regression model
log_reg = LogisticRegression()
log_reg.fit(x, y)

# Create the model
model = sm.Logit(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)

# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
vif["Feature"] = x.columns

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
Optimization terminated successfully.
         Current function value: 0.408853
         Iterations 7
                                Logit Regression Results                                
========================================================================================
Dep. Variable:     Personality_Dis_Safek_Target   No. Observations:                  800
Model:                                    Logit   Df Residuals:                      784
Method:                                     MLE   Df Model:                           15
Date:                          Tue, 09 Apr 2024   Pseudo R-squ.:                  0.1220
Time:                                  16:16:57   Log-Likelihood:                -327.08
converged:                                 True   LL-Null:                       -372.53
Covariance Type:                      nonrobust   LLR p-value:                 6.765e-13
================================================================================================
                                   coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------------------
const                           -0.0571      0.918     -0.062      0.950      -1.857       1.743
Salary_Expectations_Num         -0.2434      0.094     -2.593      0.010      -0.427      -0.059
United_Commander_or_Kazin       -0.6453      0.252     -2.559      0.011      -1.140      -0.151
Psyc_Test_450_600_Dico           0.6997      0.325      2.152      0.031       0.063       1.337
Psyc_Test_450_Less_Dico          0.8546      0.491      1.742      0.082      -0.107       1.816
Psyc_Test_600_Up_Dico           -1.9265      1.069     -1.803      0.071      -4.021       0.168
Math_4_5_Units_Num_Dico          0.7771      0.252      3.085      0.002       0.283       1.271
Combat_Service_Army_Dico         1.1754      0.244      4.819      0.000       0.697       1.653
Chronic_Disease_Dico             0.8124      0.431      1.886      0.059      -0.032       1.656
Relevant_Job_Experience_Dico     0.4494      0.205      2.189      0.029       0.047       0.852
Work_Perceived_Maching_Num      -0.2526      0.129     -1.953      0.051      -0.506       0.001
Gender_Dico                     -0.5261      0.247     -2.130      0.033      -1.010      -0.042
Actuavlia                        0.6062      0.247      2.451      0.014       0.121       1.091
Max_Procedure_Duration_Num      -0.1198      0.060     -1.992      0.046      -0.238      -0.002
Saham_Officer_Past_Dico          0.5014      0.338      1.485      0.138      -0.160       1.163
Nation_Dico                      0.6319      0.329      1.919      0.055      -0.014       1.277
================================================================================================
VIF Factors:
    VIF Factor                       Feature
0    90.668369                         const
1     1.101461       Salary_Expectations_Num
2     1.104510     United_Commander_or_Kazin
3     1.094971        Psyc_Test_450_600_Dico
4     1.035390       Psyc_Test_450_Less_Dico
5     1.055801         Psyc_Test_600_Up_Dico
6     1.116387       Math_4_5_Units_Num_Dico
7     1.266910      Combat_Service_Army_Dico
8     1.035896          Chronic_Disease_Dico
9     1.067255  Relevant_Job_Experience_Dico
10    1.032210    Work_Perceived_Maching_Num
11    1.418081                   Gender_Dico
12    1.051716                     Actuavlia
13    1.020960    Max_Procedure_Duration_Num
14    1.066560       Saham_Officer_Past_Dico
15    1.197297                   Nation_Dico
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                       Feature
0     1.101461       Salary_Expectations_Num
1     1.104510     United_Commander_or_Kazin
2     1.094971        Psyc_Test_450_600_Dico
3     1.035390       Psyc_Test_450_Less_Dico
4     1.055801         Psyc_Test_600_Up_Dico
5     1.116387       Math_4_5_Units_Num_Dico
6     1.266910      Combat_Service_Army_Dico
7     1.035896          Chronic_Disease_Dico
8     1.067255  Relevant_Job_Experience_Dico
9     1.032210    Work_Perceived_Maching_Num
10    1.418081                   Gender_Dico
11    1.051716                     Actuavlia
12    1.020960    Max_Procedure_Duration_Num
13    1.066560       Saham_Officer_Past_Dico
14    1.197297                   Nation_Dico
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"]>=5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Personality_Diss_filtered = Final_list_Personality_Diss.drop(high_vif_features,axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Personality_Diss_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (800, 16)
Final_list_Personality_Diss = Final_list_Personality_Diss_filtered.astype({"Personality_Dis_Safek_Target":'category'})
Final_list_Personality_Diss.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 800 entries, 1 to 2242
Data columns (total 16 columns):
 #   Column                        Non-Null Count  Dtype   
---  ------                        --------------  -----   
 0   Salary_Expectations_Num       800 non-null    float64 
 1   United_Commander_or_Kazin     800 non-null    float64 
 2   Psyc_Test_450_600_Dico        800 non-null    float64 
 3   Psyc_Test_450_Less_Dico       800 non-null    float64 
 4   Psyc_Test_600_Up_Dico         800 non-null    float64 
 5   Math_4_5_Units_Num_Dico       800 non-null    float64 
 6   Combat_Service_Army_Dico      800 non-null    float64 
 7   Chronic_Disease_Dico          800 non-null    float64 
 8   Relevant_Job_Experience_Dico  800 non-null    float64 
 9   Work_Perceived_Maching_Num    800 non-null    float64 
 10  Gender_Dico                   800 non-null    float64 
 11  Actuavlia                     800 non-null    float64 
 12  Max_Procedure_Duration_Num    800 non-null    float64 
 13  Saham_Officer_Past_Dico       800 non-null    float64 
 14  Nation_Dico                   800 non-null    float64 
 15  Personality_Dis_Safek_Target  800 non-null    category
dtypes: category(1), float64(15)
memory usage: 100.9 KB
1.5 The Feature Selection Process
No_colinear_list_Personality_Diss = Final_list_Personality_Diss.astype({"Personality_Dis_Safek_Target":'category'})
No_colinear_list_Personality_Diss.dtypes
Salary_Expectations_Num          float64
United_Commander_or_Kazin        float64
Psyc_Test_450_600_Dico           float64
Psyc_Test_450_Less_Dico          float64
Psyc_Test_600_Up_Dico            float64
Math_4_5_Units_Num_Dico          float64
Combat_Service_Army_Dico         float64
Chronic_Disease_Dico             float64
Relevant_Job_Experience_Dico     float64
Work_Perceived_Maching_Num       float64
Gender_Dico                      float64
Actuavlia                        float64
Max_Procedure_Duration_Num       float64
Saham_Officer_Past_Dico          float64
Nation_Dico                      float64
Personality_Dis_Safek_Target    category
dtype: object
No_colinear_list_Personality_Diss = pd.DataFrame(No_colinear_list_Personality_Diss)
temp_cols=No_colinear_list_Personality_Diss.columns.tolist()
index=No_colinear_list_Personality_Diss.columns.get_loc("Personality_Dis_Safek_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Personality_Diss=No_colinear_list_Personality_Diss[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Personality_Diss.columns[1:40]})
varSel.head(50)
                        Variable
0        Salary_Expectations_Num
1      United_Commander_or_Kazin
2         Psyc_Test_450_600_Dico
3        Psyc_Test_450_Less_Dico
4          Psyc_Test_600_Up_Dico
5        Math_4_5_Units_Num_Dico
6       Combat_Service_Army_Dico
7           Chronic_Disease_Dico
8   Relevant_Job_Experience_Dico
9     Work_Perceived_Maching_Num
10                   Gender_Dico
11                     Actuavlia
12    Max_Procedure_Duration_Num
13       Saham_Officer_Past_Dico
14                   Nation_Dico
nm = No_colinear_list_Personality_Diss.columns[1:40]
nm = nm.append(pd.Index(['Personality_Dis_Safek_Target']))
#nm
df2_Personality_Diss = No_colinear_list_Personality_Diss[nm].copy()
#df2_Personality_Diss.head(50)
5.2 Multivariable Analysis
No_colinear_list_Personality_Diss = No_colinear_list_Personality_Diss.dropna()
X = No_colinear_list_Personality_Diss[No_colinear_list_Personality_Diss.columns[~No_colinear_list_Personality_Diss.columns.isin(['Personality_Dis_Safek_Target'])]]
y = No_colinear_list_Personality_Diss['Personality_Dis_Safek_Target']
print([X.shape,y.shape])
[(800, 15), (800,)]
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectFpr
from sklearn.feature_selection import SelectFdr
from sklearn.feature_selection import SelectFwe
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LogisticRegression
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

#varSel
# SelectKBest with ANOVA F-value
skb_f = SelectKBest(score_func=f_classif, k='all').fit(X, y)
varSel['SelectKBest_f'] = skb_f.get_support().astype('int64')

#varSel
# SelectKBest with chi-squared
skb_chi2 = SelectKBest(score_func=chi2, k='all').fit(X, y)
varSel['SelectKBest_chi2'] = skb_chi2.get_support().astype('int64')

#varSel
# SelectKBest with mutual information
skb_mutual_info = SelectKBest(score_func=mutual_info_classif, k='all').fit(X, y)
varSel['SelectKBest_mutual_info'] = skb_mutual_info.get_support().astype('int64')

#varSel
# SelectFpr
select_fpr = SelectFpr().fit(X, y)
varSel['SelectFpr'] = select_fpr.get_support().astype('int64')
#varSel
# SelectFdr
select_fdr = SelectFdr().fit(X, y)
varSel['SelectFdr'] = select_fdr.get_support().astype('int64')
#varSel
# SelectFwe
select_fwe = SelectFwe().fit(X, y)
varSel['SelectFwe'] = select_fwe.get_support().astype('int64')
#varSel
# SelectPercentile
select_percentile = SelectPercentile().fit(X, y)
varSel['SelectPercentile'] = select_percentile.get_support().astype('int64')
#varSel
from sklearn.linear_model import LogisticRegression

LogisticRegression_mod = LogisticRegression().fit(X, y)
model = SelectFromModel(LogisticRegression_mod, prefit=True)
varSel['LogisticRegression'] = model.get_support().astype('int64')
#varSel
from sklearn.ensemble import RandomForestClassifier

RandomForestClassifier_mod = RandomForestClassifier().fit(X, y)
model = SelectFromModel(RandomForestClassifier_mod, prefit=True)
varSel['RandomForestClassifier'] = model.get_support().astype('int64')
#varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')
#varSel
from sklearn.linear_model import SGDClassifier

SGDClassifier_mod = SGDClassifier().fit(X, y)
model = SelectFromModel(SGDClassifier_mod, prefit=True)
varSel['SGDClassifier'] = model.get_support().astype('int64')
#varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
#varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
#varSel
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel

XGBClassifier_mod = XGBClassifier().fit(X, y_transformed)
model = SelectFromModel(XGBClassifier_mod, prefit=True)
model.get_support()

varSel['XGBClassifier'] = model.get_support().astype('int64')
#varSel
#pip install lightgbm
from lightgbm import LGBMClassifier
from sklearn.feature_selection import SelectFromModel

LGBMClassifier_mod = LGBMClassifier().fit(X, y_transformed)
model = SelectFromModel(LGBMClassifier_mod, prefit=True)
model.get_support()

varSel['LGBMClassifier'] = model.get_support().astype('int64')
#varSel
[LightGBM] [Info] Number of positive: 141, number of negative: 659
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000279 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 48
[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 15
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.176250 -> initscore=-1.541964
[LightGBM] [Info] Start training from score -1.541964
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectFromModel

DecisionTree_mod = DecisionTreeClassifier().fit(X, y_transformed)
model = SelectFromModel(DecisionTree_mod, prefit=True)
model.get_support()

varSel['DecisionTree'] = model.get_support().astype('int64')
#varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
#varSel
#varSelvarSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_list_Personality_Diss_Feature_Selection = varSel[varSel['Sum']>=4]
Final_list_Personality_Diss_Feature_Selection
                        Variable  Lasso  SelectKBest_f  SelectKBest_chi2  \
0        Salary_Expectations_Num      0              1                 1   
1      United_Commander_or_Kazin      0              1                 1   
2         Psyc_Test_450_600_Dico      0              1                 1   
3        Psyc_Test_450_Less_Dico      0              1                 1   
4          Psyc_Test_600_Up_Dico      0              1                 1   
5        Math_4_5_Units_Num_Dico      0              1                 1   
6       Combat_Service_Army_Dico      0              1                 1   
7           Chronic_Disease_Dico      0              1                 1   
8   Relevant_Job_Experience_Dico      0              1                 1   
9     Work_Perceived_Maching_Num      0              1                 1   
10                   Gender_Dico      0              1                 1   
11                     Actuavlia      0              1                 1   
12    Max_Procedure_Duration_Num      0              1                 1   
13       Saham_Officer_Past_Dico      0              1                 1   
14                   Nation_Dico      0              1                 1   

    SelectKBest_mutual_info  SelectFpr  SelectFdr  SelectFwe  \
0                         1          1          1          0   
1                         1          0          0          0   
2                         1          1          1          0   
3                         1          0          0          0   
4                         1          0          0          0   
5                         1          1          1          1   
6                         1          1          1          1   
7                         1          1          1          0   
8                         1          1          0          0   
9                         1          0          0          0   
10                        1          1          0          0   
11                        1          1          1          1   
12                        1          1          1          0   
13                        1          1          0          0   
14                        1          1          1          0   

    SelectPercentile  LogisticRegression  RandomForestClassifier  \
0                  0                   0                       1   
1                  0                   1                       0   
2                  0                   1                       0   
3                  0                   1                       0   
4                  0                   1                       0   
5                  1                   1                       0   
6                  1                   1                       0   
7                  0                   1                       0   
8                  0                   0                       1   
9                  0                   0                       1   
10                 0                   0                       0   
11                 0                   1                       0   
12                 0                   0                       1   
13                 0                   0                       0   
14                 0                   0                       0   

    GradientBoosting  SGDClassifier  AdaBoost  SVM  XGBClassifier  \
0                  1              0         1    1              0   
1                  0              0         0    0              0   
2                  1              0         0    0              1   
3                  0              1         0    0              0   
4                  0              1         0    0              0   
5                  1              0         0    0              1   
6                  1              1         1    0              1   
7                  1              1         0    0              1   
8                  0              0         0    0              0   
9                  0              1         1    1              0   
10                 0              1         0    0              0   
11                 0              0         0    0              1   
12                 1              0         1    1              0   
13                 0              0         0    0              0   
14                 1              1         0    0              1   

    LGBMClassifier  DecisionTree  Sum  
0                1             1   11  
1                0             0    4  
2                0             0    8  
3                0             0    5  
4                0             0    5  
5                0             0   10  
6                0             0   12  
7                0             0    9  
8                1             1    7  
9                1             1    9  
10               1             0    6  
11               0             0    8  
12               1             1   11  
13               0             0    4  
14               0             0    8  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"]<=3]["Variable"].tolist()
# Omit features with high VIF from the DataFrame
Personality_Diss_Sofi_Gold_List = Final_list_Personality_Diss_filtered.drop(low_sig_corrs,axis=1)
Personality_Diss_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 800 entries, 1 to 2242
Data columns (total 16 columns):
 #   Column                        Non-Null Count  Dtype  
---  ------                        --------------  -----  
 0   Salary_Expectations_Num       800 non-null    float64
 1   United_Commander_or_Kazin     800 non-null    float64
 2   Psyc_Test_450_600_Dico        800 non-null    float64
 3   Psyc_Test_450_Less_Dico       800 non-null    float64
 4   Psyc_Test_600_Up_Dico         800 non-null    float64
 5   Math_4_5_Units_Num_Dico       800 non-null    float64
 6   Combat_Service_Army_Dico      800 non-null    float64
 7   Chronic_Disease_Dico          800 non-null    float64
 8   Relevant_Job_Experience_Dico  800 non-null    float64
 9   Work_Perceived_Maching_Num    800 non-null    float64
 10  Gender_Dico                   800 non-null    float64
 11  Actuavlia                     800 non-null    float64
 12  Max_Procedure_Duration_Num    800 non-null    float64
 13  Saham_Officer_Past_Dico       800 non-null    float64
 14  Nation_Dico                   800 non-null    float64
 15  Personality_Dis_Safek_Target  800 non-null    float64
dtypes: float64(16)
memory usage: 106.2 KB
---- Gibushon ----
------------Feature Engineering-------------
Education_and_Inteligence_Index
Psyc_Test_Index
Job_Motivators_Index
#Job_Motivators_Corr = pd.DataFrame(df, columns=["Contribution_to_Society", "Independence_and_Autonomy", "Crime_Fighting", "Challenging_Interesting_Work","Good_Salary", "Personal_Development", "Diversity_at_Work", "Employment_Stability", "Convenient_Working_Hours", "Convenient_Work_Location", "Use_Force", "Exercise_Authority", "Social_Benefits", "Default_Employment", "Respectable_Job", "Wearing_Uniform", "Gibushon_Target"])
#Job_Motivators_Corr[Job_Motivators_Corr.columns[0:]].corr(method='spearman')['Gibushon_Target']
df['Gibushon_Job_Motivators_Index'] = df.Contribution_to_Society * 0.172069  + df.Independence_and_Autonomy * -0.183738  + df.Crime_Fighting * 0.182454  + df.Challenging_Interesting_Work * 0.159587  + df.Good_Salary * -0.153263  + df.Personal_Development * 0.118235  + df.Diversity_at_Work * 0.069619  + df.Employment_Stability * 0.074453  + df.Convenient_Working_Hours *  -0.042256 + df.Convenient_Work_Location * -0.056902  + df.Use_Force *  -0.057798 + df.Exercise_Authority * -0.094080  + df.Social_Benefits * -0.088683  + df.Default_Employment * -0.120173  + df.Respectable_Job * -0.097522   + df.Wearing_Uniform * -0.136205
Misconduct_Index
United_Commander_or_Kazin
United_Employment_Problems
English_4_5_Units_Num_Dico
Age_Num_Fix
Hebrew_1_Expressive_Num
Interests_and_Activities_Index - Combine Variable
df['Gibushon_Job_Interests_and_Activities_Index'] = df.Cooking * 0.182 + df.Extreme_Sports * 0.075 + df.Technology_and_Computers * -0.072 + df.Nature_and_Trips * -0.074 + df.Gambling * -0.085 + df.Science * -0.117
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Gibushon_Gold_List = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Psyc_Test_Index", "Gibushon_Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "English_4_5_Units_Num_Dico", "Age_Num", "Hebrew_1_Expressive_Num", "Gibushon_Job_Interests_and_Activities_Index", "Gibushon_Target"])
Gibushon_Gold_List[Gibushon_Gold_List.columns[0:]].corr(method='spearman')['Gibushon_Target'].sort_values(ascending=False)
Gibushon_Target                                1.000000
Gibushon_Job_Motivators_Index                  0.386673
United_Commander_or_Kazin                      0.265410
Education_and_Inteligence_Index                0.254145
Hebrew_1_Expressive_Num                        0.244885
Gibushon_Job_Interests_and_Activities_Index    0.209244
Psyc_Test_Index                                0.203853
Age_Num                                        0.203201
United_Employment_Problems                     0.148601
English_4_5_Units_Num_Dico                     0.123396
Misconduct_Index                              -0.119561
Name: Gibushon_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_M_L_R = pd.DataFrame(df, columns=["Education_and_Inteligence_Index", "Psyc_Test_Index", "Gibushon_Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "English_4_5_Units_Num_Dico", "Age_Num", "Hebrew_1_Expressive_Num", "Gibushon_Job_Interests_and_Activities_Index", "Gibushon_Target"])
Gold_List_M_L_R_cleaned = Gold_List_M_L_R.dropna()
#x = Gold_List_M_L_R_cleaned[Gold_List_M_L_R_cleaned.columns[~Gold_List_M_L_R_cleaned.columns.isin(['Gibushon_Target'])]]
#y = Gold_List_M_L_R_cleaned['Gibushon_Target']
#x = sm.add_constant(x);

#regr = linear_model.LinearRegression()
#regr.fit(x, y);
#model = sm.OLS(y, x).fit();
#predictions = model.predict(x);
### See quality measurs of the model

#print_model = model.summary().tables[0]
#print(print_model)
### See quality measurs of the model

#print_model = model.summary().tables[1]
#print(print_model)
#x = sm.add_constant(x)

#vif = pd.DataFrame()
#vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.values.shape[1])]
#vif["features"] = x.columns
#print(vif.round(1))
Upon evaluation, it was determined that the VIF index for the three weighted personality scales is less than 0.5. Consequently, there exists no concern regarding multicollinearity issues among these weighted factors.
Stepwise Linear Reggression
#pip install mlxtend
#from sklearn.linear_model import LinearRegression
#from mlxtend.feature_selection import SequentialFeatureSelector as SFS
#import pandas as pd
#import statsmodels.api as sm
### Separate predictors and target variable
#X = Gold_List_M_L_R_cleaned.drop("Gibushon_Target", axis=1)
#y = Gold_List_M_L_R_cleaned["Gibushon_Target"]
### Forward stepwise feature selection using Linear Regression as base model
#lr = LinearRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="r2",
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = sm.OLS(y, sm.add_constant(X_selected)).fit()

### Summary of the regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
### Display regression report using statsmodels
#X_selected = sm.add_constant(X_selected)  # Add constant term for intercept
#model_stats = sm.OLS(y, X_selected).fit()

###Print regression report
#print(model_stats.summary())
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Z_Scores = pd.DataFrame(Gold_List_M_L_R, columns=["Education_and_Inteligence_Index", "Psyc_Test_Index", "Gibushon_Job_Motivators_Index", "Misconduct_Index", "United_Commander_or_Kazin", "United_Employment_Problems", "English_4_5_Units_Num_Dico", "Age_Num", "Hebrew_1_Expressive_Num", "Gibushon_Job_Interests_and_Activities_Index", "Gibushon_Target"])
descriptive_stats = Gold_List_Z_Scores.describe()
#descriptive_stats
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Gibushon_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Z_Scores[numerical_columns] = (Gold_List_Z_Scores[numerical_columns] - Gold_List_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Z_Scores[numerical_columns].max() - Gold_List_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
### Exclude specific variables from adjustment
#exclude_variables = ["Gibushon_Target", "RAMA_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

# Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_M_L_R.columns:
#    Final_Gold_List_Stand[column] = Gold_List_M_L_R[column]  / Gold_List_M_L_R[column].abs().max()
#Final_Gold_List_Stand[Final_Gold_List_Stand.columns[1:]].corr()['Gibushon_Target'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
###Gold_List_Z_Scores[Gold_List_Z_Scores.columns[0:]].corr(method='spearman')['Gibushon_Target'].sort_values(ascending=False)
#Gold_List_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Z_Scores.Job_Motivators_Index * 0.390 + Gold_List_Z_Scores.Education_and_Inteligence_Index * 0.253949 + Gold_List_Z_Scores.Hebrew_Meam_Num * 0.238250 + Gold_List_Z_Scores.Interests_and_Activities_Index * 0.233427 + Gold_List_Z_Scores.United_Commander_or_Kazin * 0.220433 + Gold_List_Z_Scores.Age_Num * 0.220380 + Gold_List_Z_Scores.United_Employment_Problems * 0.203406 + Gold_List_Z_Scores.Psyc_Test_Index * 0.138320 + Gold_List_Z_Scores.Small_Class_Dico_Index * 0.135312 + Gold_List_Z_Scores.Volunteering_Dico_Index * 0.113056 + Gold_List_Z_Scores.Max_Procedure_Duration_Num * 0.093545 + Gold_List_Z_Scores.Drinking_Alcohol_Frequ_Num * 0.075455 + Gold_List_Z_Scores.Work_Perceived_Maching_Num * 0.052768 + Gold_List_Z_Scores.Saham_Officer_Past_Dico * 0.052476 + (Gold_List_Z_Scores.Temp_Mean_Num *  -0.114862) + (Gold_List_Z_Scores.Misconduct_Index * -0.123042)
#Gold_List_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Gibushon_Target']).round(2)
1.8 Creating Final List
# Create the new DataFrame 'Final_list'
Gibushon_Final_list = pd.DataFrame(Gold_List_M_L_R)
# Drop rows with missing values
Gibushon_Final_list = Gibushon_Final_list.dropna()
for col in Gibushon_Final_list:
    if col in Gibushon_Final_list.columns:
        Gibushon_Final_list[col] = Gibushon_Final_list[col].astype(np.float64)
###Final_list.head()
------------Feature Selection - Final Gibushon Grade------------
df_num_corr = pd.DataFrame(Gibushon_Final_list)
corr = Gibushon_Final_list.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list
#for i in Final_list.columns:
#        for j in Final_list.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list[i]) & ~pd.isna(Final_list[j]) 
#                    a = stats.spearmanr(Final_list[i][mask], Final_list[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
#pip install numpy
#pip install --upgrade numpy
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import linear_model
import statsmodels.api as sm
### Assuming Final_list is your DataFrame

### Separate predictors and target variable
#x = Gibushon_Final_list.drop('Gibushon_Target', axis=1)
#y = Gibushon_Final_list['Gibushon_Target']

### Add constant term for intercept
#x = sm.add_constant(x)

### Fit the linear regression model
#regr = linear_model.LinearRegression()
#regr.fit(x, y)

### Create the model
#model = sm.OLS(y, x).fit()

### Get predictions
#predictions = model.predict(x)

### Print model summary
#print_model = model.summary()
#print(print_model)
### Calculate VIF for each feature
#vif = pd.DataFrame()
#vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
#vif["Feature"] = x.columns[1:]

### Print VIF factor for each variable
#print("VIF Factors:")
#print(vif)
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"] >= 5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Gibushon_Final_List_filtered = Gibushon_Final_list.drop(high_vif_features, axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_List_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (193, 17)
#Final_List_filtered.info()
1.5 The Feature Selection Process
No_colinear_list = Gibushon_Final_List_filtered.astype({"Gibushon_Target":'category'})
No_colinear_list = pd.DataFrame(No_colinear_list)
temp_cols=No_colinear_list.columns.tolist()
index=No_colinear_list.columns.get_loc("Gibushon_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list=No_colinear_list[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list.columns[1:40]})
#varSel.head(50)
nm = No_colinear_list.columns[1:40]
nm = nm.append(pd.Index(['Gibushon_Target']))
#nm
df2 = No_colinear_list[nm].copy()
###df2.head(50)
5.2 Multivariable Analysis
No_colinear_list = No_colinear_list.dropna()
X = No_colinear_list[No_colinear_list.columns[~No_colinear_list.columns.isin(['Gibushon_Target'])]]
y = No_colinear_list['Gibushon_Target']
###y.head()
###print([X.shape,y.shape])
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

#varSel
from sklearn.linear_model import Ridge

ridge_mod = Ridge().fit(X, y)
model = SelectFromModel(ridge_mod, prefit=True)
varSel['Ridge'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import ElasticNet

elastic_net_mod = ElasticNet().fit(X, y)
model = SelectFromModel(elastic_net_mod, prefit=True)
varSel['ElasticNet'] = model.get_support().astype('int64')

###varSel
from sklearn.tree import DecisionTreeRegressor

dt_mod = DecisionTreeRegressor().fit(X, y)
model = SelectFromModel(dt_mod, prefit=True)
varSel['DecisionTree'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
from sklearn.linear_model import LinearRegression

linear_mod = LinearRegression().fit(X, y)
model = SelectFromModel(linear_mod, prefit=True)
varSel['LinearRegression'] = model.get_support().astype('int64')

###varSel
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectFromModel

rf_mod = RandomForestRegressor().fit(X, y)
model = SelectFromModel(rf_mod, prefit=True)
varSel['RandomForest'] = model.get_support().astype('int64')

RandomForestRegressor

###varSel
sklearn.ensemble._forest.RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.feature_selection import SelectFromModel

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')

###varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
###varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)

model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
###varSel
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet, SGDRegressor
SGD_Regressor = SGDRegressor().fit(X, y_transformed)

model = SelectFromModel(SGD_Regressor, prefit=True)
model.get_support()

varSel['SGD'] = model.get_support().astype('int64')
###varSel
import pandas as pd
import statsmodels.api as sm

# Assuming X and y are already defined and varSel is initialized

# Fit an OLS regression model
ols_model = sm.OLS(y, X).fit()

# Get p-values for each feature
p_values = ols_model.pvalues

# Perform feature selection based on a significance level (e.g., 0.05)
selected_features = p_values[p_values < 0.05].index

# Store the selected features in varSel
varSel['OLS'] = [1 if feature in selected_features else 0 for feature in X.columns]

# Print model summary
#print(ols_model.summary())

###varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
varSel
                                      Variable  Lasso  Ridge  ElasticNet  \
0              Education_and_Inteligence_Index      0      0           0   
1                              Psyc_Test_Index      0      0           0   
2                Gibushon_Job_Motivators_Index      0      1           0   
3                             Misconduct_Index      0      0           0   
4                    United_Commander_or_Kazin      0      1           0   
5                   United_Employment_Problems      0      0           0   
6                   English_4_5_Units_Num_Dico      0      0           0   
7                                      Age_Num      1      0           1   
8                      Hebrew_1_Expressive_Num      0      0           0   
9  Gibushon_Job_Interests_and_Activities_Index      0      1           0   

   DecisionTree  GradientBoosting  LinearRegression  RandomForest  AdaBoost  \
0             1                 1                 0             1         1   
1             0                 0                 0             0         1   
2             1                 1                 1             1         0   
3             0                 0                 0             0         0   
4             0                 0                 1             0         0   
5             0                 0                 0             0         0   
6             0                 0                 0             0         0   
7             1                 1                 0             1         0   
8             0                 0                 0             0         0   
9             1                 1                 1             1         0   

   SVM  SGD  OLS  Sum  
0    0    0    0    4  
1    0    1    0    2  
2    0    1    1    7  
3    0    1    1    2  
4    0    0    1    3  
5    0    0    0    0  
6    0    0    0    0  
7    1    0    1    7  
8    0    1    1    2  
9    0    1    1    7  
#varSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_List_Feature_Selection = varSel[varSel['Sum']>=0]
Final_List_Feature_Selection
#varSel[varSel['Sum']>=2]
                                      Variable  Lasso  Ridge  ElasticNet  \
0              Education_and_Inteligence_Index      0      0           0   
1                              Psyc_Test_Index      0      0           0   
2                Gibushon_Job_Motivators_Index      0      1           0   
3                             Misconduct_Index      0      0           0   
4                    United_Commander_or_Kazin      0      1           0   
5                   United_Employment_Problems      0      0           0   
6                   English_4_5_Units_Num_Dico      0      0           0   
7                                      Age_Num      1      0           1   
8                      Hebrew_1_Expressive_Num      0      0           0   
9  Gibushon_Job_Interests_and_Activities_Index      0      1           0   

   DecisionTree  GradientBoosting  LinearRegression  RandomForest  AdaBoost  \
0             1                 1                 0             1         1   
1             0                 0                 0             0         1   
2             1                 1                 1             1         0   
3             0                 0                 0             0         0   
4             0                 0                 1             0         0   
5             0                 0                 0             0         0   
6             0                 0                 0             0         0   
7             1                 1                 0             1         0   
8             0                 0                 0             0         0   
9             1                 1                 1             1         0   

   SVM  SGD  OLS  Sum  
0    0    0    0    4  
1    0    1    0    2  
2    0    1    1    7  
3    0    1    1    2  
4    0    0    1    3  
5    0    0    0    0  
6    0    0    0    0  
7    1    0    1    7  
8    0    1    1    2  
9    0    1    1    7  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"] < 0]["Variable"].tolist()

# Omit features with high VIF from the DataFrame
Gibushon_Grade_Sofi_Gold_List = Gibushon_Final_List_filtered.drop(low_sig_corrs, axis=1)
Gibushon_Grade_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 192 entries, 2 to 2225
Data columns (total 11 columns):
 #   Column                                       Non-Null Count  Dtype  
---  ------                                       --------------  -----  
 0   Education_and_Inteligence_Index              192 non-null    float64
 1   Psyc_Test_Index                              192 non-null    float64
 2   Gibushon_Job_Motivators_Index                192 non-null    float64
 3   Misconduct_Index                             192 non-null    float64
 4   United_Commander_or_Kazin                    192 non-null    float64
 5   United_Employment_Problems                   192 non-null    float64
 6   English_4_5_Units_Num_Dico                   192 non-null    float64
 7   Age_Num                                      192 non-null    float64
 8   Hebrew_1_Expressive_Num                      192 non-null    float64
 9   Gibushon_Job_Interests_and_Activities_Index  192 non-null    float64
 10  Gibushon_Target                              192 non-null    float64
dtypes: float64(11)
memory usage: 18.0 KB
---- Miun Result Dico ----
------------Feature Engineering-------------
United_Commander_or_Kazin
Work_Perceived_Maching_Num
Saham_Officer_Past_Dico
Misconduct_Index
Number_of_Attempts_Num
Psychiatric_Drugs_Dico
Achievements_Preception_Num
Physical_Fitness_Frequ_Num
Weight_Num
Job_Motivators_Index
Interests_and_Activities_Index
df['Gius_Interests_and_Activities_Index'] = df.Exercise_Authority * -0.812 + df.Personal_Development * 0.335 + df.Diversity_at_Work * 0.239749 + df.Contribution_to_Society * 0.181659 + df.Convenient_Work_Location * 0.320472  + df.Respectable_Job * -0.193094 + df.Good_Salary * -0.585902  + df.The_Action_In_Work * -0.377658  + df.Manage_and_Command * -0.349456
Job_Motivators_Index - Combine Variable
df['Gius_Job_Motivators_Index'] = df.Enjoy_Parties_Bars * -0.726507 + df.Technology_and_Computers * -0.502814 + df.Meeting_With_Friends * 0.336094 + df.Actuavlia * 0.39675 + df.Healt * 0.254358 + df.Volunteering * 0.391669 + df.Cooking * 0.21451 + df.Managing_Healthy_Lifestyle * 0.173105 + df.Psychology * -0.212941 + df.Science * -0.315026 + df.Theater * -0.312719
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Gius_Gold_List = pd.DataFrame(df, columns=["United_Commander_or_Kazin","Work_Perceived_Maching_Num","Saham_Officer_Past_Dico","Misconduct_Index","Number_of_Attempts_Num","Psychiatric_Drugs_Dico","Achievements_Preception_Num","Physical_Fitness_Frequ_Num","Weight_Num","Gius_Interests_and_Activities_Index","Gius_Job_Motivators_Index","Gius_Target"])
for col in Gius_Gold_List:
    if col in Gius_Gold_List.columns:
        Gius_Gold_List[col] = Gius_Gold_List[col].astype(np.float64)
Gius_Gold_List[Gius_Gold_List.columns[0:]].corr(method='spearman')['Gius_Target'].sort_values(ascending=False)
Gius_Target                            1.000000
Gius_Job_Motivators_Index              0.223541
Saham_Officer_Past_Dico                0.163613
Gius_Interests_and_Activities_Index    0.163595
Achievements_Preception_Num            0.132803
Work_Perceived_Maching_Num             0.105012
United_Commander_or_Kazin              0.104657
Physical_Fitness_Frequ_Num             0.075333
Psychiatric_Drugs_Dico                -0.099773
Number_of_Attempts_Num                -0.117336
Weight_Num                            -0.127237
Misconduct_Index                      -0.180348
Name: Gius_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Gius_Linear = pd.DataFrame(Gius_Gold_List, columns=["United_Commander_or_Kazin","Work_Perceived_Maching_Num","Saham_Officer_Past_Dico","Misconduct_Index","Number_of_Attempts_Num","Psychiatric_Drugs_Dico","Achievements_Preception_Num","Physical_Fitness_Frequ_Num","Weight_Num","Gius_Interests_and_Activities_Index","Gius_Job_Motivators_Index","Gius_Target"])
Gold_List_Gius_Linear
      United_Commander_or_Kazin  Work_Perceived_Maching_Num  \
0                           0.0                         6.0   
1                           1.0                         6.0   
2                           0.0                         5.0   
3                           0.0                         5.0   
4                           0.0                         6.0   
...                         ...                         ...   
2244                        0.0                         5.0   
2245                        0.0                         4.0   
2246                        0.0                         5.0   
2247                        2.0                         5.0   
2248                        0.0                         7.0   

      Saham_Officer_Past_Dico  Misconduct_Index  Number_of_Attempts_Num  \
0                         1.0               0.0                     0.0   
1                         0.0               0.0                     0.0   
2                         0.0               0.0                     1.0   
3                         0.0               0.0                     0.0   
4                         0.0               2.0                     1.0   
...                       ...               ...                     ...   
2244                      0.0               0.0                     0.0   
2245                      0.0               2.0                     0.0   
2246                      0.0               0.0                     1.0   
2247                      0.0               1.0                     0.0   
2248                      1.0               0.0                     0.0   

      Psychiatric_Drugs_Dico  Achievements_Preception_Num  \
0                        0.0                          5.0   
1                        0.0                          6.0   
2                        0.0                          3.0   
3                        0.0                          5.0   
4                        0.0                          3.0   
...                      ...                          ...   
2244                     0.0                          5.0   
2245                     1.0                          2.0   
2246                     0.0                          4.0   
2247                     0.0                          6.0   
2248                     0.0                          7.0   

      Physical_Fitness_Frequ_Num  Weight_Num  \
0                            4.0        71.0   
1                            4.0        75.0   
2                            3.0       105.0   
3                            3.0        96.0   
4                            3.0        87.0   
...                          ...         ...   
2244                         4.0        88.0   
2245                         3.0        89.0   
2246                         2.0        57.0   
2247                         4.0        75.0   
2248                         4.0        90.0   

      Gius_Interests_and_Activities_Index  Gius_Job_Motivators_Index  \
0                                0.756408                  -0.460926   
1                               -0.250902                  -0.313457   
2                               -1.189658                   0.634635   
3                               -0.042658                   0.804962   
4                               -0.360891                  -0.288304   
...                                   ...                        ...   
2244                             0.152675                   0.214510   
2245                            -0.237251                  -0.627745   
2246                             0.502131                   0.723709   
2247                            -0.237251                  -0.100516   
2248                             0.756408                  -0.052267   

      Gius_Target  
0             NaN  
1             1.0  
2             0.0  
3             0.0  
4             0.0  
...           ...  
2244          NaN  
2245          NaN  
2246          NaN  
2247          NaN  
2248          NaN  

[2249 rows x 12 columns]
Gold_List_Gius_Linear_cleaned = Gold_List_Gius_Linear.dropna()
Gold_List_Gius_Linear_cleaned
      United_Commander_or_Kazin  Work_Perceived_Maching_Num  \
1                           1.0                         6.0   
2                           0.0                         5.0   
3                           0.0                         5.0   
4                           0.0                         6.0   
5                           0.0                         5.0   
...                         ...                         ...   
2221                        0.0                         7.0   
2224                        1.0                         5.0   
2225                        0.0                         6.0   
2231                        0.0                         5.0   
2242                        0.0                         6.0   

      Saham_Officer_Past_Dico  Misconduct_Index  Number_of_Attempts_Num  \
1                         0.0               0.0                     0.0   
2                         0.0               0.0                     1.0   
3                         0.0               0.0                     0.0   
4                         0.0               2.0                     1.0   
5                         0.0               0.0                     2.0   
...                       ...               ...                     ...   
2221                      0.0               0.0                     1.0   
2224                      0.0               1.0                     1.0   
2225                      0.0               0.0                     0.0   
2231                      0.0               1.0                     2.0   
2242                      0.0               3.0                     1.0   

      Psychiatric_Drugs_Dico  Achievements_Preception_Num  \
1                        0.0                          6.0   
2                        0.0                          3.0   
3                        0.0                          5.0   
4                        0.0                          3.0   
5                        0.0                          5.0   
...                      ...                          ...   
2221                     0.0                          5.0   
2224                     0.0                          4.0   
2225                     0.0                          4.0   
2231                     0.0                          3.0   
2242                     0.0                          7.0   

      Physical_Fitness_Frequ_Num  Weight_Num  \
1                            4.0        75.0   
2                            3.0       105.0   
3                            3.0        96.0   
4                            3.0        87.0   
5                            5.0        71.0   
...                          ...         ...   
2221                         3.0       100.0   
2224                         5.0        62.0   
2225                         4.0        72.0   
2231                         2.0        81.0   
2242                         3.0        75.0   

      Gius_Interests_and_Activities_Index  Gius_Job_Motivators_Index  \
1                               -0.250902                  -0.313457   
2                               -1.189658                   0.634635   
3                               -0.042658                   0.804962   
4                               -0.360891                  -0.288304   
5                                0.152675                  -0.419521   
...                                   ...                        ...   
2221                             0.225293                   0.275426   
2224                            -0.250902                   0.275426   
2225                            -1.189658                  -0.481746   
2231                             0.516659                  -0.703132   
2242                             0.516659                  -0.815533   

      Gius_Target  
1             1.0  
2             0.0  
3             0.0  
4             0.0  
5             0.0  
...           ...  
2221          1.0  
2224          0.0  
2225          0.0  
2231          0.0  
2242          0.0  

[549 rows x 12 columns]
#import pandas as pd
#import statsmodels.api as sm
#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import accuracy_score, classification_report

### Assuming Gold_List_Gius_Linear_cleaned is your DataFrame

### Splitting data into predictors (x) and target variable (y)
#x = Gold_List_Gius_Linear_cleaned.drop("Gius_Target", axis=1)  # Assuming this column is your target
#y = Gold_List_Gius_Linear_cleaned['Gius_Target']

### Add a constant term to the predictor variables
#x = sm.add_constant(x)

### Instantiate the logistic regression model
#log_reg = sm.Logit(y, x)

### Fit the model on the entire dataset
#result = log_reg.fit()

### Print the full model summary
#print(result.summary())

### Make predictions on the entire dataset
#y_pred = result.predict(x) > 0.5  # Thresholding probabilities to get binary predictions

### Evaluate the model
#accuracy = accuracy_score(y, y_pred)
#print("Accuracy:", accuracy)

### You can print other evaluation metrics like classification report if needed
#print(classification_report(y, y_pred))
### Fit the model on the entire dataset
#result = log_reg.fit()

### Print the full model summary
#print(result.summary())

### Calculate VIF for each feature
#vif_data = pd.DataFrame()
#vif_data["Feature"] = x.columns
#vif_data["VIF"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
#print(vif_data)

### Make predictions on the entire dataset
#y_pred = result.predict(x) > 0.5  # Thresholding probabilities to get binary predictions

### Evaluate the model
#accuracy = accuracy_score(y, y_pred)
#print("Accuracy:", accuracy)

### You can print other evaluation metrics like classification report if needed
#print(classification_report(y, y_pred))
Stepwise Linear Reggression
#pip install mlxtend
#import pandas as pd
#import statsmodels.api as sm
#from mlxtend.feature_selection import SequentialFeatureSelector as SFS
#from sklearn.linear_model import LogisticRegression
### Separate predictors and target variable
#X = Gold_List_Gius_Linear_cleaned.drop("Gius_Target", axis=1)
#y = Gold_List_Gius_Linear_cleaned["Gius_Target"]
### Forward stepwise feature selection using Logistic Regression as base model
#lr = LogisticRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="accuracy",  # Adjusted scoring method for logistic regression
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the logistic regression model with selected features
#X_selected = X[selected_features]
#model = sm.Logit(y, sm.add_constant(X_selected)).fit()

### Summary of the logistic regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Gius_Z_Scores = pd.DataFrame(Gold_List_Gius_Linear_cleaned, columns=["United_Commander_or_Kazin","Work_Perceived_Maching_Num","Saham_Officer_Past_Dico","Misconduct_Index","Number_of_Attempts_Num","Psychiatric_Drugs_Dico","Achievements_Preception_Num","Physical_Fitness_Frequ_Num","Weight_Num","Gius_Interests_and_Activities_Index","Gius_Job_Motivators_Index","Gius_Target"])
Gold_List_Gius_Linear_cleaned
      United_Commander_or_Kazin  Work_Perceived_Maching_Num  \
1                           1.0                         6.0   
2                           0.0                         5.0   
3                           0.0                         5.0   
4                           0.0                         6.0   
5                           0.0                         5.0   
...                         ...                         ...   
2221                        0.0                         7.0   
2224                        1.0                         5.0   
2225                        0.0                         6.0   
2231                        0.0                         5.0   
2242                        0.0                         6.0   

      Saham_Officer_Past_Dico  Misconduct_Index  Number_of_Attempts_Num  \
1                         0.0               0.0                     0.0   
2                         0.0               0.0                     1.0   
3                         0.0               0.0                     0.0   
4                         0.0               2.0                     1.0   
5                         0.0               0.0                     2.0   
...                       ...               ...                     ...   
2221                      0.0               0.0                     1.0   
2224                      0.0               1.0                     1.0   
2225                      0.0               0.0                     0.0   
2231                      0.0               1.0                     2.0   
2242                      0.0               3.0                     1.0   

      Psychiatric_Drugs_Dico  Achievements_Preception_Num  \
1                        0.0                          6.0   
2                        0.0                          3.0   
3                        0.0                          5.0   
4                        0.0                          3.0   
5                        0.0                          5.0   
...                      ...                          ...   
2221                     0.0                          5.0   
2224                     0.0                          4.0   
2225                     0.0                          4.0   
2231                     0.0                          3.0   
2242                     0.0                          7.0   

      Physical_Fitness_Frequ_Num  Weight_Num  \
1                            4.0        75.0   
2                            3.0       105.0   
3                            3.0        96.0   
4                            3.0        87.0   
5                            5.0        71.0   
...                          ...         ...   
2221                         3.0       100.0   
2224                         5.0        62.0   
2225                         4.0        72.0   
2231                         2.0        81.0   
2242                         3.0        75.0   

      Gius_Interests_and_Activities_Index  Gius_Job_Motivators_Index  \
1                               -0.250902                  -0.313457   
2                               -1.189658                   0.634635   
3                               -0.042658                   0.804962   
4                               -0.360891                  -0.288304   
5                                0.152675                  -0.419521   
...                                   ...                        ...   
2221                             0.225293                   0.275426   
2224                            -0.250902                   0.275426   
2225                            -1.189658                  -0.481746   
2231                             0.516659                  -0.703132   
2242                             0.516659                  -0.815533   

      Gius_Target  
1             1.0  
2             0.0  
3             0.0  
4             0.0  
5             0.0  
...           ...  
2221          1.0  
2224          0.0  
2225          0.0  
2231          0.0  
2242          0.0  

[549 rows x 12 columns]
descriptive_stats_Gius = Gold_List_Gius_Z_Scores.describe()
#descriptive_stats_Gius
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Gius_Target"]
#numerical_columns = Gold_List_Gius_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Gius_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Gius_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Gius_Z_Scores[numerical_columns] = (Gold_List_Gius_Z_Scores[numerical_columns] - Gold_List_Gius_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Gius_Z_Scores[numerical_columns].max() - Gold_List_Gius_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Gius_Z_Scores[numerical_columns] = Gold_List_Gius_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Gius_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
# Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "Gius_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

# Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Gius_Linear_cleaned.columns:
#    Gold_List_Gius_Linear_cleaned[column] = Gold_List_Gius_Linear_cleaned[column]  / Gold_List_Gius_Linear_cleaned[column].abs().max()
#Gold_List_Gius_Linear_cleaned[Gold_List_Gius_Linear_cleaned.columns[1:]].corr()['Personality_Disqualification_Dico'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
Gold_List_Gius_Z_Scores[Gold_List_Gius_Z_Scores.columns[0:]].corr(method='spearman')['Gius_Target'].sort_values(ascending=False)
Gius_Target                            1.000000
Gius_Job_Motivators_Index              0.223541
Saham_Officer_Past_Dico                0.163613
Gius_Interests_and_Activities_Index    0.163595
Achievements_Preception_Num            0.132803
Work_Perceived_Maching_Num             0.105012
United_Commander_or_Kazin              0.104657
Physical_Fitness_Frequ_Num             0.075333
Psychiatric_Drugs_Dico                -0.099773
Number_of_Attempts_Num                -0.117336
Weight_Num                            -0.127237
Misconduct_Index                      -0.180348
Name: Gius_Target, dtype: float64
#Gold_List_Gius_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Gius_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Gius_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Gius_Target']).round(2)
1.8 Creating Final List
### Create the new DataFrame 'Final_list'
Final_list_Gius = pd.DataFrame(Gold_List_Gius_Linear_cleaned)
### Drop rows with missing values
Final_list_Gius = Final_list_Gius.dropna()
for col in Final_list_Gius:
    if col in Final_list_Gius.columns:
        Final_list_Gius[col] = Final_list_Gius[col].astype(np.float64)
------------Feature Selection - Final Gibushon Grade------------
1.1 Import Packages
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
df_num_corr = pd.DataFrame(Final_list_Gius)
corr = Final_list_Gius.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Gius
#for i in Final_list_Gius.columns:
#        for j in Final_list_Gius.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Gius[i]) & ~pd.isna(Final_list_Gius[j]) 
#                    a = stats.spearmanr(Final_list_Gius[i][mask], Final_list_Gius[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Gius.drop('Gius_Target', axis=1)
y = Final_list_Gius['Gius_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the logistic regression model
log_reg = LogisticRegression()
log_reg.fit(x, y)

# Create the model
model = sm.Logit(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)

# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
vif["Feature"] = x.columns

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
Optimization terminated successfully.
         Current function value: 0.564706
         Iterations 6
                           Logit Regression Results                           
==============================================================================
Dep. Variable:            Gius_Target   No. Observations:                  549
Model:                          Logit   Df Residuals:                      537
Method:                           MLE   Df Model:                           11
Date:                Tue, 09 Apr 2024   Pseudo R-squ.:                  0.1648
Time:                        16:17:01   Log-Likelihood:                -310.02
converged:                       True   LL-Null:                       -371.19
Covariance Type:            nonrobust   LLR p-value:                 6.125e-21
=======================================================================================================
                                          coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------------------
const                                  -3.2839      1.075     -3.055      0.002      -5.391      -1.177
United_Commander_or_Kazin               0.4640      0.204      2.277      0.023       0.065       0.863
Work_Perceived_Maching_Num              0.2799      0.142      1.969      0.049       0.001       0.559
Saham_Officer_Past_Dico                 1.1518      0.370      3.109      0.002       0.426       1.878
Misconduct_Index                       -0.7908      0.263     -3.010      0.003      -1.306      -0.276
Number_of_Attempts_Num                 -0.3081      0.106     -2.896      0.004      -0.517      -0.100
Psychiatric_Drugs_Dico                 -2.1368      0.813     -2.627      0.009      -3.731      -0.543
Achievements_Preception_Num             0.3308      0.096      3.443      0.001       0.142       0.519
Physical_Fitness_Frequ_Num              0.2109      0.104      2.025      0.043       0.007       0.415
Weight_Num                             -0.0150      0.007     -2.294      0.022      -0.028      -0.002
Gius_Interests_and_Activities_Index     0.8694      0.235      3.694      0.000       0.408       1.331
Gius_Job_Motivators_Index               0.9603      0.208      4.616      0.000       0.553       1.368
=======================================================================================================
VIF Factors:
    VIF Factor                              Feature
0   115.084242                                const
1     1.071470            United_Commander_or_Kazin
2     1.044386           Work_Perceived_Maching_Num
3     1.041965              Saham_Officer_Past_Dico
4     1.044692                     Misconduct_Index
5     1.039450               Number_of_Attempts_Num
6     1.029634               Psychiatric_Drugs_Dico
7     1.075144          Achievements_Preception_Num
8     1.069962           Physical_Fitness_Frequ_Num
9     1.062696                           Weight_Num
10    1.024149  Gius_Interests_and_Activities_Index
11    1.063015            Gius_Job_Motivators_Index
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                              Feature
0     1.071470            United_Commander_or_Kazin
1     1.044386           Work_Perceived_Maching_Num
2     1.041965              Saham_Officer_Past_Dico
3     1.044692                     Misconduct_Index
4     1.039450               Number_of_Attempts_Num
5     1.029634               Psychiatric_Drugs_Dico
6     1.075144          Achievements_Preception_Num
7     1.069962           Physical_Fitness_Frequ_Num
8     1.062696                           Weight_Num
9     1.024149  Gius_Interests_and_Activities_Index
10    1.063015            Gius_Job_Motivators_Index
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"]>=5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Gius_filtered = Final_list_Gius.drop(high_vif_features,axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Gius_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (549, 12)
Final_list_Gius = Final_list_Gius_filtered.astype({"Gius_Target":'category'})
Final_list_Gius.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 549 entries, 1 to 2242
Data columns (total 12 columns):
 #   Column                               Non-Null Count  Dtype   
---  ------                               --------------  -----   
 0   United_Commander_or_Kazin            549 non-null    float64 
 1   Work_Perceived_Maching_Num           549 non-null    float64 
 2   Saham_Officer_Past_Dico              549 non-null    float64 
 3   Misconduct_Index                     549 non-null    float64 
 4   Number_of_Attempts_Num               549 non-null    float64 
 5   Psychiatric_Drugs_Dico               549 non-null    float64 
 6   Achievements_Preception_Num          549 non-null    float64 
 7   Physical_Fitness_Frequ_Num           549 non-null    float64 
 8   Weight_Num                           549 non-null    float64 
 9   Gius_Interests_and_Activities_Index  549 non-null    float64 
 10  Gius_Job_Motivators_Index            549 non-null    float64 
 11  Gius_Target                          549 non-null    category
dtypes: category(1), float64(11)
memory usage: 52.1 KB
1.5 The Feature Selection Process
No_colinear_list_Gius = Final_list_Gius.astype({"Gius_Target":'category'})
No_colinear_list_Gius.dtypes
United_Commander_or_Kazin               float64
Work_Perceived_Maching_Num              float64
Saham_Officer_Past_Dico                 float64
Misconduct_Index                        float64
Number_of_Attempts_Num                  float64
Psychiatric_Drugs_Dico                  float64
Achievements_Preception_Num             float64
Physical_Fitness_Frequ_Num              float64
Weight_Num                              float64
Gius_Interests_and_Activities_Index     float64
Gius_Job_Motivators_Index               float64
Gius_Target                            category
dtype: object
No_colinear_list_Gius = pd.DataFrame(No_colinear_list_Gius)
temp_cols=No_colinear_list_Gius.columns.tolist()
index=No_colinear_list_Gius.columns.get_loc("Gius_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Gius=No_colinear_list_Gius[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Gius.columns[1:40]})
varSel.head(50)
                               Variable
0             United_Commander_or_Kazin
1            Work_Perceived_Maching_Num
2               Saham_Officer_Past_Dico
3                      Misconduct_Index
4                Number_of_Attempts_Num
5                Psychiatric_Drugs_Dico
6           Achievements_Preception_Num
7            Physical_Fitness_Frequ_Num
8                            Weight_Num
9   Gius_Interests_and_Activities_Index
10            Gius_Job_Motivators_Index
nm = No_colinear_list_Gius.columns[1:40]
nm = nm.append(pd.Index(['Gius_Target']))
#nm
df2_Gius = No_colinear_list_Gius[nm].copy()
#df2_Gius.head(50)
5.2 Multivariable Analysis
No_colinear_list_Gius = No_colinear_list_Gius.dropna()
X = No_colinear_list_Gius[No_colinear_list_Gius.columns[~No_colinear_list_Gius.columns.isin(['Gius_Target'])]]
y = No_colinear_list_Gius['Gius_Target']
print([X.shape,y.shape])
[(549, 11), (549,)]
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectFpr
from sklearn.feature_selection import SelectFdr
from sklearn.feature_selection import SelectFwe
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LogisticRegression
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

#varSel
# SelectKBest with ANOVA F-value
skb_f = SelectKBest(score_func=f_classif, k='all').fit(X, y)
varSel['SelectKBest_f'] = skb_f.get_support().astype('int64')

#varSel
# SelectKBest with mutual information
skb_mutual_info = SelectKBest(score_func=mutual_info_classif, k='all').fit(X, y)
varSel['SelectKBest_mutual_info'] = skb_mutual_info.get_support().astype('int64')

#varSel
# SelectFpr
select_fpr = SelectFpr().fit(X, y)
varSel['SelectFpr'] = select_fpr.get_support().astype('int64')
#varSel
# SelectFdr
select_fdr = SelectFdr().fit(X, y)
varSel['SelectFdr'] = select_fdr.get_support().astype('int64')
#varSel
# SelectFwe
select_fwe = SelectFwe().fit(X, y)
varSel['SelectFwe'] = select_fwe.get_support().astype('int64')
#varSel
# SelectPercentile
select_percentile = SelectPercentile().fit(X, y)
varSel['SelectPercentile'] = select_percentile.get_support().astype('int64')
#varSel
from sklearn.linear_model import LogisticRegression

LogisticRegression_mod = LogisticRegression().fit(X, y)
model = SelectFromModel(LogisticRegression_mod, prefit=True)
varSel['LogisticRegression'] = model.get_support().astype('int64')
#varSel
from sklearn.ensemble import RandomForestClassifier

RandomForestClassifier_mod = RandomForestClassifier().fit(X, y)
model = SelectFromModel(RandomForestClassifier_mod, prefit=True)
varSel['RandomForestClassifier'] = model.get_support().astype('int64')
#varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')
#varSel
from sklearn.linear_model import SGDClassifier

SGDClassifier_mod = SGDClassifier().fit(X, y)
model = SelectFromModel(SGDClassifier_mod, prefit=True)
varSel['SGDClassifier'] = model.get_support().astype('int64')
#varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
#varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
#varSel
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel

XGBClassifier_mod = XGBClassifier().fit(X, y_transformed)
model = SelectFromModel(XGBClassifier_mod, prefit=True)
model.get_support()

varSel['XGBClassifier'] = model.get_support().astype('int64')
#varSel
#pip install lightgbm
from lightgbm import LGBMClassifier
from sklearn.feature_selection import SelectFromModel

LGBMClassifier_mod = LGBMClassifier().fit(X, y_transformed)
model = SelectFromModel(LGBMClassifier_mod, prefit=True)
model.get_support()

varSel['LGBMClassifier'] = model.get_support().astype('int64')
#varSel
[LightGBM] [Info] Number of positive: 224, number of negative: 325
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 290
[LightGBM] [Info] Number of data points in the train set: 549, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.408015 -> initscore=-0.372179
[LightGBM] [Info] Start training from score -0.372179
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectFromModel

DecisionTree_mod = DecisionTreeClassifier().fit(X, y_transformed)
model = SelectFromModel(DecisionTree_mod, prefit=True)
model.get_support()

varSel['DecisionTree'] = model.get_support().astype('int64')
#varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
#varSel
#varSelvarSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_list_Gius_Feature_Selection = varSel[varSel['Sum']>=4]
Final_list_Gius_Feature_Selection
                               Variable  Lasso  SelectKBest_f  \
0             United_Commander_or_Kazin      0              1   
1            Work_Perceived_Maching_Num      0              1   
2               Saham_Officer_Past_Dico      0              1   
3                      Misconduct_Index      0              1   
4                Number_of_Attempts_Num      0              1   
5                Psychiatric_Drugs_Dico      0              1   
6           Achievements_Preception_Num      0              1   
8                            Weight_Num      0              1   
9   Gius_Interests_and_Activities_Index      0              1   
10            Gius_Job_Motivators_Index      0              1   

    SelectKBest_mutual_info  SelectFpr  SelectFdr  SelectFwe  \
0                         1          1          1          1   
1                         1          1          1          0   
2                         1          1          1          1   
3                         1          1          1          1   
4                         1          1          1          1   
5                         1          1          1          0   
6                         1          1          1          1   
8                         1          1          1          1   
9                         1          1          1          1   
10                        1          1          1          1   

    SelectPercentile  LogisticRegression  RandomForestClassifier  \
0                  0                   0                       0   
1                  0                   0                       0   
2                  0                   1                       0   
3                  0                   1                       0   
4                  0                   0                       0   
5                  0                   1                       0   
6                  0                   0                       0   
8                  0                   0                       1   
9                  0                   1                       1   
10                 1                   1                       1   

    GradientBoosting  SGDClassifier  AdaBoost  SVM  XGBClassifier  \
0                  0              1         0    0              0   
1                  0              0         0    0              0   
2                  0              1         0    0              1   
3                  0              1         0    0              1   
4                  0              1         1    1              0   
5                  0              0         0    0              1   
6                  0              0         0    1              0   
8                  1              0         1    1              0   
9                  1              1         1    0              0   
10                 1              1         1    1              0   

    LGBMClassifier  DecisionTree  Sum  
0                0             0    6  
1                0             0    4  
2                0             0    8  
3                0             0    8  
4                0             0    8  
5                0             0    6  
6                0             0    6  
8                1             1   11  
9                1             1   12  
10               1             1   14  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"]<=3]["Variable"].tolist()
# Omit features with high VIF from the DataFrame
Gius_Sofi_Gold_List = Final_list_Gius_filtered.drop(low_sig_corrs,axis=1)
Gius_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 549 entries, 1 to 2242
Data columns (total 11 columns):
 #   Column                               Non-Null Count  Dtype  
---  ------                               --------------  -----  
 0   United_Commander_or_Kazin            549 non-null    float64
 1   Work_Perceived_Maching_Num           549 non-null    float64
 2   Saham_Officer_Past_Dico              549 non-null    float64
 3   Misconduct_Index                     549 non-null    float64
 4   Number_of_Attempts_Num               549 non-null    float64
 5   Psychiatric_Drugs_Dico               549 non-null    float64
 6   Achievements_Preception_Num          549 non-null    float64
 7   Weight_Num                           549 non-null    float64
 8   Gius_Interests_and_Activities_Index  549 non-null    float64
 9   Gius_Job_Motivators_Index            549 non-null    float64
 10  Gius_Target                          549 non-null    float64
dtypes: float64(11)
memory usage: 51.5 KB
---- Final Gibushon Dico ----
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Spearman correlation between Variables and the target - Final Gibushun Grade :
Final_Gibushon_Dico_Gold_List = pd.DataFrame(df, columns=["Hebrew_1_Expressive_Num","United_Commander_or_Kazin","Misconduct_Index","English_3_Units_Num_Dico","Job_Motivators_Index","Interests_and_Activities_Index","Previous_Job_Salary_Num","Married_Dico","Psychiatric_Drugs_Dico","Evaluation_Center_Filed_Last_Attempt_Dico","Physical_Fitness_Frequ_Num", "Year_of_Service_Army_Dico", "Number_of_Attempts_Num", "Evaluation_Center_Dico_Target"])
for col in Final_Gibushon_Dico_Gold_List:
    if col in Final_Gibushon_Dico_Gold_List.columns:
        Final_Gibushon_Dico_Gold_List[col] = Final_Gibushon_Dico_Gold_List[col].astype(np.float64)
Final_Gibushon_Dico_Gold_List[Final_Gibushon_Dico_Gold_List.columns[0:]].corr(method='spearman')['Evaluation_Center_Dico_Target'].sort_values(ascending=False)
Evaluation_Center_Dico_Target                1.000000
Job_Motivators_Index                         0.366708
Interests_and_Activities_Index               0.246221
Hebrew_1_Expressive_Num                      0.202659
United_Commander_or_Kazin                    0.185151
Previous_Job_Salary_Num                      0.096102
Married_Dico                                 0.081395
Psychiatric_Drugs_Dico                       0.080583
Physical_Fitness_Frequ_Num                  -0.066210
Evaluation_Center_Filed_Last_Attempt_Dico   -0.070952
Number_of_Attempts_Num                      -0.078852
Year_of_Service_Army_Dico                   -0.089098
Misconduct_Index                            -0.137528
English_3_Units_Num_Dico                    -0.146211
Name: Evaluation_Center_Dico_Target, dtype: float64
Linear Reggression
Below, a linear regression analysis will be presented to explore the correlation between the three weighted personality factors and personality disqualification. The primary emphasis of this analysis will be directed towards evaluating the Variance Inflation Factor (VIF) index.
Gold_List_Gibushon_Dico_Linear = pd.DataFrame(Final_Gibushon_Dico_Gold_List, columns=["Hebrew_1_Expressive_Num","United_Commander_or_Kazin","Misconduct_Index","English_3_Units_Num_Dico","Job_Motivators_Index","Interests_and_Activities_Index","Previous_Job_Salary_Num","Married_Dico","Psychiatric_Drugs_Dico","Evaluation_Center_Filed_Last_Attempt_Dico","Physical_Fitness_Frequ_Num", "Year_of_Service_Army_Dico", "Number_of_Attempts_Num", "Evaluation_Center_Dico_Target"])
Gold_List_Gibushon_Dico_Linear
      Hebrew_1_Expressive_Num  United_Commander_or_Kazin  Misconduct_Index  \
0                         5.0                        0.0               0.0   
1                         6.0                        1.0               0.0   
2                         6.0                        0.0               0.0   
3                         4.0                        0.0               0.0   
4                         6.0                        0.0               2.0   
...                       ...                        ...               ...   
2244                      5.0                        0.0               0.0   
2245                      2.0                        0.0               2.0   
2246                      5.0                        0.0               0.0   
2247                      6.0                        2.0               1.0   
2248                      7.0                        0.0               0.0   

      English_3_Units_Num_Dico  Job_Motivators_Index  \
0                          0.0              0.416883   
1                          0.0              0.099965   
2                          1.0              0.257007   
3                          0.0              0.156437   
4                          0.0              0.259535   
...                        ...                   ...   
2244                       1.0              0.299684   
2245                       1.0             -0.293880   
2246                       0.0              0.312026   
2247                       0.0             -0.133318   
2248                       0.0              0.094945   

      Interests_and_Activities_Index  Previous_Job_Salary_Num  Married_Dico  \
0                                1.0                      4.0           1.0   
1                                3.0                      2.0           0.0   
2                                5.0                      3.0           1.0   
3                                3.0                      3.0           0.0   
4                                3.0                      4.0           1.0   
...                              ...                      ...           ...   
2244                             4.0                      5.0           0.0   
2245                             3.0                      2.0           1.0   
2246                             4.0                      3.0           1.0   
2247                             3.0                      3.0           0.0   
2248                             4.0                      2.0           1.0   

      Psychiatric_Drugs_Dico  Evaluation_Center_Filed_Last_Attempt_Dico  \
0                        0.0                                        0.0   
1                        0.0                                        0.0   
2                        0.0                                        1.0   
3                        0.0                                        0.0   
4                        0.0                                        0.0   
...                      ...                                        ...   
2244                     0.0                                        0.0   
2245                     1.0                                        0.0   
2246                     0.0                                        0.0   
2247                     0.0                                        0.0   
2248                     0.0                                        0.0   

      Physical_Fitness_Frequ_Num  Year_of_Service_Army_Dico  \
0                            4.0                        0.0   
1                            4.0                        0.0   
2                            3.0                        0.0   
3                            3.0                        0.0   
4                            3.0                        0.0   
...                          ...                        ...   
2244                         4.0                        0.0   
2245                         3.0                        0.0   
2246                         2.0                        0.0   
2247                         4.0                        0.0   
2248                         4.0                        0.0   

      Number_of_Attempts_Num  Evaluation_Center_Dico_Target  
0                        0.0                            NaN  
1                        0.0                            NaN  
2                        1.0                            0.0  
3                        0.0                            1.0  
4                        1.0                            0.0  
...                      ...                            ...  
2244                     0.0                            NaN  
2245                     0.0                            NaN  
2246                     1.0                            NaN  
2247                     0.0                            NaN  
2248                     0.0                            NaN  

[2249 rows x 14 columns]
Gold_List_Gibushon_Dico_Linear_cleaned = Gold_List_Gibushon_Dico_Linear.dropna()
Gold_List_Gibushon_Dico_Linear_cleaned
      Hebrew_1_Expressive_Num  United_Commander_or_Kazin  Misconduct_Index  \
2                         6.0                        0.0               0.0   
3                         4.0                        0.0               0.0   
4                         6.0                        0.0               2.0   
19                        6.0                        0.0               0.0   
21                        4.0                        0.0               0.0   
...                       ...                        ...               ...   
2112                      6.0                        1.0               0.0   
2132                      6.0                        0.0               0.0   
2186                      6.0                        1.0               0.0   
2224                      5.0                        1.0               1.0   
2225                      5.0                        0.0               0.0   

      English_3_Units_Num_Dico  Job_Motivators_Index  \
2                          1.0              0.257007   
3                          0.0              0.156437   
4                          0.0              0.259535   
19                         0.0              0.181566   
21                         0.0              0.401428   
...                        ...                   ...   
2112                       0.0              0.015317   
2132                       0.0              0.235908   
2186                       0.0              0.449117   
2224                       0.0              0.099965   
2225                       1.0             -0.080795   

      Interests_and_Activities_Index  Previous_Job_Salary_Num  Married_Dico  \
2                                5.0                      3.0           1.0   
3                                3.0                      3.0           0.0   
4                                3.0                      4.0           1.0   
19                               2.0                      2.0           0.0   
21                               4.0                      3.0           0.0   
...                              ...                      ...           ...   
2112                             4.0                      3.0           0.0   
2132                             3.0                      3.0           0.0   
2186                             4.0                      1.0           0.0   
2224                             2.0                      4.0           0.0   
2225                             1.0                      3.0           0.0   

      Psychiatric_Drugs_Dico  Evaluation_Center_Filed_Last_Attempt_Dico  \
2                        0.0                                        1.0   
3                        0.0                                        0.0   
4                        0.0                                        0.0   
19                       0.0                                        0.0   
21                       0.0                                        0.0   
...                      ...                                        ...   
2112                     0.0                                        0.0   
2132                     0.0                                        0.0   
2186                     0.0                                        0.0   
2224                     0.0                                        0.0   
2225                     0.0                                        0.0   

      Physical_Fitness_Frequ_Num  Year_of_Service_Army_Dico  \
2                            3.0                        0.0   
3                            3.0                        0.0   
4                            3.0                        0.0   
19                           4.0                        0.0   
21                           3.0                        0.0   
...                          ...                        ...   
2112                         4.0                        0.0   
2132                         3.0                        0.0   
2186                         4.0                        0.0   
2224                         5.0                        0.0   
2225                         4.0                        0.0   

      Number_of_Attempts_Num  Evaluation_Center_Dico_Target  
2                        1.0                            0.0  
3                        0.0                            1.0  
4                        1.0                            0.0  
19                       1.0                            0.0  
21                       0.0                            1.0  
...                      ...                            ...  
2112                     0.0                            1.0  
2132                     1.0                            1.0  
2186                     0.0                            1.0  
2224                     1.0                            0.0  
2225                     0.0                            0.0  

[193 rows x 14 columns]
#import pandas as pd
#import statsmodels.api as sm
#from sklearn.linear_model import LogisticRegression
#from sklearn.metrics import accuracy_score, classification_report

### Assuming Gold_List_Gibushon_Dico_Linear_cleaned is your DataFrame

### Splitting data into predictors (x) and target variable (y)
#x = Gold_List_Gibushon_Dico_Linear_cleaned.drop("Evaluation_Center_Dico_Target", axis=1)  # Assuming this column is your target
#y = Gold_List_Gibushon_Dico_Linear_cleaned['Evaluation_Center_Dico_Target']

### Add a constant term to the predictor variables
#x = sm.add_constant(x)

### Instantiate the logistic regression model
#log_reg = sm.Logit(y, x)

### Fit the model on the entire dataset
#result = log_reg.fit()

### Print the full model summary
#print(result.summary())

### Make predictions on the entire dataset
#y_pred = result.predict(x) > 0.5  # Thresholding probabilities to get binary predictions

### Evaluate the model
#accuracy = accuracy_score(y, y_pred)
#print("Accuracy:", accuracy)

### You can print other evaluation metrics like classification report if needed
#print(classification_report(y, y_pred))
### Fit the model on the entire dataset
#result = log_reg.fit()

### Print the full model summary
#print(result.summary())

### Calculate VIF for each feature
#vif_data = pd.DataFrame()
#vif_data["Feature"] = x.columns
#vif_data["VIF"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
#print(vif_data)

### Make predictions on the entire dataset
#y_pred = result.predict(x) > 0.5  # Thresholding probabilities to get binary predictions

### Evaluate the model
#accuracy = accuracy_score(y, y_pred)
#print("Accuracy:", accuracy)

### You can print other evaluation metrics like classification report if needed
#print(classification_report(y, y_pred))
Stepwise Linear Reggression
#pip install mlxtend
#import pandas as pd
#import statsmodels.api as sm
#from mlxtend.feature_selection import SequentialFeatureSelector as SFS
#from sklearn.linear_model import LogisticRegression
### Separate predictors and target variable
#X = Gold_List_Gibushon_Dico_Linear_cleaned.drop("Evaluation_Center_Dico_Target", axis=1)
#y = Gold_List_Gibushon_Dico_Linear_cleaned["Evaluation_Center_Dico_Target"]
### Forward stepwise feature selection using Logistic Regression as base model
#lr = LogisticRegression()
#sfs = SFS(
#    lr,
#    k_features="best",
#    forward=True,
#    floating=False,
#    scoring="accuracy",  # Adjusted scoring method for logistic regression
#    cv=5
#)

#sfs = sfs.fit(X, y)
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the logistic regression model with selected features
#X_selected = X[selected_features]
#model = sm.Logit(y, sm.add_constant(X_selected)).fit()

### Summary of the logistic regression model
#print(model.summary())
### Selected features
#selected_features = list(sfs.k_feature_names_)
#print("Selected Features:", selected_features)

### Fit the model with selected features
#X_selected = X[selected_features]
#model = lr.fit(X_selected, y)
1.5 Transfer to Z - Squares
Limit to -2.5 to +2.5, 2 Digits after dot
Gold_List_Gibushon_Dico_Z_Scores = pd.DataFrame(Gold_List_Gibushon_Dico_Linear_cleaned, columns=["Hebrew_1_Expressive_Num","United_Commander_or_Kazin","Misconduct_Index","English_3_Units_Num_Dico","Job_Motivators_Index","Interests_and_Activities_Index","Previous_Job_Salary_Num","Married_Dico","Psychiatric_Drugs_Dico","Evaluation_Center_Filed_Last_Attempt_Dico","Physical_Fitness_Frequ_Num", "Year_of_Service_Army_Dico", "Number_of_Attempts_Num", "Evaluation_Center_Dico_Target"])
Gold_List_Gibushon_Dico_Linear_cleaned
      Hebrew_1_Expressive_Num  United_Commander_or_Kazin  Misconduct_Index  \
2                         6.0                        0.0               0.0   
3                         4.0                        0.0               0.0   
4                         6.0                        0.0               2.0   
19                        6.0                        0.0               0.0   
21                        4.0                        0.0               0.0   
...                       ...                        ...               ...   
2112                      6.0                        1.0               0.0   
2132                      6.0                        0.0               0.0   
2186                      6.0                        1.0               0.0   
2224                      5.0                        1.0               1.0   
2225                      5.0                        0.0               0.0   

      English_3_Units_Num_Dico  Job_Motivators_Index  \
2                          1.0              0.257007   
3                          0.0              0.156437   
4                          0.0              0.259535   
19                         0.0              0.181566   
21                         0.0              0.401428   
...                        ...                   ...   
2112                       0.0              0.015317   
2132                       0.0              0.235908   
2186                       0.0              0.449117   
2224                       0.0              0.099965   
2225                       1.0             -0.080795   

      Interests_and_Activities_Index  Previous_Job_Salary_Num  Married_Dico  \
2                                5.0                      3.0           1.0   
3                                3.0                      3.0           0.0   
4                                3.0                      4.0           1.0   
19                               2.0                      2.0           0.0   
21                               4.0                      3.0           0.0   
...                              ...                      ...           ...   
2112                             4.0                      3.0           0.0   
2132                             3.0                      3.0           0.0   
2186                             4.0                      1.0           0.0   
2224                             2.0                      4.0           0.0   
2225                             1.0                      3.0           0.0   

      Psychiatric_Drugs_Dico  Evaluation_Center_Filed_Last_Attempt_Dico  \
2                        0.0                                        1.0   
3                        0.0                                        0.0   
4                        0.0                                        0.0   
19                       0.0                                        0.0   
21                       0.0                                        0.0   
...                      ...                                        ...   
2112                     0.0                                        0.0   
2132                     0.0                                        0.0   
2186                     0.0                                        0.0   
2224                     0.0                                        0.0   
2225                     0.0                                        0.0   

      Physical_Fitness_Frequ_Num  Year_of_Service_Army_Dico  \
2                            3.0                        0.0   
3                            3.0                        0.0   
4                            3.0                        0.0   
19                           4.0                        0.0   
21                           3.0                        0.0   
...                          ...                        ...   
2112                         4.0                        0.0   
2132                         3.0                        0.0   
2186                         4.0                        0.0   
2224                         5.0                        0.0   
2225                         4.0                        0.0   

      Number_of_Attempts_Num  Evaluation_Center_Dico_Target  
2                        1.0                            0.0  
3                        0.0                            1.0  
4                        1.0                            0.0  
19                       1.0                            0.0  
21                       0.0                            1.0  
...                      ...                            ...  
2112                     0.0                            1.0  
2132                     1.0                            1.0  
2186                     0.0                            1.0  
2224                     1.0                            0.0  
2225                     0.0                            0.0  

[193 rows x 14 columns]
descriptive_stats_Gibushon_Dico = Gold_List_Gibushon_Dico_Z_Scores.describe()
#descriptive_stats_Gibushon_Dico
#import pandas as pd
#from sklearn.preprocessing import StandardScaler

### Exclude specific variables from standardization
#exclude_variables = ["Evaluation_Center_Dico_Target"]
#numerical_columns = Gold_List_Gibushon_Dico_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

### Standardize the variables
#scaler = StandardScaler()
#Gold_List_Gibushon_Dico_Z_Scores[numerical_columns] = scaler.fit_transform(Gold_List_Gibushon_Dico_Z_Scores[numerical_columns])

### Linear transformation to the desired range (min_range to max_range)
#min_range = -2.5
#max_range = 2.5

#scaled_min = min_range
#scaled_max = max_range

#Gold_List_Gibushon_Dico_Z_Scores[numerical_columns] = (Gold_List_Gibushon_Dico_Z_Scores[numerical_columns] - Gold_List_Gibushon_Dico_Z_Scores[numerical_columns].min()) / \
#                                      (Gold_List_Gibushon_Dico_Z_Scores[numerical_columns].max() - Gold_List_Gibushon_Dico_Z_Scores[numerical_columns].min()) * \
#                                      (scaled_max - scaled_min) + scaled_min

### Round the Z scores to 2 decimal digits
#Gold_List_Gibushon_Dico_Z_Scores[numerical_columns] = Gold_List_Gibushon_Dico_Z_Scores[numerical_columns].round(2)

### Print the DataFrame after transformation
#print(Gold_List_Gibushon_Dico_Z_Scores)
Converting Z - Scores Potential Range to 0 (Min) to 5 (Max)
The purpose of the conversion is to avoid using negative values that may disrupt orders that will be executed later, and this, while basing itself on an order-preserving transformation that does not create a bias
# Exclude specific variables from adjustment
#exclude_variables = ["Evaluation_Center_Target", "Evaluation_Center_Dico_Target"]
#numerical_columns = Gold_List_Z_Scores.select_dtypes(include='number').columns.difference(exclude_variables)

# Apply the adjustment to make the minimum value zero
#min_values = Gold_List_Z_Scores[numerical_columns].min()
#Gold_List_Z_Scores[numerical_columns] = Gold_List_Z_Scores[numerical_columns] + (min_values * -1)
#Gold_List_Z_Scores.head()
1.6 Data Standatization
#for column in Gold_List_Gibushon_Dico_Linear_cleaned.columns:
#    Gold_List_Gibushon_Dico_Linear_cleaned[column] = Gold_List_Gibushon_Dico_Linear_cleaned[column]  / Gold_List_Gibushon_Dico_Linear_cleaned[column].abs().max()
#Gold_List_Gibushon_Dico_Linear_cleaned[Gold_List_Gibushon_Dico_Linear_cleaned.columns[1:]].corr()['Personality_Disqualification_Dico'].sort_values(ascending=False)
1.7 Composite Measure
Spearman correlation between Variables and the target - Final Gibushun Grade
Gold_List_Gibushon_Dico_Z_Scores[Gold_List_Gibushon_Dico_Z_Scores.columns[0:]].corr(method='spearman')['Evaluation_Center_Dico_Target'].sort_values(ascending=False)
Evaluation_Center_Dico_Target                1.000000
Job_Motivators_Index                         0.366708
Interests_and_Activities_Index               0.246221
Hebrew_1_Expressive_Num                      0.202659
United_Commander_or_Kazin                    0.185151
Previous_Job_Salary_Num                      0.096102
Married_Dico                                 0.081395
Psychiatric_Drugs_Dico                       0.080583
Physical_Fitness_Frequ_Num                  -0.066210
Evaluation_Center_Filed_Last_Attempt_Dico   -0.070952
Number_of_Attempts_Num                      -0.078852
Year_of_Service_Army_Dico                   -0.089098
Misconduct_Index                            -0.137528
English_3_Units_Num_Dico                    -0.146211
Name: Evaluation_Center_Dico_Target, dtype: float64
#Gold_List_Gibushon_Dico_Z_Scores['Composite_Measure_Spearman_Weights'] = Gold_List_Gibushon_Dico_Z_Scores.Job_Motivators_Index * 0.390 +)
#Gold_List_Gibushon_Dico_Z_Scores['Composite_Measure_Spearman_Weights']. corr(df['Evaluation_Center_Dico_Target']).round(2)
1.8 Creating Final List
### Create the new DataFrame 'Final_list'
Final_list_Gibushon_Dico = pd.DataFrame(Gold_List_Gibushon_Dico_Linear_cleaned)
### Drop rows with missing values
Final_list_Gibushon_Dico = Final_list_Gibushon_Dico.dropna()
for col in Final_list_Gibushon_Dico:
    if col in Final_list_Gibushon_Dico.columns:
        Final_list_Gibushon_Dico[col] = Final_list_Gibushon_Dico[col].astype(np.float64)
------------Feature Selection - Final Gibushon Grade------------
1.1 Import Packages
#pd.set_option('display.max_rows', None, 'display.max_columns', None)
df_num_corr = pd.DataFrame(Final_list_Gibushon_Dico)
corr = Final_list_Gibushon_Dico.corr(method = 'spearman')
#corr
#from scipy import stats

#vars_correlations = pd.DataFrame(columns=['var_1','var_2','Spear corr','p_value'])
#df2_num_corr = Final_list_Gibushon_Dico
#for i in Final_list_Gibushon_Dico.columns:
#        for j in Final_list_Gibushon_Dico.columns:
#            b = "{}/{}".format(i,j)
#            c = "{}/{}".format(j,i)
#            if i != j:
#                if (c not in vars_correlations.index):
#                    mask = ~pd.isna(Final_list_Gibushon_Dico[i]) & ~pd.isna(Final_list_Gibushon_Dico[j]) 
#                    a = stats.spearmanr(Final_list_Gibushon_Dico[i][mask], Final_list_Gibushon_Dico[j][mask])
#                    vars_correlations.loc[b] = [i,j,abs(a[0]),a[1]]
        
#Multicollinearity_correlations = vars_correlations.loc[(vars_correlations['Spear corr'] > 0.8) & (vars_correlations['p_value'] < 0.05)]
#Multicollinearity_correlations = Multicollinearity_correlations.round(2)
#Multicollinearity_correlations.sort_values(by=['Spear corr'], ascending=False)
VIF index in Multivariate linear regression
In the upcoming section, we will conduct a multivariate linear regression analysis to assess the Variance Inflation Factor (VIF). As a general guideline, a VIF index equal to or exceeding 5 may indicate the presence of multicollinearity among independent variables within the model.
It's important to emphasize that our primary objective with the regression results is not to ascertain the significance or strength of relationships between features and the target factor. Rather, we employ these results for a straightforward, rapid, and intuitive identification of predictors exhibiting robust correlation. To evaluate the significance and strength of relationships, we will employ non-parametric tests tailored to the project's target variable (a dichotomous classification problem).
import pandas as pd
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.linear_model import LogisticRegression

# Assuming Final_list is your DataFrame

# Separate predictors and target variable
x = Final_list_Gibushon_Dico.drop('Evaluation_Center_Dico_Target', axis=1)
y = Final_list_Gibushon_Dico['Evaluation_Center_Dico_Target']

# Add constant term for intercept
x = sm.add_constant(x)

# Fit the logistic regression model
log_reg = LogisticRegression()
log_reg.fit(x, y)

# Create the model
model = sm.Logit(y, x).fit()

# Get predictions
predictions = model.predict(x)

# Print model summary
print_model = model.summary()
print(print_model)

# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]
vif["Feature"] = x.columns

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
Optimization terminated successfully.
         Current function value: 0.469735
         Iterations 7
                                 Logit Regression Results                                
=========================================================================================
Dep. Variable:     Evaluation_Center_Dico_Target   No. Observations:                  193
Model:                                     Logit   Df Residuals:                      179
Method:                                      MLE   Df Model:                           13
Date:                           Tue, 09 Apr 2024   Pseudo R-squ.:                  0.3165
Time:                                   16:17:03   Log-Likelihood:                -90.659
converged:                                  True   LL-Null:                       -132.63
Covariance Type:                       nonrobust   LLR p-value:                 1.984e-12
=============================================================================================================
                                                coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------------------------
const                                        -6.3591      1.733     -3.669      0.000      -9.756      -2.962
Hebrew_1_Expressive_Num                       0.5090      0.206      2.473      0.013       0.106       0.912
United_Commander_or_Kazin                     0.7882      0.481      1.638      0.101      -0.155       1.731
Misconduct_Index                             -1.4412      0.485     -2.972      0.003      -2.392      -0.491
English_3_Units_Num_Dico                     -1.0250      0.523     -1.959      0.050      -2.050       0.000
Job_Motivators_Index                          4.1078      0.902      4.556      0.000       2.341       5.875
Interests_and_Activities_Index                0.7046      0.180      3.907      0.000       0.351       1.058
Previous_Job_Salary_Num                       0.5876      0.223      2.633      0.008       0.150       1.025
Married_Dico                                  0.4573      0.494      0.926      0.355      -0.511       1.426
Psychiatric_Drugs_Dico                        2.3908      1.498      1.596      0.111      -0.546       5.327
Evaluation_Center_Filed_Last_Attempt_Dico    -1.4517      0.637     -2.280      0.023      -2.700      -0.204
Physical_Fitness_Frequ_Num                   -0.2122      0.212     -1.003      0.316      -0.627       0.202
Year_of_Service_Army_Dico                    -3.7110      1.537     -2.414      0.016      -6.724      -0.698
Number_of_Attempts_Num                       -0.4094      0.191     -2.149      0.032      -0.783      -0.036
=============================================================================================================
VIF Factors:
    VIF Factor                                    Feature
0    71.358987                                      const
1     1.100618                    Hebrew_1_Expressive_Num
2     1.087119                  United_Commander_or_Kazin
3     1.124890                           Misconduct_Index
4     1.124058                   English_3_Units_Num_Dico
5     1.062231                       Job_Motivators_Index
6     1.089629             Interests_and_Activities_Index
7     1.177653                    Previous_Job_Salary_Num
8     1.137014                               Married_Dico
9     1.050204                     Psychiatric_Drugs_Dico
10    1.186459  Evaluation_Center_Filed_Last_Attempt_Dico
11    1.117474                 Physical_Fitness_Frequ_Num
12    1.043482                  Year_of_Service_Army_Dico
13    1.167275                     Number_of_Attempts_Num
# Calculate VIF for each feature
vif = pd.DataFrame()
vif["VIF Factor"] = [variance_inflation_factor(x.values, i) for i in range(1, x.shape[1])]
vif["Feature"] = x.columns[1:]

# Print VIF factor for each variable
print("VIF Factors:")
print(vif)
VIF Factors:
    VIF Factor                                    Feature
0     1.100618                    Hebrew_1_Expressive_Num
1     1.087119                  United_Commander_or_Kazin
2     1.124890                           Misconduct_Index
3     1.124058                   English_3_Units_Num_Dico
4     1.062231                       Job_Motivators_Index
5     1.089629             Interests_and_Activities_Index
6     1.177653                    Previous_Job_Salary_Num
7     1.137014                               Married_Dico
8     1.050204                     Psychiatric_Drugs_Dico
9     1.186459  Evaluation_Center_Filed_Last_Attempt_Dico
10    1.117474                 Physical_Fitness_Frequ_Num
11    1.043482                  Year_of_Service_Army_Dico
12    1.167275                     Number_of_Attempts_Num
Omitting multicollinear predictors from dataframe
# Filter features with VIF index equal to or higher than 5
high_vif_features = vif[vif["VIF Factor"]>=5]["Feature"].tolist()

# Omit features with high VIF from the DataFrame
Final_list_Gibushon_Dico_filtered = Final_list_Gibushon_Dico.drop(high_vif_features,axis=1)

# Print the features with high VIF index
print("Features with VIF index >= 5:", high_vif_features)

# Print the shape of the filtered DataFrame
print("Shape of Final_list after filtering:", Final_list_Gibushon_Dico_filtered.shape)
Features with VIF index >= 5: []
Shape of Final_list after filtering: (193, 14)
Final_list_Gibushon_Dico = Final_list_Gibushon_Dico_filtered.astype({"Evaluation_Center_Dico_Target":'category'})
Final_list_Gibushon_Dico.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 193 entries, 2 to 2225
Data columns (total 14 columns):
 #   Column                                     Non-Null Count  Dtype   
---  ------                                     --------------  -----   
 0   Hebrew_1_Expressive_Num                    193 non-null    float64 
 1   United_Commander_or_Kazin                  193 non-null    float64 
 2   Misconduct_Index                           193 non-null    float64 
 3   English_3_Units_Num_Dico                   193 non-null    float64 
 4   Job_Motivators_Index                       193 non-null    float64 
 5   Interests_and_Activities_Index             193 non-null    float64 
 6   Previous_Job_Salary_Num                    193 non-null    float64 
 7   Married_Dico                               193 non-null    float64 
 8   Psychiatric_Drugs_Dico                     193 non-null    float64 
 9   Evaluation_Center_Filed_Last_Attempt_Dico  193 non-null    float64 
 10  Physical_Fitness_Frequ_Num                 193 non-null    float64 
 11  Year_of_Service_Army_Dico                  193 non-null    float64 
 12  Number_of_Attempts_Num                     193 non-null    float64 
 13  Evaluation_Center_Dico_Target              193 non-null    category
dtypes: category(1), float64(13)
memory usage: 21.4 KB
1.5 The Feature Selection Process
No_colinear_list_Gibushon_Dico = Final_list_Gibushon_Dico.astype({"Evaluation_Center_Dico_Target":'category'})
No_colinear_list_Gibushon_Dico.dtypes
Hebrew_1_Expressive_Num                       float64
United_Commander_or_Kazin                     float64
Misconduct_Index                              float64
English_3_Units_Num_Dico                      float64
Job_Motivators_Index                          float64
Interests_and_Activities_Index                float64
Previous_Job_Salary_Num                       float64
Married_Dico                                  float64
Psychiatric_Drugs_Dico                        float64
Evaluation_Center_Filed_Last_Attempt_Dico     float64
Physical_Fitness_Frequ_Num                    float64
Year_of_Service_Army_Dico                     float64
Number_of_Attempts_Num                        float64
Evaluation_Center_Dico_Target                category
dtype: object
No_colinear_list_Gibushon_Dico = pd.DataFrame(No_colinear_list_Gibushon_Dico)
temp_cols=No_colinear_list_Gibushon_Dico.columns.tolist()
index=No_colinear_list_Gibushon_Dico.columns.get_loc("Evaluation_Center_Dico_Target")
new_cols=temp_cols[index:index+1] + temp_cols[0:index] + temp_cols[index+1:]
No_colinear_list_Gibushon_Dico=No_colinear_list_Gibushon_Dico[new_cols]
from importlib import reload
from pyMechkar.analysis import Table1
#reload(tb1)
varSel = pd.DataFrame({'Variable': No_colinear_list_Gibushon_Dico.columns[1:40]})
varSel.head(50)
                                     Variable
0                     Hebrew_1_Expressive_Num
1                   United_Commander_or_Kazin
2                            Misconduct_Index
3                    English_3_Units_Num_Dico
4                        Job_Motivators_Index
5              Interests_and_Activities_Index
6                     Previous_Job_Salary_Num
7                                Married_Dico
8                      Psychiatric_Drugs_Dico
9   Evaluation_Center_Filed_Last_Attempt_Dico
10                 Physical_Fitness_Frequ_Num
11                  Year_of_Service_Army_Dico
12                     Number_of_Attempts_Num
nm = No_colinear_list_Gibushon_Dico.columns[1:40]
nm = nm.append(pd.Index(['Evaluation_Center_Dico_Target']))
#nm
df2_Gibushon_Dico = No_colinear_list_Gibushon_Dico[nm].copy()
#df2_Gius.head(50)
5.2 Multivariable Analysis
No_colinear_list_Gibushon_Dico = No_colinear_list_Gibushon_Dico.dropna()
X = No_colinear_list_Gibushon_Dico[No_colinear_list_Gibushon_Dico.columns[~No_colinear_list_Gibushon_Dico.columns.isin(['Evaluation_Center_Dico_Target'])]]
y = No_colinear_list_Gibushon_Dico['Evaluation_Center_Dico_Target']
print([X.shape,y.shape])
[(193, 13), (193,)]
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import chi2
from sklearn.feature_selection import mutual_info_classif
from sklearn.feature_selection import SelectFpr
from sklearn.feature_selection import SelectFdr
from sklearn.feature_selection import SelectFwe
from sklearn.feature_selection import SelectPercentile
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LogisticRegression
Variable Selection using LASSO
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

lasso_mod = Lasso().fit(X, y)
model = SelectFromModel(lasso_mod, prefit=True)
varSel['Lasso'] = model.get_support().astype('int64')

#varSel
# SelectKBest with ANOVA F-value
skb_f = SelectKBest(score_func=f_classif, k='all').fit(X, y)
varSel['SelectKBest_f'] = skb_f.get_support().astype('int64')

#varSel
# SelectKBest with mutual information
skb_mutual_info = SelectKBest(score_func=mutual_info_classif, k='all').fit(X, y)
varSel['SelectKBest_mutual_info'] = skb_mutual_info.get_support().astype('int64')

#varSel
# SelectFpr
select_fpr = SelectFpr().fit(X, y)
varSel['SelectFpr'] = select_fpr.get_support().astype('int64')
#varSel
# SelectFdr
select_fdr = SelectFdr().fit(X, y)
varSel['SelectFdr'] = select_fdr.get_support().astype('int64')
#varSel
# SelectFwe
select_fwe = SelectFwe().fit(X, y)
varSel['SelectFwe'] = select_fwe.get_support().astype('int64')
#varSel
# SelectPercentile
select_percentile = SelectPercentile().fit(X, y)
varSel['SelectPercentile'] = select_percentile.get_support().astype('int64')
#varSel
from sklearn.linear_model import LogisticRegression

LogisticRegression_mod = LogisticRegression().fit(X, y)
model = SelectFromModel(LogisticRegression_mod, prefit=True)
varSel['LogisticRegression'] = model.get_support().astype('int64')
#varSel
from sklearn.ensemble import RandomForestClassifier

RandomForestClassifier_mod = RandomForestClassifier().fit(X, y)
model = SelectFromModel(RandomForestClassifier_mod, prefit=True)
varSel['RandomForestClassifier'] = model.get_support().astype('int64')
#varSel
from sklearn.ensemble import GradientBoostingRegressor

gb_mod = GradientBoostingRegressor().fit(X, y)
model = SelectFromModel(gb_mod, prefit=True)
varSel['GradientBoosting'] = model.get_support().astype('int64')
#varSel
from sklearn.linear_model import SGDClassifier

SGDClassifier_mod = SGDClassifier().fit(X, y)
model = SelectFromModel(SGDClassifier_mod, prefit=True)
varSel['SGDClassifier'] = model.get_support().astype('int64')
#varSel
Variable Selection using AdaBoostClassifier
from sklearn import preprocessing
from sklearn import utils

#convert y values to categorical values
lab = preprocessing.LabelEncoder()
y_transformed = lab.fit_transform(y)
from sklearn.ensemble import AdaBoostClassifier

AdaBoost = AdaBoostClassifier().fit(X, y_transformed)
model = SelectFromModel(AdaBoost, prefit=True)
model.get_support()

varSel['AdaBoost'] = model.get_support().astype('int64')
#varSel
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectFromModel

svmmod = LinearSVC(C=0.01, penalty="l1",dual=False).fit(X, y_transformed)
model = SelectFromModel(svmmod, prefit=True)
model.get_support()

varSel['SVM'] = model.get_support().astype('int64')
#varSel
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel

XGBClassifier_mod = XGBClassifier().fit(X, y_transformed)
model = SelectFromModel(XGBClassifier_mod, prefit=True)
model.get_support()

varSel['XGBClassifier'] = model.get_support().astype('int64')
#varSel
#pip install lightgbm
from lightgbm import LGBMClassifier
from sklearn.feature_selection import SelectFromModel

LGBMClassifier_mod = LGBMClassifier().fit(X, y_transformed)
model = SelectFromModel(LGBMClassifier_mod, prefit=True)
model.get_support()

varSel['LGBMClassifier'] = model.get_support().astype('int64')
#varSel
[LightGBM] [Info] Number of positive: 107, number of negative: 86
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 84
[LightGBM] [Info] Number of data points in the train set: 193, number of used features: 10
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.554404 -> initscore=0.218482
[LightGBM] [Info] Start training from score 0.218482
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_selection import SelectFromModel

DecisionTree_mod = DecisionTreeClassifier().fit(X, y_transformed)
model = SelectFromModel(DecisionTree_mod, prefit=True)
model.get_support()

varSel['DecisionTree'] = model.get_support().astype('int64')
#varSel
Summarization and Selection of Variables
varSel['Sum'] =  np.sum(varSel,axis=1)
#varSel
#varSelvarSel.groupby('Sum')['Variable'].count()
Final Gold List
Final_list_Gibushon_Dico_Feature_Selection = varSel[varSel['Sum']>=2]
Final_list_Gibushon_Dico_Feature_Selection
                                     Variable  Lasso  SelectKBest_f  \
0                     Hebrew_1_Expressive_Num      0              1   
1                   United_Commander_or_Kazin      0              1   
2                            Misconduct_Index      0              1   
3                    English_3_Units_Num_Dico      0              1   
4                        Job_Motivators_Index      0              1   
5              Interests_and_Activities_Index      0              1   
6                     Previous_Job_Salary_Num      0              1   
7                                Married_Dico      0              1   
8                      Psychiatric_Drugs_Dico      0              1   
9   Evaluation_Center_Filed_Last_Attempt_Dico      0              1   
10                 Physical_Fitness_Frequ_Num      0              1   
11                  Year_of_Service_Army_Dico      0              1   
12                     Number_of_Attempts_Num      0              1   

    SelectKBest_mutual_info  SelectFpr  SelectFdr  SelectFwe  \
0                         1          1          1          1   
1                         1          1          1          0   
2                         1          1          0          0   
3                         1          1          0          0   
4                         1          1          1          1   
5                         1          1          1          1   
6                         1          0          0          0   
7                         1          0          0          0   
8                         1          0          0          0   
9                         1          0          0          0   
10                        1          0          0          0   
11                        1          0          0          0   
12                        1          0          0          0   

    SelectPercentile  LogisticRegression  RandomForestClassifier  \
0                  0                   0                       1   
1                  0                   0                       0   
2                  0                   1                       0   
3                  0                   0                       0   
4                  1                   1                       1   
5                  1                   0                       1   
6                  0                   0                       1   
7                  0                   0                       0   
8                  0                   0                       0   
9                  0                   1                       0   
10                 0                   0                       1   
11                 0                   1                       0   
12                 0                   0                       0   

    GradientBoosting  SGDClassifier  AdaBoost  SVM  XGBClassifier  \
0                  0              0         1    0              1   
1                  0              0         0    0              1   
2                  0              1         1    0              1   
3                  0              0         0    0              1   
4                  1              1         1    0              1   
5                  1              0         1    1              1   
6                  0              0         1    0              1   
7                  0              0         0    0              0   
8                  0              1         0    0              0   
9                  0              1         0    0              1   
10                 0              0         0    0              0   
11                 0              1         0    0              0   
12                 0              0         1    0              0   

    LGBMClassifier  DecisionTree  Sum  
0                0             1    9  
1                0             0    5  
2                0             0    7  
3                0             0    4  
4                1             1   14  
5                1             1   13  
6                1             1    7  
7                0             0    2  
8                0             0    3  
9                0             0    5  
10               1             0    4  
11               0             0    4  
12               1             0    4  
# Filter features with small number of significant correlations
low_sig_corrs = varSel[varSel["Sum"]<=1]["Variable"].tolist()
# Omit features with high VIF from the DataFrame
Gius_Sofi_Gold_List = Final_list_Gibushon_Dico_filtered.drop(low_sig_corrs,axis=1)
Gius_Sofi_Gold_List.info()
<class 'pandas.core.frame.DataFrame'>
Int64Index: 193 entries, 2 to 2225
Data columns (total 14 columns):
 #   Column                                     Non-Null Count  Dtype  
---  ------                                     --------------  -----  
 0   Hebrew_1_Expressive_Num                    193 non-null    float64
 1   United_Commander_or_Kazin                  193 non-null    float64
 2   Misconduct_Index                           193 non-null    float64
 3   English_3_Units_Num_Dico                   193 non-null    float64
 4   Job_Motivators_Index                       193 non-null    float64
 5   Interests_and_Activities_Index             193 non-null    float64
 6   Previous_Job_Salary_Num                    193 non-null    float64
 7   Married_Dico                               193 non-null    float64
 8   Psychiatric_Drugs_Dico                     193 non-null    float64
 9   Evaluation_Center_Filed_Last_Attempt_Dico  193 non-null    float64
 10  Physical_Fitness_Frequ_Num                 193 non-null    float64
 11  Year_of_Service_Army_Dico                  193 non-null    float64
 12  Number_of_Attempts_Num                     193 non-null    float64
 13  Evaluation_Center_Dico_Target              193 non-null    float64
dtypes: float64(14)
memory usage: 22.6 KB
Export to CSV
download_path = r'C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/2Input_processed/Final_Data_Prep.xlsx'
df.to_excel(download_path, index=False)

print(f"Excel file saved to: {download_path}")
Excel file saved to: C:/Users/Knowl/Desktop/‏‏SADAC System Project/1Input/2Input_processed/Final_Data_Prep.xlsx

